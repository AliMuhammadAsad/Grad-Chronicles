{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-VQT3ju9eMM"
      },
      "source": [
        "# Assignment 2: Counting Tokens and Estimating Cost\n",
        "\n",
        "Example:\n",
        "https://tiktokenizer.vercel.app/?model=gpt-4-1106-preview\n",
        "\n",
        "## The Completed Tasks are done towards the end of the notebook\n",
        "\n",
        "## Objective:\n",
        "Write a Python program to count the number of tokens in a given text source (paragraph, text file, or PDF file) and calculate the cost of processing these tokens using OpenAI's GPT-4o pricing model.\n",
        "\n",
        "## Requirements:\n",
        "1. **Input**:\n",
        "   - A paragraph as a string.\n",
        "   - A text file containing the content.\n",
        "   - A PDF file with the content.\n",
        "2. **Output**:\n",
        "   - The total number of tokens in the input.\n",
        "   - The estimated cost of processing the input tokens using GPT-4o pricing ($2.50 per 1M tokens).\n",
        "3. **Constraints**:\n",
        "   - Use OpenAI's `tiktoken` library to tokenize the input.\n",
        "   - Ensure compatibility with different file formats (text and PDF).\n",
        "   - Handle invalid or empty inputs gracefully.\n",
        "\n",
        "## Example:\n",
        "### Input:\n",
        "```plaintext\n",
        "Paragraph: \"Tiktoken is a tokenizer by OpenAI. It splits text into tokens.\"\n",
        "```\n",
        "### Output:\n",
        "```plaintext\n",
        "Total Tokens: 12\n",
        "Estimated Cost: $0.00003\n",
        "```\n",
        "\n",
        "### Input:\n",
        "```plaintext\n",
        "Text File: \"example.txt\" (contains 500 tokens)\n",
        "```\n",
        "### Output:\n",
        "```plaintext\n",
        "Total Tokens: 500\n",
        "Estimated Cost: $0.00125\n",
        "```\n",
        "\n",
        "## Extra Credit:\n",
        "Whoever calculates the total number of tokens and the cost for all 6 **Harry Potter** books combined will receive an **assignment pass**, which can be used to skip any future assignment.\n",
        "\n",
        "### Additional Notes:\n",
        "- Use `tiktoken`'s `encode` method to calculate token counts.\n",
        "- For PDF files, extract the text content first (e.g., using a library like `PyPDF2`).\n",
        "- Costs should be calculated as `total_tokens / 1,000,000 * 2.50`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai6QcQDJ7uVi"
      },
      "source": [
        "# How to count tokens with tiktoken\n",
        "\n",
        "[`tiktoken`](https://github.com/openai/tiktoken/blob/main/README.md) is a fast open-source tokenizer by OpenAI.\n",
        "\n",
        "Given a text string (e.g., `\"tiktoken is great!\"`) and an encoding (e.g., `\"cl100k_base\"`), a tokenizer can split the text string into a list of tokens (e.g., `[\"t\", \"ik\", \"token\", \" is\", \" great\", \"!\"]`).\n",
        "\n",
        "Splitting text strings into tokens is useful because GPT models see text in the form of tokens. Knowing how many tokens are in a text string can tell you (a) whether the string is too long for a text model to process and (b) how much an OpenAI API call costs (as usage is priced by token).\n",
        "\n",
        "\n",
        "## Encodings\n",
        "\n",
        "Encodings specify how text is converted into tokens. Different models use different encodings.\n",
        "\n",
        "`tiktoken` supports three encodings used by OpenAI models:\n",
        "\n",
        "| Encoding name           | OpenAI models                                       |\n",
        "|-------------------------|-----------------------------------------------------|\n",
        "| `o200k_base`            | `gpt-4o`, `gpt-4o-mini`                             |\n",
        "| `cl100k_base`           | `gpt-4-turbo`, `gpt-4`, `gpt-3.5-turbo`, `text-embedding-ada-002`, `text-embedding-3-small`, `text-embedding-3-large`  |\n",
        "| `p50k_base`             | Codex models, `text-davinci-002`, `text-davinci-003`|\n",
        "| `r50k_base` (or `gpt2`) | GPT-3 models like `davinci`                         |\n",
        "\n",
        "You can retrieve the encoding for a model using `tiktoken.encoding_for_model()` as follows:\n",
        "```python\n",
        "encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
        "```\n",
        "\n",
        "Note that `p50k_base` overlaps substantially with `r50k_base`, and for non-code applications, they will usually give the same tokens.\n",
        "\n",
        "## Tokenizer libraries by language\n",
        "\n",
        "For `o200k_base`, `cl100k_base` and `p50k_base` encodings:\n",
        "- Python: [tiktoken](https://github.com/openai/tiktoken/blob/main/README.md)\n",
        "- .NET / C#: [SharpToken](https://github.com/dmitry-brazhenko/SharpToken), [TiktokenSharp](https://github.com/aiqinxuancai/TiktokenSharp)\n",
        "- Java: [jtokkit](https://github.com/knuddelsgmbh/jtokkit)\n",
        "- Golang: [tiktoken-go](https://github.com/pkoukk/tiktoken-go)\n",
        "- Rust: [tiktoken-rs](https://github.com/zurawiki/tiktoken-rs)\n",
        "\n",
        "For `r50k_base` (`gpt2`) encodings, tokenizers are available in many languages.\n",
        "- Python: [tiktoken](https://github.com/openai/tiktoken/blob/main/README.md) (or alternatively [GPT2TokenizerFast](https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2TokenizerFast))\n",
        "- JavaScript: [gpt-3-encoder](https://www.npmjs.com/package/gpt-3-encoder)\n",
        "- .NET / C#: [GPT Tokenizer](https://github.com/dluc/openai-tools)\n",
        "- Java: [gpt2-tokenizer-java](https://github.com/hyunwoongko/gpt2-tokenizer-java)\n",
        "- PHP: [GPT-3-Encoder-PHP](https://github.com/CodeRevolutionPlugins/GPT-3-Encoder-PHP)\n",
        "- Golang: [tiktoken-go](https://github.com/pkoukk/tiktoken-go)\n",
        "- Rust: [tiktoken-rs](https://github.com/zurawiki/tiktoken-rs)\n",
        "\n",
        "(OpenAI makes no endorsements or guarantees of third-party libraries.)\n",
        "\n",
        "\n",
        "## How strings are typically tokenized\n",
        "\n",
        "In English, tokens commonly range in length from one character to one word (e.g., `\"t\"` or `\" great\"`), though in some languages tokens can be shorter than one character or longer than one word. Spaces are usually grouped with the starts of words (e.g., `\" is\"` instead of `\"is \"` or `\" \"`+`\"is\"`). You can quickly check how a string is tokenized at the [OpenAI Tokenizer](https://beta.openai.com/tokenizer), or the third-party [Tiktokenizer](https://tiktokenizer.vercel.app/) webapp."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wendmGSL7uVk"
      },
      "source": [
        "## 0. Install `tiktoken`\n",
        "\n",
        "If needed, install `tiktoken` with `pip`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNgxRDZa7uVl",
        "outputId": "b3941b93-0d76-47c0-f266-1d432eb279d0"
      },
      "outputs": [],
      "source": [
        "# %pip install --upgrade tiktoken -q\n",
        "# %pip install --upgrade openai -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuxBm3Os7uVm"
      },
      "source": [
        "## 1. Import `tiktoken`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bo-PN1gQ7uVm"
      },
      "outputs": [],
      "source": [
        "import tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIWYiQcl7uVm"
      },
      "source": [
        "## 2. Load an encoding\n",
        "\n",
        "Use `tiktoken.get_encoding()` to load an encoding by name.\n",
        "\n",
        "The first time this runs, it will require an internet connection to download. Later runs won't need an internet connection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aBKsV_BS7uVm"
      },
      "outputs": [],
      "source": [
        "# encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "encoding = tiktoken.get_encoding(\"o200k_base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8te9XsO07uVn"
      },
      "source": [
        "Use `tiktoken.encoding_for_model()` to automatically load the correct encoding for a given model name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "X80VuaDU7uVn"
      },
      "outputs": [],
      "source": [
        "# encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XmCT7Cz7uVn"
      },
      "source": [
        "## 3. Turn text into tokens with `encoding.encode()`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-m2imT67uVn"
      },
      "source": [
        "The `.encode()` method converts a text string into a list of token integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMpqB1Jm7uVn",
        "outputId": "7bdea7d0-3097-4b61-f210-8d54ce0bec8b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[83, 8251, 2488, 382, 2212, 0]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoding.encode(\"tiktoken is great!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js033ojb7uVn"
      },
      "source": [
        "Count tokens by counting the length of the list returned by `.encode()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "G3KpHVOw7uVn"
      },
      "outputs": [],
      "source": [
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qqtia3cP7uVo",
        "outputId": "b54b433e-e6fa-40ce-f2fb-c2e2ecdf0e8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_tokens_from_string(\"tiktoken is great!\", \"o200k_base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snO2f70B7uVo"
      },
      "source": [
        "## 4. Turn tokens into text with `encoding.decode()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5letL4i7uVo"
      },
      "source": [
        "`.decode()` converts a list of token integers to a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "tvpNAiE17uVo",
        "outputId": "6575ad97-7dcf-45bf-fcb7-5ece9fba6958"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'tiktoken is great!'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoding.decode([83, 8251, 2488, 382, 2212, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5m3do1u7uVo"
      },
      "source": [
        "Warning: although `.decode()` can be applied to single tokens, beware that it can be lossy for tokens that aren't on utf-8 boundaries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxdBHBuh7uVo"
      },
      "source": [
        "For single tokens, `.decode_single_token_bytes()` safely converts a single integer token to the bytes it represents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Nc-_cvZ7uVo",
        "outputId": "6a6bd06a-0d7c-4c0c-e9b1-4eb116781b21"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[b't', b'ikt', b'oken', b' is', b' great', b'!']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[encoding.decode_single_token_bytes(token) for token in [83, 8251, 2488, 382, 2212, 0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmlt5AmM7uVo"
      },
      "source": [
        "(The `b` in front of the strings indicates that the strings are byte strings.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXgvVoVf7uVo"
      },
      "source": [
        "## 5. Comparing encodings\n",
        "\n",
        "Different encodings vary in how they split words, group spaces, and handle non-English characters. Using the methods above, we can compare different encodings on a few example strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "z7EtgZno7uVo"
      },
      "outputs": [],
      "source": [
        "def compare_encodings(example_string: str) -> None:\n",
        "    \"\"\"Prints a comparison of three string encodings.\"\"\"\n",
        "    # print the example string\n",
        "    print(f'\\nExample string: \"{example_string}\"')\n",
        "    # for each encoding, print the # of tokens, the token integers, and the token bytes\n",
        "    for encoding_name in [\"r50k_base\", \"p50k_base\", \"cl100k_base\", \"o200k_base\"]:\n",
        "        encoding = tiktoken.get_encoding(encoding_name)\n",
        "        token_integers = encoding.encode(example_string)\n",
        "        num_tokens = len(token_integers)\n",
        "        token_bytes = [encoding.decode_single_token_bytes(token) for token in token_integers]\n",
        "        print()\n",
        "        print(f\"{encoding_name}: {num_tokens} tokens\")\n",
        "        print(f\"token integers: {token_integers}\")\n",
        "        print(f\"token bytes: {token_bytes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmIlYgHB7uVo",
        "outputId": "d5ce369c-fdd6-4539-93f7-0225fc2b8049"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example string: \"antidisestablishmentarianism\"\n",
            "\n",
            "r50k_base: 5 tokens\n",
            "token integers: [415, 29207, 44390, 3699, 1042]\n",
            "token bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\n",
            "\n",
            "p50k_base: 5 tokens\n",
            "token integers: [415, 29207, 44390, 3699, 1042]\n",
            "token bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\n",
            "\n",
            "cl100k_base: 6 tokens\n",
            "token integers: [519, 85342, 34500, 479, 8997, 2191]\n",
            "token bytes: [b'ant', b'idis', b'establish', b'ment', b'arian', b'ism']\n",
            "\n",
            "o200k_base: 6 tokens\n",
            "token integers: [493, 129901, 376, 160388, 21203, 2367]\n",
            "token bytes: [b'ant', b'idis', b'est', b'ablishment', b'arian', b'ism']\n"
          ]
        }
      ],
      "source": [
        "compare_encodings(\"antidisestablishmentarianism\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byM1D1bi7uVo",
        "outputId": "de38c38d-ac8c-4ba7-8fa9-d83d1b92ce2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example string: \"2 + 2 = 4\"\n",
            "\n",
            "r50k_base: 5 tokens\n",
            "token integers: [17, 1343, 362, 796, 604]\n",
            "token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n",
            "\n",
            "p50k_base: 5 tokens\n",
            "token integers: [17, 1343, 362, 796, 604]\n",
            "token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n",
            "\n",
            "cl100k_base: 7 tokens\n",
            "token integers: [17, 489, 220, 17, 284, 220, 19]\n",
            "token bytes: [b'2', b' +', b' ', b'2', b' =', b' ', b'4']\n",
            "\n",
            "o200k_base: 7 tokens\n",
            "token integers: [17, 659, 220, 17, 314, 220, 19]\n",
            "token bytes: [b'2', b' +', b' ', b'2', b' =', b' ', b'4']\n"
          ]
        }
      ],
      "source": [
        "compare_encodings(\"2 + 2 = 4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cyr9NUa7uVp",
        "outputId": "ffbb7229-39ec-49f3-f776-ef9f1f66026d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example string: \"السلام علیکم ، کیسے ہیں آپ؟\"\n",
            "\n",
            "r50k_base: 36 tokens\n",
            "token integers: [23525, 45692, 13862, 12919, 25405, 17550, 117, 13862, 151, 234, 150, 102, 25405, 17550, 234, 220, 150, 102, 151, 234, 45692, 151, 240, 220, 151, 223, 151, 234, 150, 118, 17550, 95, 149, 122, 148, 253]\n",
            "token bytes: [b'\\xd8\\xa7\\xd9\\x84', b'\\xd8\\xb3', b'\\xd9\\x84', b'\\xd8\\xa7', b'\\xd9\\x85', b' \\xd8', b'\\xb9', b'\\xd9\\x84', b'\\xdb', b'\\x8c', b'\\xda', b'\\xa9', b'\\xd9\\x85', b' \\xd8', b'\\x8c', b' ', b'\\xda', b'\\xa9', b'\\xdb', b'\\x8c', b'\\xd8\\xb3', b'\\xdb', b'\\x92', b' ', b'\\xdb', b'\\x81', b'\\xdb', b'\\x8c', b'\\xda', b'\\xba', b' \\xd8', b'\\xa2', b'\\xd9', b'\\xbe', b'\\xd8', b'\\x9f']\n",
            "\n",
            "p50k_base: 36 tokens\n",
            "token integers: [23525, 45692, 13862, 12919, 25405, 17550, 117, 13862, 151, 234, 150, 102, 25405, 17550, 234, 220, 150, 102, 151, 234, 45692, 151, 240, 220, 151, 223, 151, 234, 150, 118, 17550, 95, 149, 122, 148, 253]\n",
            "token bytes: [b'\\xd8\\xa7\\xd9\\x84', b'\\xd8\\xb3', b'\\xd9\\x84', b'\\xd8\\xa7', b'\\xd9\\x85', b' \\xd8', b'\\xb9', b'\\xd9\\x84', b'\\xdb', b'\\x8c', b'\\xda', b'\\xa9', b'\\xd9\\x85', b' \\xd8', b'\\x8c', b' ', b'\\xda', b'\\xa9', b'\\xdb', b'\\x8c', b'\\xd8\\xb3', b'\\xdb', b'\\x92', b' ', b'\\xdb', b'\\x81', b'\\xdb', b'\\x8c', b'\\xda', b'\\xba', b' \\xd8', b'\\xa2', b'\\xd9', b'\\xbe', b'\\xd8', b'\\x9f']\n",
            "\n",
            "cl100k_base: 27 tokens\n",
            "token integers: [32482, 20665, 8700, 50488, 45082, 8700, 14728, 33411, 10386, 8979, 234, 46693, 14728, 20665, 151, 240, 220, 151, 223, 14728, 150, 118, 8979, 95, 68483, 148, 253]\n",
            "token bytes: [b'\\xd8\\xa7\\xd9\\x84', b'\\xd8\\xb3', b'\\xd9\\x84', b'\\xd8\\xa7\\xd9\\x85', b' \\xd8\\xb9', b'\\xd9\\x84', b'\\xdb\\x8c', b'\\xda\\xa9', b'\\xd9\\x85', b' \\xd8', b'\\x8c', b' \\xda\\xa9', b'\\xdb\\x8c', b'\\xd8\\xb3', b'\\xdb', b'\\x92', b' ', b'\\xdb', b'\\x81', b'\\xdb\\x8c', b'\\xda', b'\\xba', b' \\xd8', b'\\xa2', b'\\xd9\\xbe', b'\\xd8', b'\\x9f']\n",
            "\n",
            "o200k_base: 8 tokens\n",
            "token integers: [131610, 47103, 105106, 4281, 179928, 12406, 26418, 11388]\n",
            "token bytes: [b'\\xd8\\xa7\\xd9\\x84\\xd8\\xb3\\xd9\\x84\\xd8\\xa7\\xd9\\x85', b' \\xd8\\xb9\\xd9\\x84\\xdb\\x8c', b'\\xda\\xa9\\xd9\\x85', b' \\xd8\\x8c', b' \\xda\\xa9\\xdb\\x8c\\xd8\\xb3\\xdb\\x92', b' \\xdb\\x81\\xdb\\x8c\\xda\\xba', b' \\xd8\\xa2\\xd9\\xbe', b'\\xd8\\x9f']\n"
          ]
        }
      ],
      "source": [
        "compare_encodings(\"السلام علیکم ، کیسے ہیں آپ؟\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMcXOqlH7uVp"
      },
      "source": [
        "## 6. Counting tokens for chat completions API calls\n",
        "\n",
        "ChatGPT models like `gpt-4o-mini` and `gpt-4` use tokens in the same way as older completions models, but because of their message-based formatting, it's more difficult to count how many tokens will be used by a conversation.\n",
        "\n",
        "Below is an example function for counting tokens for messages passed to `gpt-3.5-turbo`, `gpt-4`, `gpt-4o` and `gpt-4o-mini`.\n",
        "\n",
        "Note that the exact way that tokens are counted from messages may change from model to model. Consider the counts from the function below an estimate, not a timeless guarantee.\n",
        "\n",
        "In particular, requests that use the optional functions input will consume extra tokens on top of the estimates calculated below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LLvS_AdI7uVp"
      },
      "outputs": [],
      "source": [
        "def num_tokens_from_messages(messages, model=\"gpt-4o-mini-2024-07-18\"):\n",
        "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        print(\"Warning: model not found. Using o200k_base encoding.\")\n",
        "        encoding = tiktoken.get_encoding(\"o200k_base\")\n",
        "    if model in {\n",
        "        \"gpt-3.5-turbo-0125\",\n",
        "        \"gpt-4-0314\",\n",
        "        \"gpt-4-32k-0314\",\n",
        "        \"gpt-4-0613\",\n",
        "        \"gpt-4-32k-0613\",\n",
        "        \"gpt-4o-mini-2024-07-18\",\n",
        "        \"gpt-4o-2024-08-06\"\n",
        "        }:\n",
        "        tokens_per_message = 3\n",
        "        tokens_per_name = 1\n",
        "    elif \"gpt-3.5-turbo\" in model:\n",
        "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0125.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0125\")\n",
        "    elif \"gpt-4o-mini\" in model:\n",
        "        print(\"Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-4o-mini-2024-07-18\")\n",
        "    elif \"gpt-4o\" in model:\n",
        "        print(\"Warning: gpt-4o and gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-2024-08-06.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-4o-2024-08-06\")\n",
        "    elif \"gpt-4\" in model:\n",
        "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}.\"\"\"\n",
        "        )\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
        "    return num_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJRUQEQP-yJW",
        "outputId": "f7a7461c-b0c9-4d28-943f-55d5c076c38f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Tokens: 15\n",
            "Estimated Cost: $0.000038\n"
          ]
        }
      ],
      "source": [
        "def count_tokens_and_cost_with_messages(messages, model=\"gpt-4o-mini-2024-07-18\"):\n",
        "    \"\"\"\n",
        "    Counts the total tokens and calculates the processing cost for a list of messages.\n",
        "\n",
        "    Args:\n",
        "    - messages (list): A list of message dictionaries, where each dictionary represents a message.\n",
        "    - model (str): The model name used for token calculation (default: \"gpt-4o-mini-2024-07-18\").\n",
        "\n",
        "    Returns:\n",
        "    - dict: A dictionary containing the total tokens and estimated cost.\n",
        "    \"\"\"\n",
        "    # Calculate the total tokens using the existing method\n",
        "    total_tokens = num_tokens_from_messages(messages, model)\n",
        "\n",
        "    # Calculate the cost based on GPT-4o pricing\n",
        "    cost_per_million_tokens = 2.50  # $2.50 per 1M tokens\n",
        "    cost = (total_tokens / 1_000_000) * cost_per_million_tokens\n",
        "\n",
        "    # Return the result as a dictionary\n",
        "    return {\"Total Tokens\": total_tokens, \"Cost (USD)\": f\"{cost:.6f}\"}\n",
        "\n",
        "# Example usage\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is the weather today in Karachi?\"}\n",
        "]\n",
        "\n",
        "result = count_tokens_and_cost_with_messages(messages)\n",
        "print(f\"Total Tokens: {result['Total Tokens']}\")\n",
        "print(f\"Estimated Cost: ${result['Cost (USD)']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Assignment Begin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from PyPDF2 import PdfReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "COST_PER_MILLION_TOKENS = 2.50  # $2.50 per 1M tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_paragraphs(para: str, encoding_name: str = \"cl100k_base\"):\n",
        "    \"\"\"Returns the number of tokens and cost of reading the tokens in the given text.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    tokens = encoding.encode(para)\n",
        "    cost = len(tokens) / 1_000_000 * COST_PER_MILLION_TOKENS\n",
        "    # return f\"Number of tokens: {len(tokens)}, Cost: ${cost:.8f}\"\n",
        "    return (len(tokens), cost)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "paragraph = \"\"\"\n",
        "V For Vendetta\n",
        "Remember, remember The fifth of November The gunpowder treason and plot. I know of no reason Why the gunpowder treason Should ever be forgot.\" But what of the man? I know his name was Guy Fawkes, and I know that, in 1605, he attempted to blow up the houses of Parliament. But who was he really? What was he like? We are told to remember the idea, not the man, because a man can fail. He can be caught. He can be killed and forgotten. But four hundred years later an idea can still change the world. I've witnessed firsthand the power of ideas. I've seen people kill in the name of them; and die defending them. But you cannot kiss an idea, cannot touch it or hold it. Ideas do not bleed. They do not feel pain. They do not love. And it is not an idea that I miss, it is a man. A man that made me remember the fifth of November. A man that I will never forget.\n",
        "\n",
        "\n",
        "Lord of the rings, Sam <3\n",
        "FRODO: I can’t do this, Sam.\n",
        "\n",
        "SAM: I know. It’s all wrong. By rights we shouldn’t even be here. But we are. It’s like in the great stories, Mr. Frodo. The ones that really mattered. Full of darkness and danger they were. And sometimes you didn’t want to know the end. Because how could the end be happy. How could the world go back to the way it was when so much bad had happened. But in the end, it’s only a passing thing, this shadow. Even darkness must pass. A new day will come. And when the sun shines it will shine out the clearer. Those were the stories that stayed with you. That meant something. Even if you were too small to understand why. But I think, Mr. Frodo, I do understand. I know now. Folk in those stories had lots of chances of turning back only they didn’t. Because they were holding on to something.\n",
        "\n",
        "FRODO: What are we holding on to, Sam?\n",
        "\n",
        "SAM: That there’s some good in this world, Mr. Frodo. And it’s worth fighting for.\n",
        "\n",
        "\n",
        "Lord of the rings, Gandalf <3\n",
        "Deserves it! I daresay he does. Many that live deserve death. And some that die deserve life. Can you give it to them? Then do not be too eager to deal out death in judgement. For even the very wise cannot see all ends.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(526, 0.001315)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parse_paragraphs(paragraph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Tokens: 229\n",
            "Estimated Cost: $0.000572\n"
          ]
        }
      ],
      "source": [
        "def parse_text_file(filepath: str):\n",
        "    \"\"\"Reads and returns the tokens and cost for reading a text file\"\"\"\n",
        "    if not os.path.isfile(filepath):\n",
        "        raise FileNotFoundError(\"File not found: {filepath}\")\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    \n",
        "    lines = [lines.strip().replace('\\n', ' ') for lines in text.split('\\n') if lines.strip()]\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": line} for line in lines]\n",
        "    res = count_tokens_and_cost_with_messages(messages)\n",
        "    return res[\"Total Tokens\"], float(res[\"Cost (USD)\"])\n",
        "    \n",
        "    # return parse_paragraphs(text)\n",
        "\n",
        "tokens, cost = parse_text_file(\"sample.txt\")\n",
        "print(f\"Total Tokens: {tokens}\")\n",
        "print(f\"Estimated Cost: ${cost:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Tokens: 229\n",
            "Estimated Cost: $0.000572\n"
          ]
        }
      ],
      "source": [
        "def parse_pdf_file(filepath: str):\n",
        "    \"\"\"Reads and returns the tokens and cost for reading a pdf file\"\"\"\n",
        "    if not os.path.isfile(filepath):\n",
        "        raise FileNotFoundError(\"File not found: {filepath}\")\n",
        "    # reader = PdfFileReader(filepath)\n",
        "    reader = PdfReader(filepath)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() or \"\"\n",
        "    \n",
        "    lines = [lines.strip().replace('\\n', ' ') for lines in text.split('\\n') if lines.strip()]\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": line} for line in lines]\n",
        "    res = count_tokens_and_cost_with_messages(messages)\n",
        "    return res[\"Total Tokens\"], float(res[\"Cost (USD)\"])\n",
        "\n",
        "tokens, cost = parse_pdf_file(\"sample.pdf\")\n",
        "print(f\"Total Tokens: {tokens}\")\n",
        "print(f\"Estimated Cost: ${cost:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extra Credit - All 6 Harry Potter Books (7 - including deathly hallows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hp_books = [\n",
        "#     \"HP1-SorcerersStone.pdf\",\n",
        "#     \"HP2-ChamberOfSecrets.pdf\",\n",
        "#     \"HP3-PrisonerOfAzkaban.pdf\",\n",
        "#     \"HP4-GobletOfFire.pdf\",\n",
        "#     \"HP5-OrderOfThePhoenix.pdf\",\n",
        "#     \"HP6-HalfBloodPrince.pdf\",\n",
        "#     \"HP7-DeathlyHallows.pdf\"\n",
        "# ]\n",
        "hp = \"harrypotter.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Tokens: 2682675, Total Cost: $6.70668800\n"
          ]
        }
      ],
      "source": [
        "total_tokens = 0\n",
        "total_cost = 0\n",
        "\n",
        "# for book in hp_books:\n",
        "#     count_tokens_and_cost_with_pdf_files(book)\n",
        "#     tokens, cost = count_tokens_and_cost_with_pdf_files(book)\n",
        "#     total_tokens += tokens\n",
        "#     total_cost += cost\n",
        "total_tokens, total_cost = parse_pdf_file(hp)\n",
        "\n",
        "print(f\"Total Tokens: {total_tokens}, Total Cost: ${total_cost:.8f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
