{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8JYQo65eezn9",
   "metadata": {
    "executionInfo": {
     "elapsed": 540,
     "status": "ok",
     "timestamp": 1737808915810,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "8JYQo65eezn9"
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "#                                                               #\n",
    "#  CS435 Generative AI: Security, Ethics and Governance         #\n",
    "#                                                               #\n",
    "#  Instructor: Dr. Adnan Masood                                 #\n",
    "#  Contact:    adnanmasood@gmail.com                            #\n",
    "#                                                               #\n",
    "#  Notebook is MIT Licensed                                     #\n",
    "#################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eE0Ew0pezn-",
   "metadata": {
    "id": "8eE0Ew0pezn-"
   },
   "source": [
    "# Reinforcement Learning from Human Feedback (RLHF)\n",
    "\n",
    "## Table of Contents\n",
    "1. [Overview](#overview)\n",
    "2. [Building an Intuitive Understanding](#5-levels)\n",
    "3. [Intuition Behind RLHF](#intuition)\n",
    "4. [Brief History and Invention of RLHF](#history)\n",
    "5. [Underlying Technology](#underlying_tech)\n",
    "6. [Mathematical Explanation](#math_explanation)\n",
    "7. [Illustrative Example with Code](#illustrative_example)\n",
    "    - [Example Calculations](#mock_calculations)\n",
    "    - [Step-by-Step Example with a Public Dataset](#step_by_step)\n",
    "8. [Illustrative Problem It Solves](#illustrative_problem)\n",
    "9. [Real-World Problem It Solves](#real_world_problem)\n",
    "10. [Solving a Real-World Problem Using RLHF](#solving_real_world)\n",
    "11. [Questions to Ponder & Their Answers (with code)](#questions_ponder)\n",
    "12. [A Sample Exercise](#easy_code_sample)\n",
    "13. [Glossary](#glossary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {
    "id": "overview"
   },
   "source": [
    "<a id=\"overview\"></a>\n",
    "## Overview\n",
    "Reinforcement Learning from Human Feedback (RLHF) is a technique where machines learn to perform tasks or generate outputs that humans find more aligned with their intentions, preferences, or values. Instead of learning purely from an automated reward signal, the model integrates human feedback (like preferences, rankings, or explicit reinforcement signals) to adjust its behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5-levels",
   "metadata": {
    "id": "5-levels"
   },
   "source": [
    "# Building an Intuitive Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level1",
   "metadata": {
    "id": "level1"
   },
   "source": [
    "*(A middle-school-friendly explanation)*\n",
    "\n",
    "Imagine you have a robot friend who wants to learn how to make the best sandwich for you. You taste each sandwich it makes, and you give the robot a thumbs-up or thumbs-down. Over time, the robot figures out what makes you happy and makes better sandwiches. **That** is Reinforcement Learning from Human Feedback: using people's opinions to teach a machine how to do something **people** like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level2",
   "metadata": {
    "id": "level2"
   },
   "source": [
    "*(A slightly more detailed explanation)*\n",
    "\n",
    "Now, think of a computer program trying to answer questions in a friendly, helpful way. It might see examples of good answers and bad answers (provided by humans). The program learns patterns of what makes an answer \"good\" (maybe it's correct, polite, and easy to read). Over time, it figures out how to answer better. In RLHF, the computer updates its approach based on human feedback about whether it's doing the right thing. This feedback becomes a \"reward\" for the computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level3",
   "metadata": {
    "id": "level3"
   },
   "source": [
    "*(Going deeper into the concepts)*\n",
    "\n",
    "In standard machine learning, you often have a fixed dataset of questions and correct answers. However, with **Reinforcement Learning (RL)**, an agent (our model) performs actions and then observes rewards. When we add a human in the loop, we use people's preferences or evaluations as part of those rewards. This method can correct the model in nuanced situations where pure algorithmic rewards might be tricky to design.\n",
    "\n",
    "For instance, a language model might generate text. Humans then rank these outputs from best to worst. The model uses these rankings to learn a reward function—a function that estimates how good any answer is. The RL algorithm tries to maximize this learned reward function, effectively aligning model outputs with human preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level4",
   "metadata": {
    "id": "level4"
   },
   "source": [
    "*(A more detailed, mathematical discussion but still approachable)*\n",
    "\n",
    "Typically, RLHF has three main steps:\n",
    "1. **Supervised Fine-Tuning (SFT)**: The model is first trained or fine-tuned on high-quality data so it produces reasonable outputs.\n",
    "2. **Reward Modeling**: Human labelers rank different outputs (e.g., answers). We use these rankings to train a **reward model** $R_\\theta$ which assigns higher scores to outputs that humans like.\n",
    "3. **Policy Optimization (RL)**: We treat the language model's responses as \"actions\" and feed them into the reward model. We adjust the model's parameters to **maximize** the predicted reward from $R_\\theta$. Algorithms like **PPO (Proximal Policy Optimization)** are commonly used.\n",
    "\n",
    "Mathematically, if $\\pi_\\phi$ is our policy (the language model), we want to solve:\n",
    "$$\n",
    "\\max_{\\phi} \\mathbb{E}_{x \\sim D, y \\sim \\pi_\\phi(x)}[ R_\\theta(x, y) ].\n",
    "$$\n",
    "Here,\n",
    "- $x$ is some input (question or context),\n",
    "- $y$ is a generated output (answer),\n",
    "- $R_\\theta$ is the reward model parameterized by $\\theta$,\n",
    "- $\\pi_\\phi$ is the policy (also the language model) parameterized by $\\phi$.\n",
    "\n",
    "In simpler terms, **pick the parameters** $\\phi$ that yield the highest average reward according to the human-trained reward model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level5",
   "metadata": {
    "id": "level5"
   },
   "source": [
    "*(A deeper dive into theoretical aspects)*\n",
    "\n",
    "In a more formal RL sense, RLHF can be viewed as a structured approach to the credit assignment problem in partially observed environments, where human-generated signals replace or augment traditional numeric reward signals. Because human feedback is often **noisy and subjective**, there's research into Bayesian models of annotation to handle inter-annotator disagreement, as well as active learning strategies to query the most informative samples for human evaluation.\n",
    "\n",
    "Further complexity arises in ensuring **robustness** and **generalization** of the learned reward function. We must also consider the possibility of **reward hacking**, where the policy exploits the learned reward model's imperfections. Techniques like **KL regularization** (often used in PPO) help keep the updated policy close to the supervised policy to avoid drifting into strange territory.\n",
    "\n",
    "Mathematically, advanced treatments involve measuring the divergence between the policy distribution and the baseline distribution, using metrics like the Kullback-Leibler (KL) divergence:\n",
    "$$\n",
    "\\mathcal{L}_{\\mathrm{RLHF}}(\\phi) = \\mathbb{E}_{x \\sim D, y \\sim \\pi_\\phi}[R_\\theta(x,y)] - \\beta \\; KL\\big(\\pi_\\phi(y|x)\\,\\|\\,\\pi_{\\mathrm{SFT}}(y|x)\\big).\n",
    "$$\n",
    "Here, $\\beta$ is a hyperparameter controlling how tightly we want to stay near the supervised fine-tuned policy $\\pi_{\\mathrm{SFT}}$. This encourages the model not to stray too far from safe, known-good behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intuition",
   "metadata": {
    "id": "intuition"
   },
   "source": [
    "# Intuition Behind RLHF\n",
    "Many tasks require **human judgment**—for instance, deciding if a response is polite, if an image is suitable for children, or if an answer is factually correct. Pure automatic rewards can be insufficient or too simplistic. RLHF effectively **outsources** part of the reward design to humans, who can judge or rank outputs. As the model sees more examples of what is \"approved\" versus what is \"not approved,\" it aligns its internal policy to produce the sorts of outputs that people prefer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "history",
   "metadata": {
    "id": "history"
   },
   "source": [
    "# Brief History and Invention of RLHF\n",
    "RLHF builds on the foundational ideas of **reinforcement learning** (from the 1970s-1980s) and **preference learning** (where a function is learned from preference comparisons). Over the last decade, it became prominent for large language models to ensure they generate **more human-aligned** responses. The popular usage in tools like **GPT** has shown how training a model with feedback (through advanced RL algorithms) can significantly improve user satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying_tech",
   "metadata": {
    "id": "underlying_tech"
   },
   "source": [
    "# Underlying Technology\n",
    "1. **Supervised Fine-Tuning**: Start with a pretrained model. Fine-tune it on high-quality data.\n",
    "2. **Reward Model**: Train a separate model to predict human preference scores or rankings.\n",
    "3. **RL Optimization (e.g., PPO)**: Update the main model's parameters to **maximize** the reward model's score for each output.\n",
    "4. **Safety and Alignment**: Techniques such as KL-regularization and carefully curated instructions are used to ensure the model doesn't drift into undesired behaviors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "math_explanation",
   "metadata": {
    "id": "math_explanation"
   },
   "source": [
    "# Mathematical Explanation\n",
    "1. We have a function that measures how good an answer is, based on what humans said. We call that a *reward*. Our model tries different answers. If the reward is high, it will make similar answers next time. If the reward is low, it will avoid those answers.\n",
    "2. **More Formally**: We have a policy $\\pi_\\phi(y|x)$ that gives the probability of generating answer $y$ given an input $x$. We also have a reward function $R_\\theta(x,y)$. We want to choose $\\phi$ (the parameters of the model) to maximize the expected reward:\n",
    "$$\n",
    "\\max_{\\phi} \\mathbb{E}_{x \\sim D, y \\sim \\pi_\\phi(x)}[ R_\\theta(x, y) ].\n",
    "$$\n",
    "3. **Tie to Observations**: Humans provide the ground truth for the reward function by labeling or ranking outputs. That means the reward model is trained on data like: \"Given output A and output B for the same question, which one did humans prefer?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illustrative_example",
   "metadata": {
    "id": "illustrative_example"
   },
   "source": [
    "# Illustrative Example with Code\n",
    "In this simplified example, we'll imagine we have a small set of **text replies** to user questions, and humans have labeled which replies they prefer. We'll train:\n",
    "1. A **Reward Model** that takes a text and outputs a score.\n",
    "2. A simple **Policy** (which might just produce a short piece of text) that tries to maximize that reward.\n",
    "\n",
    "*Note:* This is a toy example to illustrate the concepts. Real RLHF typically uses large language models and advanced reinforcement learning methods like PPO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mock_calculations",
   "metadata": {
    "id": "mock_calculations"
   },
   "source": [
    "## Example Calculations\n",
    "- **Weights (W)** and **Biases (b)** in a neural network are the numbers that get updated during training. They help the model represent **how** to convert an input (like a piece of text) into an output (like a reward score).\n",
    "- **Forward Pass**: We pass an input through the network layers, multiply by weights, add biases, and apply activation functions to get predictions.\n",
    "- **Backward Pass** (a.k.a. **Backpropagation**): We calculate how \"off\" our prediction was from the target (the feedback). We then adjust the weights and biases to minimize that error (or maximize reward).\n",
    "- **Loss Function**: In reward modeling, this might be a *ranking loss*, or in RL, an RL objective. The point is to define how we measure \"good\" or \"bad\" predictions so we can update the model.\n",
    "\n",
    "For instance, if a single input sample says:\n",
    "\"For question 'Q', between answer A and answer B, humans preferred B.\"\n",
    "We might want the network to produce `score(B) > score(A)`. If `score(A)` is 0.8 and `score(B)` is 0.5, we have an error. We'll adjust parameters to push `score(B)` higher next time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step_by_step",
   "metadata": {
    "id": "step_by_step"
   },
   "source": [
    "## Step-by-Step Example with a Public Dataset\n",
    "For simplicity, let's make a tiny artificial dataset of possible answers to a single question and which ones humans prefer. Our question might be: \"What is the capital of France?\"\n",
    "\n",
    "### Toy Data\n",
    "We have the following answers:\n",
    "1. **\"Paris\"** - Humans love this answer (correct, concise).\n",
    "2. **\"paris\"** - Humans like this but slightly less due to style.\n",
    "3. **\"The capital of France is Paris.\"** - Also good, maybe even better style.\n",
    "4. **\"I think it might be Berlin?\"** - Humans dislike this (incorrect).\n",
    "5. **\"Paris, obviously!\"** - Good, but maybe too informal.\n",
    "\n",
    "Humans rank them as something like:\n",
    "```\n",
    "Ranking: 3 > 1 > 2 > 5 >>> 4\n",
    "```\n",
    "Meaning #3 is best, #1 is second best, #2 is third best, #5 is fourth, and #4 is last. We'll use these to train a tiny reward model. Then we'll have a policy that, given the question, tries to produce the best answer (highest reward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55o4GoRoezoA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2467,
     "status": "ok",
     "timestamp": 1737809082779,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "execution_count": null,
    "id": "55o4GoRoezoA",
    "outputId": "e30b11e8-5df2-444b-9df3-d92698bcbb95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200, Loss: 0.0000\n",
      "Epoch 100/200, Loss: 0.0000\n",
      "Epoch 150/200, Loss: 0.0000\n",
      "Epoch 200/200, Loss: 0.0000\n",
      "\n",
      "Final learned scores for each answer:\n",
      "Answer 0: 'Paris' -> Score: 0.806\n",
      "Answer 1: 'paris' -> Score: 0.700\n",
      "Answer 2: 'The capital of France is Paris.' -> Score: 0.951\n",
      "Answer 3: 'I think it might be Berlin?' -> Score: 0.187\n",
      "Answer 4: 'Paris, obviously!' -> Score: 0.349\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Step 1: Create a toy dataset\n",
    "answers = [\n",
    "    \"Paris\",\n",
    "    \"paris\",\n",
    "    \"The capital of France is Paris.\",\n",
    "    \"I think it might be Berlin?\",\n",
    "    \"Paris, obviously!\"\n",
    "]\n",
    "\n",
    "# Humans rank them: 3 > 1 > 2 > 5 >>> 4\n",
    "# We'll convert that into a list of pairs (A_i, A_j) where A_i should have higher reward than A_j.\n",
    "# 3 > 1, 3 > 2, 3 > 5, 3 > 4, 1 > 2, 1 > 5, 1 > 4, 2 > 5, 2 > 4, 5 > 4\n",
    "\n",
    "ranking_pairs = [\n",
    "    (2, 0),  # index 2 better than index 0\n",
    "    (2, 1),\n",
    "    (2, 4),\n",
    "    (2, 3),\n",
    "    (0, 1),\n",
    "    (0, 4),\n",
    "    (0, 3),\n",
    "    (1, 4),\n",
    "    (1, 3),\n",
    "    (4, 3)\n",
    "]\n",
    "\n",
    "# Step 2: Create a simple reward model\n",
    "# We'll represent each answer with an embedding (just random for now),\n",
    "# and use a small linear layer to produce a scalar reward.\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, vocab_size=50, embedding_dim=8):\n",
    "        super().__init__()\n",
    "        # We'll pretend each answer is an index and we have an embedding for each.\n",
    "        # This is just to illustrate.\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is an integer index of the answer\n",
    "        emb = self.embeddings(x)\n",
    "        # shape of emb is (batch_size, embedding_dim)\n",
    "        score = self.linear(emb)\n",
    "        # shape of score is (batch_size, 1)\n",
    "        return score\n",
    "\n",
    "# Let's map each answer to an integer ID in this toy setting\n",
    "answer_to_id = {\n",
    "    0: 10, # just arbitrary IDs, but unique for each answer\n",
    "    1: 11,\n",
    "    2: 12,\n",
    "    3: 13,\n",
    "    4: 14\n",
    "}\n",
    "\n",
    "reward_model = RewardModel(vocab_size=100, embedding_dim=8)\n",
    "optimizer = optim.Adam(reward_model.parameters(), lr=0.01)\n",
    "\n",
    "# Step 3: Define a ranking loss.\n",
    "# We'll use a simple margin ranking loss: we want R(A_i) > R(A_j) by some margin.\n",
    "\n",
    "margin = 0.1\n",
    "ranking_loss_fn = nn.MarginRankingLoss(margin=margin)\n",
    "\n",
    "# Step 4: Train the reward model\n",
    "\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for better_idx, worse_idx in ranking_pairs:\n",
    "        # Prepare the input\n",
    "        better_id = torch.LongTensor([answer_to_id[better_idx]])\n",
    "        worse_id = torch.LongTensor([answer_to_id[worse_idx]])\n",
    "\n",
    "        # Forward pass\n",
    "        better_score = reward_model(better_id)\n",
    "        worse_score = reward_model(worse_id)\n",
    "\n",
    "        # For margin ranking loss: we want better_score to be at least margin above worse_score.\n",
    "        # target = +1 indicates better_score should be > worse_score.\n",
    "        target = torch.ones(better_score.shape) # Change this line to match the shape of better_score and worse_score\n",
    "        loss = ranking_loss_fn(better_score, worse_score, target)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "\n",
    "# Now we can see what the model scores each answer\n",
    "print(\"\\nFinal learned scores for each answer:\")\n",
    "for i, ans in enumerate(answers):\n",
    "    ans_id = torch.LongTensor([answer_to_id[i]])\n",
    "    score = reward_model(ans_id).item()\n",
    "    print(f\"Answer {i}: '{ans}' -> Score: {score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nPo4Nov2ezoA",
   "metadata": {
    "id": "nPo4Nov2ezoA"
   },
   "source": [
    "### Interpretation\n",
    "Ideally, the reward model will score the best answer (index 2) highest, then index 0, then index 1, etc., following our ranking. This is a toy approach, so it might not be perfect, but it should hopefully reflect the ranking we taught it.\n",
    "\n",
    "Next, let's imagine we have a **policy** that tries to pick which answer to give. We'll do something extremely simplified: the policy is just a set of learnable parameters that produce a **probability** of picking each answer. We'll then try to **maximize the expected reward** according to our reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8kP5epybezoA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 828,
     "status": "ok",
     "timestamp": 1737809089007,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "execution_count": null,
    "id": "8kP5epybezoA",
    "outputId": "70551dfc-62d2-40c9-abc0-8aacfdb7fb84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Policy probabilities:\n",
      "Answer 0: 'Paris' -> P = 0.329\n",
      "Answer 1: 'paris' -> P = 0.156\n",
      "Answer 2: 'The capital of France is Paris.' -> P = 0.393\n",
      "Answer 3: 'I think it might be Berlin?' -> P = 0.062\n",
      "Answer 4: 'Paris, obviously!' -> P = 0.060\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SimplePolicy(nn.Module):\n",
    "    def __init__(self, num_answers=5):\n",
    "        super().__init__()\n",
    "        # We'll keep a raw score for each answer\n",
    "        self.logits = nn.Parameter(torch.zeros(num_answers))\n",
    "\n",
    "    def forward(self):\n",
    "        # Convert logits to probabilities\n",
    "        probs = F.softmax(self.logits, dim=-1)\n",
    "        return probs\n",
    "\n",
    "policy = SimplePolicy()\n",
    "policy_optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    policy_optimizer.zero_grad()\n",
    "\n",
    "    # The policy picks an answer i with probability p_i\n",
    "    # The expected reward = sum_i p_i * R(answer_i)\n",
    "    probs = policy()\n",
    "    # Compute the reward for each answer from the reward model\n",
    "    rewards = []\n",
    "    for i in range(len(answers)):\n",
    "        ans_id = torch.LongTensor([answer_to_id[i]])\n",
    "        r = reward_model(ans_id)\n",
    "        rewards.append(r)\n",
    "    # Stack them\n",
    "    rewards = torch.stack(rewards).squeeze()\n",
    "    # Expected reward\n",
    "    expected_reward = torch.sum(probs * rewards)\n",
    "\n",
    "    # We want to MAXIMIZE the reward, but PyTorch by default MINIMIZES.\n",
    "    # So we take negative.\n",
    "    loss = -expected_reward\n",
    "    loss.backward()\n",
    "    policy_optimizer.step()\n",
    "\n",
    "# Now see what probabilities the policy assigns:\n",
    "print(\"\\nPolicy probabilities:\")\n",
    "final_probs = policy().detach().numpy()\n",
    "for i, ans in enumerate(answers):\n",
    "    print(f\"Answer {i}: '{ans}' -> P = {final_probs[i]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Gm4lRp6UezoA",
   "metadata": {
    "id": "Gm4lRp6UezoA"
   },
   "source": [
    "After training, the policy should (in theory) put most of its probability on the answers that the **reward model** scored highest. In a real RLHF setting with a large language model, we'd generate a wide variety of possible responses and use an algorithm like PPO to update the model parameters to favor those that get higher reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illustrative_problem",
   "metadata": {
    "id": "illustrative_problem"
   },
   "source": [
    "# What Illustrative Problem Does RLHF Solve?\n",
    "It solves the challenge of capturing **human preferences** in tasks where it's difficult to encode a purely \"automatic\" reward function. By systematically using human judgments, we can guide models to produce outputs more aligned with what humans want—like polite chatbot responses, accurate question answering, or creative writing with a certain style."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "real_world_problem",
   "metadata": {
    "id": "real_world_problem"
   },
   "source": [
    "# What Real-World Problem Does RLHF Solve?\n",
    "In many real applications, we want AI to produce not just **technically correct** answers, but answers that consider context, ethics, politeness, and user experience. For example, a **virtual assistant** might provide correct answers but in an unfriendly tone. By applying RLHF, we can fine-tune the assistant's style and content so it becomes not only **correct** but also **helpful and pleasant** to interact with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solving_real_world",
   "metadata": {
    "id": "solving_real_world"
   },
   "source": [
    "# How to Solve a Real-World Problem Using RLHF\n",
    "1. **Identify the Task**: e.g., building a helpful customer service chatbot.\n",
    "2. **Gather Human Feedback**: Let real users chat with a prototype, and label good vs. bad responses.\n",
    "3. **Train a Reward Model**: Use these labels or preference rankings to train a model that scores the chatbot's responses.\n",
    "4. **Use RL to Optimize**: Use an RL algorithm (like PPO) to adjust the chatbot's parameters, so it consistently generates high-scoring (preferred) responses.\n",
    "5. **Validate and Iterate**: Keep collecting more feedback to refine the reward model and further align the chatbot.\n",
    "\n",
    "This ensures your solution is grounded in actual human preferences, which are often more complex and subtle than simple correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "questions_ponder",
   "metadata": {
    "id": "questions_ponder"
   },
   "source": [
    "# Questions to Ponder & Their Answers (with code examples)\n",
    "\n",
    "1. **How do we handle conflicting human labels?**\n",
    "   - We can treat them as noisy labels and use techniques like majority voting, or we can model labelers individually using Bayesian methods. Essentially, we allow for some disagreement and try to find a consensus or distribution of preferences.\n",
    "2. **How do we make sure the model doesn’t exploit the reward model’s weaknesses?**\n",
    "   - Techniques like **reward model regularization**, **adversarial training**, or restricting how far the policy can move from its initial state (like KL-regularization in PPO) help mitigate reward hacking.\n",
    "3. **Could the model overfit to the training data of preferences?**\n",
    "   - Yes, overfitting is possible. We need validation sets of preferences and continuous user feedback. Cross-validation or further user studies can help measure real-world performance.\n",
    "4. **What if the human feedback is biased or unrepresentative?**\n",
    "   - This can cause the model to learn skewed behaviors. Collecting feedback from diverse, carefully screened groups can help mitigate bias.\n",
    "5. **Can RLHF be used in other domains beyond NLP (Natural Language Processing)?**\n",
    "   - Absolutely. RLHF can be used for robotics (humans rank certain movements or decisions) or recommendation systems (humans provide direct feedback on recommended items)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "easy_code_sample",
   "metadata": {
    "id": "easy_code_sample"
   },
   "source": [
    "# A Sample Exercise\n",
    "\n",
    "Below is a **simplified** code snippet where you need to complete the **TODO** items. This code demonstrates a basic ranking-based training loop. You can adapt it for your own RLHF experiments.\n",
    "\n",
    "## Instructions\n",
    "1. Read through the code.\n",
    "2. Complete the **TODO** sections.\n",
    "3. Run the code to see how the model's parameters update.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfclMM9WezoB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 289,
     "status": "ok",
     "timestamp": 1737809231792,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "dfclMM9WezoB",
    "outputId": "5abea8aa-e2fe-4b72-9e62-c74aaef1d1a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.5033\n",
      "Epoch 20, Loss: 0.0077\n",
      "Epoch 30, Loss: 0.0000\n",
      "Epoch 40, Loss: 0.0000\n",
      "Epoch 50, Loss: 0.0000\n",
      "\n",
      "Learned reward scores:\n",
      "Answer 0 -> Score: 0.890\n",
      "Answer 1 -> Score: 0.174\n",
      "Answer 2 -> Score: -0.497\n"
     ]
    }
   ],
   "source": [
    "#  Code Sample for Students to Complete\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "###############################################\n",
    "# TODO 1: Create your own simple dataset\n",
    "# Hint: Make a list of pairs (better_answer, worse_answer)\n",
    "# for instance, dataset = [(0, 1), (0, 2)] etc.\n",
    "# indicating answer 0 is better than answer 1, etc.\n",
    "###############################################\n",
    "dataset = [ # Replace with your own pairs\n",
    "    # (better_idx, worse_idx)\n",
    "    (0, 1),\n",
    "    (1, 2),\n",
    "    (0, 2)\n",
    "]\n",
    "\n",
    "###############################################\n",
    "# TODO 2: Create a model that has an embedding layer and a linear layer\n",
    "# to predict a scalar reward.\n",
    "# Name it MyRewardModel.\n",
    "###############################################\n",
    "class MyRewardModel(nn.Module):\n",
    "    def __init__(self, num_answers=3, embed_dim=5):\n",
    "        super().__init__()\n",
    "        # TODO: define your embedding and linear layers\n",
    "        self.embedding = nn.Embedding(num_answers, embed_dim)\n",
    "        self.linear = nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: pass the input through embedding and linear\n",
    "        emb = self.embedding(x)\n",
    "        score = self.linear(emb)\n",
    "        return score\n",
    "\n",
    "# We'll pretend we have 3 possible answers. Indices 0, 1, 2.\n",
    "model = MyRewardModel(num_answers=3, embed_dim=5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# We'll use a margin ranking loss.\n",
    "ranking_loss = nn.MarginRankingLoss(margin=0.5)\n",
    "\n",
    "# Change this line to match the shape of better_score and worse_score\n",
    "# target = torch.tensor([1.0])  # Original line - Incorrect shape\n",
    "target = torch.tensor([1.0]).view_as(better_score)\n",
    "\n",
    "\n",
    "# We'll do a few epochs of training\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for (better_idx, worse_idx) in dataset:\n",
    "        better_tensor = torch.LongTensor([better_idx])\n",
    "        worse_tensor = torch.LongTensor([worse_idx])\n",
    "\n",
    "        better_score = model(better_tensor)\n",
    "        worse_score = model(worse_tensor)\n",
    "\n",
    "        loss = ranking_loss(better_score, worse_score, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "print(\"\\nLearned reward scores:\")\n",
    "for i in range(3):\n",
    "    score = model(torch.LongTensor([i])).item()\n",
    "    print(f\"Answer {i} -> Score: {score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "glossary",
   "metadata": {
    "id": "glossary"
   },
   "source": [
    "# Glossary\n",
    "- **RL (Reinforcement Learning)**: A type of machine learning where an agent learns to take actions to maximize some reward.\n",
    "- **Human Feedback**: In RLHF, this is typically preference data (like user rankings) or explicit thumbs-up/thumbs-down signals.\n",
    "- **Reward Model**: A neural network that assigns a \"goodness\" score to outputs, trained on human feedback.\n",
    "- **Policy**: The model or agent that generates actions (like text responses) and is optimized to get high reward.\n",
    "- **Supervised Fine-Tuning (SFT)**: Training a model on a supervised dataset before applying RL.\n",
    "- **PPO (Proximal Policy Optimization)**: A popular RL algorithm often used in RLHF for stable training.\n",
    "- **KL Divergence**: A measure of how one probability distribution differs from another.\n",
    "- **Margin Ranking Loss**: A loss function that ensures a \"better\" item is ranked higher than a \"worse\" one by at least some margin.\n",
    "- **Overfitting**: When a model learns details and noise in the training data to the extent that it negatively impacts its performance on new data.\n",
    "- **Reward Hacking**: When an agent finds a way to maximize the reward function in unintended ways, exploiting loopholes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v7Oq3KvTezoB",
   "metadata": {
    "id": "v7Oq3KvTezoB"
   },
   "source": [
    "## End of Notebook\n",
    "You now have a comprehensive view of **RLHF**—from a simple sandwich analogy to the advanced mathematics and code. Feel free to experiment with these code snippets and explore the exciting world of **human-aligned AI**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "gv99dgqJezoB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1737809235171,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "gv99dgqJezoB",
    "outputId": "a46c1701-b113-48bb-abec-abbb4d8b007c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Acknowledgement +++\n",
      "Executed on: 2025-01-29 01:45:43.042371\n",
      "In Google Colab: No\n",
      "System info: Linux 6.8.0-51-generic\n",
      "Node name: alimuhammad-Inspiron-7559\n",
      "MAC address: 20:47:47:74:94:05\n",
      "IP address: 127.0.1.1\n",
      "Signing off, Ali Muhammad Asad\n"
     ]
    }
   ],
   "source": [
    "import os, sys, platform, datetime, uuid, socket\n",
    "\n",
    "def signoff(name=\"Anonymous\"):\n",
    "    colab_check = \"Yes\" if 'google.colab' in sys.modules else \"No\"\n",
    "    mac_addr = ':'.join(format((uuid.getnode() >> i) & 0xff, '02x') for i in reversed(range(0, 48, 8)))\n",
    "    print(\"+++ Acknowledgement +++\")\n",
    "    print(f\"Executed on: {datetime.datetime.now()}\")\n",
    "    print(f\"In Google Colab: {colab_check}\")\n",
    "    print(f\"System info: {platform.system()} {platform.release()}\")\n",
    "    print(f\"Node name: {platform.node()}\")\n",
    "    print(f\"MAC address: {mac_addr}\")\n",
    "    try:\n",
    "        print(f\"IP address: {socket.gethostbyname(socket.gethostname())}\")\n",
    "    except:\n",
    "        print(\"IP address: Unknown\")\n",
    "    print(f\"Signing off, {name}\")\n",
    "\n",
    "signoff(\"Ali Muhammad Asad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d6f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "name": "RLHF_Explanation.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
