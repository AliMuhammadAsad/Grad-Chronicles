{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "_kXaQOtmrrgR",
   "metadata": {
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1737812289261,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "_kXaQOtmrrgR"
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "#                                                               #\n",
    "#  CS435 Generative AI: Security, Ethics and Governance         #\n",
    "#                                                               #\n",
    "#  Instructor: Dr. Adnan Masood                                 #\n",
    "#  Contact:    adnanmasood@gmail.com                            #\n",
    "#                                                               #\n",
    "#  Notebook is MIT Licensed                                     #\n",
    "#################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7PZ6Ry_7rrgT",
   "metadata": {
    "id": "7PZ6Ry_7rrgT"
   },
   "source": [
    "# **Transformer Architecture: A Comprehensive Tutorial**\n",
    "\n",
    "Author: **Dr. Adnan Masood**\n",
    "\n",
    "References:\n",
    "- *Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright Â© 2024-2025.*\n",
    "- *Attention Is All You Need (Vaswani et al., 2017)*\n",
    "- Various open-source materials, including PyTorch documentation.\n",
    "\n",
    "---\n",
    "# Building an Intuitive Understanding\n",
    "\n",
    "We will explain the **Transformer Architecture** in **five levels** of detail. This single notebook walks you from a very light conceptual introduction all the way to an in-depth, mathematically comprehensive discussion. These five levels are:\n",
    "\n",
    "1. (Intuitive, middle-school-level language)\n",
    "2. (Slightly deeper, but still accessible)\n",
    "3. (Clear discussion of the main ideas)\n",
    "4. (Technical details and bridging to the math)\n",
    "5. (Full details with equations, derivations, and advanced insights)\n",
    "\n",
    "Each level adds more detail, culminating in code examples and conceptual expansions. By the end, you will be able to both **explain and implement** a Transformer in **PyTorch**, see how it is used to solve real-world problems, and explore Q&A with code.\n",
    "\n",
    "Let's begin!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "N-e_3UMTrrgT",
   "metadata": {
    "id": "N-e_3UMTrrgT"
   },
   "source": [
    "### Intuitive Explanation\n",
    "\n",
    "Imagine you're trying to read a sentence in another language. You want to find the best way to turn it into English so you can understand it. A **Transformer** is like a very clever friend that reads sentences in one language, learns what each word really means by looking at all the other words, and then writes it out in another language.\n",
    "\n",
    "1. We split the sentence into \"pieces\" (tokens or words) so it can read them one at a time.\n",
    "2. The **Transformer** has two main parts:\n",
    "   - An **Encoder** that reads the words.\n",
    "   - A **Decoder** that writes the translation.\n",
    "3. Inside, there's **attention**, a way for the Transformer to focus on the words that matter the most. For example, if we have a sentence \"The cat sat on the mat because it was tired,\" the word **\"it\"** might need to look closely at **\"cat\"** to know that \"it\" means \"the cat.\"\n",
    "\n",
    "By using **attention**, the Transformer can figure out how different words in a sentence connect to each other and then produce an accurate translation (or do other tasks like summarization or text generation). That's the simplest picture!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0kP5QcqOrrgT",
   "metadata": {
    "id": "0kP5QcqOrrgT"
   },
   "source": [
    "### More Detail on Concepts\n",
    "\n",
    "**The Transformer** is a type of deep learning model that excels in understanding and generating language. It's used in things like **ChatGPT**, **Google Translate**, and many other AI-based language tools.\n",
    "\n",
    "- **Encoder**: Takes a sentence (like \"Il fait beau aujourd'hui\" in French) and transforms it into a hidden representation (a bunch of numbers that the computer can understand).\n",
    "- **Decoder**: Uses that hidden representation to generate an English sentence (e.g., \"The weather is nice today\").\n",
    "\n",
    "Key ideas:\n",
    "1. **Self-Attention**: Each word in the sentence can \"attend\" to every other word. For the word \"it\" in the example above, the model looks at all other words to see which word \"it\" should refer to.\n",
    "2. **Positional Encoding**: Transformers don't process words one after another in a chain (like older RNNs do). Instead, they process them all at once. We still need to tell the model which word is first, second, etc. *Positional encoding* does this by adding specific patterns of numbers to the word representations.\n",
    "3. **Multi-Head Attention**: Instead of using just one attention map, we use multiple. Imagine multiple sets of eyes looking at the sentence in different ways, each capturing some nuance of meaning.\n",
    "\n",
    "Overall, the Transformer model can read the entire sentence at once, figure out important relationships, and then generate a whole new sentence in the target language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YKzBxBYfrrgU",
   "metadata": {
    "id": "YKzBxBYfrrgU"
   },
   "source": [
    "### The Architecture Explained\n",
    "\n",
    "Consider a sentence of length $N$. We break it up into tokens (words/pieces). The Transformer has:\n",
    "\n",
    "1. **Input Embeddings**: Convert each token into a vector of real numbers (dimension $d$).\n",
    "2. **Positional Encodings**: Add a positional encoding to each token embedding to mark its position in the sequence.\n",
    "3. **Encoder Stack**: $N$ tokens go into the bottom encoder. Each encoder has:\n",
    "   - A **Multi-Head Self-Attention** sub-layer: the model learns to pay attention to parts of the input.\n",
    "   - A **Feed-Forward Network (FFN)** sub-layer: a small neural network that processes each position independently.\n",
    "   - **Residual** connections and **Layer Normalization** around each sub-layer.\n",
    "4. **Decoder Stack**: Takes the previously generated tokens as input and does almost the same steps, but also has an **\"encoder-decoder attention\"** that looks at the output of the **encoder stack**.\n",
    "5. **Final Linear + Softmax**: The decoder output is turned into a probability distribution over all possible words in the vocabulary.\n",
    "\n",
    "### Self-Attention in a Nutshell\n",
    "For each word, we create 3 vectors:\n",
    "- **Query (Q)**\n",
    "- **Key (K)**\n",
    "- **Value (V)**\n",
    "\n",
    "All words produce Q, K, and V. The attention for a given word is computed by:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "where $d_k$ is the dimension of the key vectors. This gives us a weighted sum of the values, with weights determined by how well the query matches the keys.\n",
    "\n",
    "### Multi-Head Attention\n",
    "We do that self-attention multiple times in parallel (heads). Each head learns to pay attention to different relationships or patterns in the sentence.\n",
    "\n",
    "### Why Transformers?\n",
    "- They allow parallel processing of the entire sentence.\n",
    "- Self-attention can capture long-range dependencies better than older RNN-based approaches.\n",
    "- Achieves **state-of-the-art** results on many language tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HGQ2Rux2rrgU",
   "metadata": {
    "id": "HGQ2Rux2rrgU"
   },
   "source": [
    "### A Bit More Math and Detail\n",
    "\n",
    "The model dimension is $d$, typically 512 or higher. Suppose we have an input sequence (tokens) of length $N$. We embed them into a matrix $X \\in \\mathbb{R}^{N \\times d}$.\n",
    "\n",
    "We add **positional encodings** $P$ to get the final input to the first encoder:\n",
    "$$\n",
    "X_0 = X + P, \\quad X_0 \\in \\mathbb{R}^{N \\times d}.\n",
    "$$\n",
    "\n",
    "**Encoder**:\n",
    "\n",
    "Each encoder layer $\\ell$ (for $\\ell = 1, ..., L$) has:\n",
    "\n",
    "1. **Layer Normalization** on the input:\n",
    "$$\n",
    "T_1 = \\text{LayerNorm}(X_{\\ell-1}).\n",
    "$$\n",
    "2. **Multi-Head Self-Attention**:\n",
    "$$\n",
    "\\text{MHA}(T_1) = \\left(\\sum_{h=1}^H \\text{softmax}\\left(\\frac{Q^h {K^h}^T}{\\sqrt{d_k}}\\right) V^h \\right) W^O,\n",
    "$$\n",
    "   where each head $h$ has its own projection matrices $W^h_Q, W^h_K, W^h_V$ to get Q, K, V.\n",
    "\n",
    "3. **Add & Norm** (a residual connection):\n",
    "$$\n",
    "Z = X_{\\ell-1} + \\text{MHA}(T_1).\n",
    "$$\n",
    "4. Another **Layer Normalization**:\n",
    "$$\n",
    "T_2 = \\text{LayerNorm}(Z).\n",
    "$$\n",
    "5. **Feed-Forward Network** (FFN):\n",
    "$$\n",
    "\\text{FFN}(T_2) = \\max(0, T_2 W_1 + b_1) W_2 + b_2.\n",
    "$$\n",
    "6. **Add & Norm** again:\n",
    "$$\n",
    "X_{\\ell} = Z + \\text{FFN}(T_2).\n",
    "$$\n",
    "\n",
    "Hence, we get the next layer's input. After $L$ layers, the final output of the encoder stack is $X_L$.\n",
    "\n",
    "**Decoder** has a similar structure, with the modification:\n",
    "- We apply **Masked** Multi-Head Self-Attention (so the decoder cannot peek at future tokens in the output sequence).\n",
    "- We have an extra multi-head attention that uses the **encoder output** as the keys and values, and the decoder hidden states as queries.\n",
    "\n",
    "### Training\n",
    "We use a **cross-entropy loss** across the entire sequence. For each position in the output sentence, we want the correct word to have the highest probability.\n",
    "\n",
    "### Complexity\n",
    "- Self-attention is $O(N^2 \\times d)$. This is why it's sometimes expensive for large sequences, but faster in practice than very deep recurrent networks for many tasks.\n",
    "\n",
    "### Summary So Far\n",
    "- The Transformer is a stack of encoders and decoders.\n",
    "- Each uses (1) multi-head self-attention, (2) feed-forward, (3) residual connections, (4) layer normalization.\n",
    "- The decoder is also conditioned on the encoder outputs and prevents attending to future output words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T6vAaPtGrrgU",
   "metadata": {
    "id": "T6vAaPtGrrgU"
   },
   "source": [
    "### Under-the-Hood Details\n",
    "\n",
    "1. **Learned Embeddings** $E$: Typically dimension $d$. For a vocabulary size $|V|$, $E \\in \\mathbb{R}^{|V| \\times d}$. Input tokens become rows in $E$.\n",
    "\n",
    "2. **Positional Encoding**: Original paper uses:\n",
    "$$\n",
    "PE(pos, 2i) = \\sin \\left(pos / 10^{4i/d} \\right), \\quad PE(pos, 2i+1) = \\cos \\left(pos / 10^{4i/d}\\right).\n",
    "$$\n",
    "\n",
    "3. **Scaled Dot-Product**:\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V.\n",
    "$$\n",
    "   - $Q = X W^Q$, $K = X W^K$, $V = X W^V$.\n",
    "   - $Q, K, V \\in \\mathbb{R}^{N \\times d_k}$\n",
    "   - Then we do multi-head by replicating this with separate parameters for each head.\n",
    "\n",
    "4. **Masking**: For decoding, we apply a causal mask so the output at time $t$ can only attend to earlier times (1 to $t-1$), not future times.\n",
    "\n",
    "5. **Final Linear**: We often share embedding weights with the final linear (\"unembedding\") step (weight tying), ensuring a large matrix doesn't blow up parameters.\n",
    "\n",
    "6. **Optimization**: Often we use the **Adam** or **Adafactor** optimizer with warm-up learning rates.\n",
    "\n",
    "7. **Implementation**: In large language models, we sometimes have many modifications, but the central concept remains the same.\n",
    "\n",
    "That's the fundamental architecture from a deeper perspective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SecCn-HFrrgU",
   "metadata": {
    "id": "SecCn-HFrrgU"
   },
   "source": [
    "---\n",
    "# **Intuition and Illustrative Examples**\n",
    "\n",
    "### Example: Short Sentence Translation\n",
    "\n",
    "Suppose we want to translate from French to English:\n",
    "\n",
    "1. We have **Input**: \"Le chat dort\" (meaning \"The cat sleeps\").\n",
    "2. The **Encoder** transforms [\"Le\", \"chat\", \"dort\"] into some high-level representation.\n",
    "3. The **Decoder** uses these representations to generate [\"The\", \"cat\", \"is\", \"sleeping\"] or something close.\n",
    "\n",
    "Here is the overall flow:\n",
    "1. Each French word is turned into an embedding: `[[LE_vec], [CHAT_vec], [DORT_vec]]`.\n",
    "2. The encoder layers apply **self-attention** so each word's encoding can see the other words.\n",
    "3. The final encoder output is passed to the decoder.\n",
    "4. The decoder does masked self-attention on the partial English phrase it has generated, plus attends to the encoder's output.\n",
    "5. Finally, we get a distribution from the last linear+softmax. The highest probability word is selected as the next word.\n",
    "\n",
    "### A Brief History\n",
    "- **Before Transformers**: We had RNNs (like LSTM, GRU) for machine translation and sequence tasks. They struggled with long sequences.\n",
    "- **The Attention Mechanism** in RNN-based seq2seq improved translation quality a lot.\n",
    "- **Transformers (Vaswani et al. 2017)** removed the recurrence entirely and used only attention (plus feed-forward networks, etc.).\n",
    "- Has led to large language models like BERT, GPT, T5, etc.\n",
    "\n",
    "### Real-World Problem Solved\n",
    "- **Machine Translation**: The Transformer provides superior translations while training faster in parallel.\n",
    "- **Other**: Summarization, question-answering, text classification, code generation, and more. Essentially, Transformers are the backbone of modern LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0R1hdjRPrrgU",
   "metadata": {
    "id": "0R1hdjRPrrgU"
   },
   "source": [
    "---\n",
    "# **Example Calculations**\n",
    "\n",
    "In this mini example, let's do a small attention calculation by hand.\n",
    "\n",
    "Assume we have **2 words** in the input (for simplicity), and each embedding is 2-dimensional. Let the words be `W1=[0.5, 1.0]` and `W2=[-0.5, 0.5]`.\n",
    "\n",
    "1. We define weight matrices for Q, K, V each as 2x2 to keep it small. For instance,\n",
    "$$\n",
    "W^Q = \\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{pmatrix},\\quad\n",
    "W^K = \\begin{pmatrix}\n",
    "0.5 & 0 \\\\\n",
    "0 & 0.5\n",
    "\\end{pmatrix},\\quad\n",
    "W^V = \\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "2. For word1: Q1 = W^Q * W1 = [0.5, 1.0], K1 = W^K * W1 = [0.25, 0.5], V1 = W^V * W1 = [0.5, 1.0].\n",
    "   For word2: Q2 = W^Q * W2 = [-0.5, 0.5], K2 = W^K * W2 = [-0.25, 0.25], V2 = W^V * W2 = [-0.5, 0.5].\n",
    "\n",
    "3. Suppose we compute attention for word1 on both words:\n",
    "   - Dot product(Q1, K1) = 0.5*0.25 + 1.0*0.5 = 0.125 + 0.5 = 0.625\n",
    "   - Dot product(Q1, K2) = 0.5*(-0.25) + 1.0*0.25 = -0.125 + 0.25 = 0.125\n",
    "   - Suppose $d_k=2$, so we divide by sqrt(2) ~ 1.414. Then:\n",
    "     - score1 = 0.625/1.414 ~ 0.442\n",
    "     - score2 = 0.125/1.414 ~ 0.088\n",
    "   - Softmax: e^{0.442} ~ 1.556, e^{0.088} ~ 1.092\n",
    "     sum=2.648\n",
    "     - alpha1=1.556/2.648 ~0.587, alpha2=1.092/2.648 ~0.413\n",
    "   - Weighted sum of values: alpha1*V1 + alpha2*V2 = 0.587*[0.5,1.0] + 0.413*[-0.5,0.5] ~ [0.2935 -0.2065, 0.587 +0.2065] = [0.087, 0.7935]\n",
    "\n",
    "That final vector [0.087, 0.7935] is the self-attention output for the first word. This is an example of how the final vector might incorporate some of the second word's representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W8z_wG8prrgU",
   "metadata": {
    "id": "W8z_wG8prrgU"
   },
   "source": [
    "---\n",
    "# **Step by Step Example**: Building a Tiny Transformer from Scratch\n",
    "\n",
    "### 1. Tokenize Input\n",
    "- We assume you have some data, e.g. pairs `(\"je suis Ã©tudiant\", \"i am a student\").`\n",
    "### 2. Create Embedding Layer\n",
    "- `nn.Embedding(vocab_size, d_model)` in PyTorch.\n",
    "### 3. Positional Encoding\n",
    "- Implement a function that returns a matrix of shape `(max_len, d_model)` with sine/cosine patterns.\n",
    "### 4. Build an Encoder\n",
    "1. Self-Attention sub-layer with multi-head.\n",
    "2. Feed-Forward sub-layer.\n",
    "3. Residual + LayerNorm.\n",
    "### 5. Build a Decoder\n",
    "1. Masked Self-Attention.\n",
    "2. Encoder-Decoder Attention.\n",
    "3. Feed-Forward.\n",
    "4. Residual + LayerNorm.\n",
    "### 6. Final Linear + Softmax\n",
    "- For each decoder output position, produce distribution over vocabulary.\n",
    "### 7. Loss Function (Cross Entropy)\n",
    "### 8. Optimizer (Adam)\n",
    "### 9. Train\n",
    "- For each batch, run forward pass, compute loss, backprop, update parameters.\n",
    "### 10. Inference\n",
    "- Greedy or beam search for generating translations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Wps2G7akrrgU",
   "metadata": {
    "id": "Wps2G7akrrgU"
   },
   "source": [
    "# **What Problems Does This Solve?**\n",
    "\n",
    "- **Machine Translation**: High-quality, fast translation.\n",
    "- **Text Summarization**: Summarize long documents.\n",
    "- **Chatbots** / **Language Modeling**: Big LLMs are built upon Transformer blocks.\n",
    "- **Any sequence-based prediction** problem: code completion, music generation, text classification, Q&A, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dyaxRoxhrrgV",
   "metadata": {
    "id": "dyaxRoxhrrgV"
   },
   "source": [
    "# **How to Solve a Real-World Problem**\n",
    "\n",
    "1. **Obtain Data**: For example, a parallel corpus of text in two languages.\n",
    "2. **Preprocess and Tokenize**: Convert each sentence into tokens, build vocabulary.\n",
    "3. **Create/Load a Transformer**: Implementation from scratch or from libraries like PyTorch.\n",
    "4. **Training**:\n",
    "   - Write a training loop.\n",
    "   - Use Adam optimizer, cross-entropy loss.\n",
    "   - Possibly use GPUs.\n",
    "5. **Evaluate**: Compare with known references. For translation, measure BLEU or similar metrics.\n",
    "6. **Deployment**: Save the trained model. In your application, encode input text, run the decoder to generate output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72tfXCaLrrgV",
   "metadata": {
    "id": "72tfXCaLrrgV"
   },
   "source": [
    "# **Points to Ponder** (Questions)\n",
    "1. How do we handle words outside of the vocabulary? (Answer: subword tokenization, or use special <UNK> tokens.)\n",
    "2. What about very long sequences? (Answer: Transformers can be extended with special techniques to handle longer contexts, e.g. sparse attention, or efficient variants.)\n",
    "3. Does the Transformer only do machine translation? (Answer: No, it can do many tasks: summarization, question answering, text generation, etc.)\n",
    "4. How do we pick the number of heads, layers, etc.? (Answer: Typically a hyperparameter search or by referencing large standard models like GPT, T5, BERT which have known configurations.)\n",
    "5. How is training speed improved over RNNs? (Answer: Parallelization. The entire sequence is processed at once, as opposed to step-by-step in RNNs.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Y8peY6x6rrgV",
   "metadata": {
    "id": "Y8peY6x6rrgV"
   },
   "source": [
    "# **Answers to the Points to Ponder**\n",
    "1. **Handling Out-of-Vocabulary**: We use subword approaches (e.g., Byte Pair Encoding) that break words into smaller subwords, so almost all tokens are covered.\n",
    "2. **Very Long Sequences**: We can employ specialized architectures or chunking. Recent research addresses memory overhead (e.g., \"Longformer\", \"Big Bird\", etc.).\n",
    "3. **Beyond MT**: The same architecture is used for chatbots, code generation, summarization, and so on.\n",
    "4. **Hyperparameters**: Often chosen empirically. Common dimension sizes are 256, 512, or 1024, with multiple heads (e.g., 8 or 16 heads) and anywhere from 6 to 96 layers.\n",
    "5. **Parallelization**: Self-attention operates across the entire input in parallel, leading to faster training on modern hardware.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wz_8x73hrrgV",
   "metadata": {
    "id": "wz_8x73hrrgV"
   },
   "source": [
    "# **A Sample Exercise**\n",
    "\n",
    "Here is an **easy PyTorch code sample** to illustrate the main concepts. This example is not a fully optimized production code, but a conceptual illustration. We'll define a small Transformer and run it on synthetic data.\n",
    "\n",
    "## **TODO Items**:\n",
    "1. Modify the dimension sizes (e.g., `d_model` or `nhead`), see how that changes the model parameters.\n",
    "2. Experiment with the sequence length to see the effect on training time.\n",
    "3. Try adding a small positional encoding function.\n",
    "\n",
    "Let's proceed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "CCm-3h1vrrgV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {},
    "executionInfo": {
     "elapsed": 23132,
     "status": "ok",
     "timestamp": 1737812312724,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "CCm-3h1vrrgV",
    "outputId": "02b4b9c7-d6db-4af6-e266-c22e59463d9b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nightwing/.local/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2/10, Loss=3.1810\n",
      "Step 4/10, Loss=3.0239\n",
      "Step 6/10, Loss=3.0087\n",
      "Step 8/10, Loss=3.0242\n",
      "Step 10/10, Loss=2.9732\n",
      "\n",
      "Training complete. This is a demonstration of how to set up a PyTorch Transformer.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Modify code as needed.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# For demonstration: tiny example with minimal code.\n",
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size=20, d_model=16, nhead=2, num_encoder_layers=2, num_decoder_layers=2):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        # Embedding for tokens\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # We will use PyTorch's built-in Transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=64, # smaller feed-forward for demonstration\n",
    "            dropout=0.1\n",
    "        )\n",
    "\n",
    "        # Final linear layer\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # src: (S, N) shape [sequence_len, batch_size]\n",
    "        # tgt: (T, N)\n",
    "        # Let's embed them\n",
    "        embedded_src = self.embedding(src) * (self.d_model ** 0.5)\n",
    "        embedded_tgt = self.embedding(tgt) * (self.d_model ** 0.5)\n",
    "\n",
    "        # For the built-in Transformer, we want shape [sequence_len, batch_size, d_model]\n",
    "        # embedded_src and embedded_tgt are [S, N, d_model]\n",
    "        embedded_src = embedded_src\n",
    "        embedded_tgt = embedded_tgt\n",
    "\n",
    "        # We can create source and target masks (e.g. causal mask) if we want\n",
    "        # But let's skip that for the simplest demonstration\n",
    "\n",
    "        # pass through the transformer\n",
    "        out = self.transformer(\n",
    "            src=embedded_src,\n",
    "            tgt=embedded_tgt\n",
    "        )  # out shape [T, N, d_model]\n",
    "\n",
    "        # final linear\n",
    "        logits = self.fc_out(out)  # shape [T, N, vocab_size]\n",
    "        return logits\n",
    "\n",
    "# Let's run a tiny example training loop with synthetic data.\n",
    "def generate_fake_data(num_samples=64, seq_len=5, vocab_size=20):\n",
    "    # returns (src, tgt), each shape (seq_len, num_samples)\n",
    "    src = torch.randint(0, vocab_size, (seq_len, num_samples))\n",
    "    tgt = torch.randint(0, vocab_size, (seq_len, num_samples))\n",
    "    return src, tgt\n",
    "\n",
    "def train_tiny_transformer(model, vocab_size=20, steps=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    for step in range(steps):\n",
    "        src, tgt = generate_fake_data()\n",
    "        # let's say we want the model to shift the target by one, just to have a training objective\n",
    "        # typical of sequence to sequence. We'll feed the entire tgt but compare with next-step.\n",
    "        # For the sake of demonstration, we simply do a random shift.\n",
    "        # In reality, you'd set up your training data properly.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(src, tgt[:-1, :])  # input all but last token, predict next token\n",
    "        # logits shape: [T, N, vocab_size], T=seq_len-1\n",
    "        # we want to compare with the 'gold' next token, which is: tgt[1:,:]\n",
    "\n",
    "        # flatten them for cross entropy\n",
    "        # logits => [ (T*N), vocab_size ]\n",
    "        # target => [ T*N ]\n",
    "        Tdim, Ndim, _ = logits.shape\n",
    "        loss = criterion(logits.view(Tdim*Ndim, -1), tgt[1:,:].view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (step+1) % 2 == 0:\n",
    "            print(f\"Step {step+1}/{steps}, Loss={loss.item():.4f}\")\n",
    "\n",
    "# Instantiate and train a tiny model\n",
    "vocab_size = 20\n",
    "model = SimpleTransformer(vocab_size=vocab_size, d_model=16, nhead=2, num_encoder_layers=1, num_decoder_layers=1)\n",
    "\n",
    "train_tiny_transformer(model, vocab_size=vocab_size, steps=10)\n",
    "\n",
    "print(\"\\nTraining complete. This is a demonstration of how to set up a PyTorch Transformer.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ueq31gYKrrgV",
   "metadata": {
    "id": "Ueq31gYKrrgV"
   },
   "source": [
    "# **Glossary**\n",
    "\n",
    "- **Attention**: A method by which a model can dynamically focus on certain parts of its input.\n",
    "- **Self-Attention**: The attention mechanism where a sequence element attends to other positions in the same sequence.\n",
    "- **Multi-Head Attention**: Using multiple sets (heads) of attention in parallel, which allows the model to capture different types of relationships.\n",
    "- **Embedding**: The mapping from discrete tokens (e.g. words) to continuous vectors.\n",
    "- **Positional Encoding**: A method to inject sequence order information into the model.\n",
    "- **Feed-Forward Layer**: A fully connected network that applies transformations position-wise.\n",
    "- **Residual Connection**: A shortcut connection that adds an input to the output of a layer, which helps training deep networks.\n",
    "- **Layer Normalization**: A method to normalize the input across features in each sample, improving training stability.\n",
    "- **Encoder**: The part of the model that processes the input tokens.\n",
    "- **Decoder**: The part of the model that produces the output tokens.\n",
    "- **Masked Self-Attention**: Self-attention in which future tokens are not visible, ensuring causality.\n",
    "- **Cross Entropy Loss**: A common loss function for classification, comparing predicted probabilities to the target distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zdnQpFVarrgV",
   "metadata": {
    "id": "zdnQpFVarrgV"
   },
   "source": [
    "# **Conclusion**\n",
    "\n",
    "We've explored the Transformer architecture in five incremental levels of detail, from a child's perspective to a PhD-level deep dive. This model revolutionized NLP and is widely used in modern large language models.\n",
    "\n",
    "In the code example, you see a small PyTorch demonstration of how the core ideas can be put into practice. Feel free to extend the code with more layers, heads, embeddings, positional encodings, etc. The Transformer is extremely versatile and has led to breakthroughs in translation, summarization, question answering, code generation, and beyond.\n",
    "\n",
    "Happy Transforming!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4pvir9BSrrgV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1737812312725,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "4pvir9BSrrgV",
    "outputId": "55816100-a047-464c-fc41-e710d31688dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Acknowledgement +++\n",
      "Executed on: 2025-01-29 01:31:05.772474\n",
      "In Google Colab: No\n",
      "System info: Linux 6.8.0-51-generic\n",
      "Node name: alimuhammad-Inspiron-7559\n",
      "MAC address: 20:47:47:74:94:05\n",
      "IP address: 127.0.1.1\n",
      "Signing off, Ali Muhammad Asad\n"
     ]
    }
   ],
   "source": [
    "import os, sys, platform, datetime, uuid, socket\n",
    "\n",
    "def signoff(name=\"Anonymous\"):\n",
    "    colab_check = \"Yes\" if 'google.colab' in sys.modules else \"No\"\n",
    "    mac_addr = ':'.join(format((uuid.getnode() >> i) & 0xff, '02x') for i in reversed(range(0, 48, 8)))\n",
    "    print(\"+++ Acknowledgement +++\")\n",
    "    print(f\"Executed on: {datetime.datetime.now()}\")\n",
    "    print(f\"In Google Colab: {colab_check}\")\n",
    "    print(f\"System info: {platform.system()} {platform.release()}\")\n",
    "    print(f\"Node name: {platform.node()}\")\n",
    "    print(f\"MAC address: {mac_addr}\")\n",
    "    try:\n",
    "        print(f\"IP address: {socket.gethostbyname(socket.gethostname())}\")\n",
    "    except:\n",
    "        print(\"IP address: Unknown\")\n",
    "    print(f\"Signing off, {name}\")\n",
    "\n",
    "signoff(\"Ali Muhammad Asad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5950cc6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "name": "Transformer_Architecture_Explained.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
