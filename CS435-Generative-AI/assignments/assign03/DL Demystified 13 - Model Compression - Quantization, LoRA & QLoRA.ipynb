{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aICU-jlNllcT",
   "metadata": {
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1737810925749,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "aICU-jlNllcT"
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "#                                                               #\n",
    "#  CS435 Generative AI: Security, Ethics and Governance         #\n",
    "#                                                               #\n",
    "#  Instructor: Dr. Adnan Masood                                 #\n",
    "#  Contact:    adnanmasood@gmail.com                            #\n",
    "#                                                               #\n",
    "#  Notebook is MIT Licensed                                     #\n",
    "#################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NS3LUGIZllcV",
   "metadata": {
    "id": "NS3LUGIZllcV"
   },
   "source": [
    "# Quantization, LoRA, and QLoRA Explained in a Jupyter Notebook\n",
    "\n",
    "Welcome! In this notebook, we’ll explore **Quantization**, **LoRA**, and **QLoRA** in detail. We will begin with an intuitive, simple explanation, and progressively dive deeper into the technical and mathematical aspects. This notebook is structured to provide a multi-level explanation: from a very simple level to a very advanced level. We'll go step-by-step with examples and code. By the end of this notebook, you’ll have a thorough understanding of how these techniques work, why they matter, and how to apply them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Th-zO-OyllcW",
   "metadata": {
    "id": "Th-zO-OyllcW"
   },
   "source": [
    "# Table of Contents\n",
    "1. [Building an Intuitive Understanding](#five-levels)\n",
    "2. [Intuition Behind Quantization, LoRA, and QLoRA](#intuition)\n",
    "3. [Brief History and Underlying Technology](#history)\n",
    "4. [Math Behind the Techniques](#math)\n",
    "5. [Illustrative Example with Code](#example_code)\n",
    "6. [Example Calculations](#mock_calcs)\n",
    "7. [Step-by-Step Example from Scratch](#step_by_step)\n",
    "8. [Illustrative Problem It Solves](#illustrative_problem)\n",
    "9. [Real-World Problems Solved](#realworld_problem)\n",
    "10. [Using This Tech to Solve Real Problems](#use_tech)\n",
    "11. [Questions to Ponder](#questions)\n",
    "12. [Answers to the Questions (with Code)](#answers)\n",
    "13. [A Sample Exercise](#easy_sample)\n",
    "14. [Glossary](#glossary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sOeWuvq1llcW",
   "metadata": {
    "id": "sOeWuvq1llcW",
    "tags": []
   },
   "source": [
    "<a id=\"five-levels\"></a>\n",
    "# 1. Building an Intuitive Understanding\n",
    "Below is a progression of explanations for **Quantization, LoRA, and QLoRA**, each adding more detail but focusing on the same core ideas:\n",
    "\n",
    "**Quantization**: Imagine you have a big box of colored pencils with many shades. Quantization means you're picking fewer shades to color your picture—this makes your box smaller and easier to carry, but you can still draw almost the same picture.\n",
    "\n",
    "**LoRA**: Think of it like having a big puzzle, but you only add a small set of new pieces to update your puzzle’s picture. This is much easier than remaking the whole puzzle from scratch.\n",
    "\n",
    "**QLoRA**: Combine the idea of picking fewer shades (Quantization) **and** adding small puzzle pieces (LoRA) together to make learning and using a big AI model faster and cheaper.\n",
    "\n",
    "When dealing with neural networks, we store numbers (weights) in computers. Normally, we use full 32-bit numbers (floats). Quantization shrinks them to fewer bits (like 8-bit or 4-bit) to use less memory and compute faster.\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is a method to update a large model with fewer parameters. Instead of changing all the model’s weights, we factorize them into smaller matrices (low-rank) and only update those parts, saving memory and training time.\n",
    "\n",
    "QLoRA is a technique that uses 4-bit quantization for a large model while applying LoRA for fine-tuning. It drastically cuts down memory usage and speeds up training and inference.\n",
    "\n",
    "Neural networks rely on large weight matrices. Using high-precision floating-point data can be overkill. Quantization maps these continuous values to discrete sets of integers in fewer bits, e.g., using 8-bit or 4-bit integers instead of 32-bit floats. This requires some scaling and offset for each weight.\n",
    "\n",
    "LoRA decouples the main model weights from the update process. Instead of fine-tuning every single weight, we introduce low-dimensional matrices (A and B). We only learn these matrices during training, which drastically reduces the number of trainable parameters.\n",
    "\n",
    "QLoRA combines 4-bit quantization (to store and run the large base model more efficiently) with LoRA to fine-tune it on new data. It offers the benefit of minimal hardware requirements alongside parameter-efficient fine-tuning.\n",
    "\n",
    "We can view quantization as approximating the weight vector space with a smaller set of possible values. For instance, 4-bit quantization defines 2^4 = 16 possible levels. Each weight is mapped to a scaled integer representation, e.g., $w_{quant} = \\text{round}((w - \\alpha)/\\delta)$ where $\\alpha$ is an offset and $\\delta$ is the step size.\n",
    "\n",
    "If a weight matrix $W$ is dimension $d \\times d$, LoRA factorizes the update into $A$ and $B$, each with dimensions $d \\times r$ and $r \\times d$, leading to a low-rank update: $\\Delta W = A B^T$. We only train $A$ and $B$, keeping the base $W$ fixed, thereby drastically reducing the parameter count.\n",
    "\n",
    "With QLoRA, the base weights are quantized to 4 bits (using specific algorithms designed for large language models). The LoRA low-rank matrices remain in higher precision. This effectively combines memory savings of quantization with the parameter efficiency of LoRA.\n",
    "\n",
    "In large-scale LLMs, advanced quantization strategies might be used, such as per-channel scaling or block-wise quantization. There's ongoing research on the representational capacity of low-rank updates, how the rank $r$ determines the model’s flexibility, and how quantization error interacts with low-rank decomposition. QLoRA enables fine-tuning multi-billion-parameter models on GPUs with limited memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82tfbE2llcW",
   "metadata": {
    "id": "a82tfbE2llcW"
   },
   "source": [
    "<a id=\"intuition\"></a>\n",
    "# 2. Intuition Behind Quantization, LoRA, and QLoRA\n",
    "**Intuition**:\n",
    "- **Quantization**: We reduce the precision of numbers. Think of it like rounding prices to the nearest dollar instead of using pennies. You lose some fine detail, but you speed up calculations and reduce storage.\n",
    "- **LoRA**: Instead of rewriting the entire book (model weights), you add notes in the margins that reference which parts of the book need updating. This is more efficient.\n",
    "- **QLoRA**: It’s like having a short summary (4-bit quantization) of the big book plus those margin notes (LoRA) combined. You can store everything in a smaller space and still adapt it for new tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Id-uae1sllcX",
   "metadata": {
    "id": "Id-uae1sllcX"
   },
   "source": [
    "<a id=\"history\"></a>\n",
    "# 3. Brief History and Underlying Tech\n",
    "- **Quantization**: Has been around in digital signal processing for decades. Became popular in neural networks to speed up inference on edge devices and reduce memory.\n",
    "- **LoRA**: Introduced as a parameter-efficient fine-tuning technique; it's a response to the exploding size of large language models (LLMs). The original paper demonstrated significant savings in GPU memory and training speed.\n",
    "- **QLoRA**: Proposed to leverage both 4-bit quantization for base LLM weights and LoRA for fine-tuning. Helps reduce the massive cost of hardware (GPU) and memory usage, enabling LLM tuning on more modest systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7y3dWLbzllcX",
   "metadata": {
    "id": "7y3dWLbzllcX"
   },
   "source": [
    "<a id=\"math\"></a>\n",
    "# 4. Math Behind the Techniques\n",
    "### 4.1 Quantization\n",
    "- **Simple equation**: If $w$ is a real-valued weight,\n",
    "$$\n",
    "w_{q} = \\text{round}\\left(\\frac{w - \\alpha}{\\delta}\\right),\n",
    "$$\n",
    "where $w_{q}$ is the quantized integer, $\\alpha$ is a zero point (offset), and $\\delta$ is the scale. The real value is approximated by:\n",
    "$$\n",
    "w_{approx} = w_q \\times \\delta + \\alpha.\n",
    "$$\n",
    "\n",
    "### 4.2 LoRA\n",
    "- A weight matrix $W$ is large. We keep it **frozen** and add a small update $\\Delta W$ that can be written as $A B^T$:\n",
    "$$\n",
    "W_{\\text{new}} = W + A B^T,\n",
    "$$\n",
    "where $A$ and $B$ are much smaller, rank-$r$ matrices, typically with dimension $d \\times r$ and $r \\times d$, respectively.\n",
    "\n",
    "### 4.3 QLoRA\n",
    "- **4-bit Quantization of the base weights**: In practice, a technique like [GPTQ](https://arxiv.org/abs/2210.17323) or [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) is used.\n",
    "- **LoRA applied on top**: The original weights are stored in 4-bit precision, while the LoRA matrices remain in higher precision. We can train just the LoRA part, combining memory efficiency with effective fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f8J98ellcX",
   "metadata": {
    "id": "64f8J98ellcX"
   },
   "source": [
    "<a id=\"example_code\"></a>\n",
    "# 5. Illustrative Example with Code\n",
    "In this section, we’ll do a very simplified demonstration of how quantization might work on a small tensor, then how LoRA can be conceptually applied in PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "swFPkzMAllcX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {},
    "executionInfo": {
     "elapsed": 7990,
     "status": "ok",
     "timestamp": 1737810934130,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "swFPkzMAllcX",
    "outputId": "82cfd4a0-c43f-4678-b054-bf909d5387fb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data: tensor([0.1000, 0.2000, 0.2500, 0.7500, 1.0000, 1.1000])\n",
      "\n",
      "Quantized Data (4-bit): tensor([ 0.,  2.,  2., 10., 13., 15.])\n",
      "Scale: tensor(0.0667) Zero Point: tensor(0.1000)\n",
      "Dequantized Data: tensor([0.1000, 0.2333, 0.2333, 0.7667, 0.9667, 1.1000])\n"
     ]
    }
   ],
   "source": [
    "# Let's do a toy example of quantization in Python\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def simple_quantize(tensor, num_bits=4):\n",
    "    # For demonstration, we do a naive min-max scaling\n",
    "    # Determine min and max\n",
    "    t_min = tensor.min()\n",
    "    t_max = tensor.max()\n",
    "\n",
    "    # Number of levels\n",
    "    levels = 2 ** num_bits\n",
    "\n",
    "    # Scale and zero-point\n",
    "    scale = (t_max - t_min) / (levels - 1)\n",
    "    zero_point = t_min\n",
    "\n",
    "    # Quantize\n",
    "    quantized = torch.round((tensor - zero_point) / scale)\n",
    "\n",
    "    return quantized, scale, zero_point\n",
    "\n",
    "def simple_dequantize(quantized, scale, zero_point):\n",
    "    return quantized * scale + zero_point\n",
    "\n",
    "# Example tensor\n",
    "data = torch.tensor([0.1, 0.2, 0.25, 0.75, 1.0, 1.1], dtype=torch.float32)\n",
    "print(\"Original Data:\", data)\n",
    "\n",
    "q_data, scale, zp = simple_quantize(data, num_bits=4)\n",
    "dq_data = simple_dequantize(q_data, scale, zp)\n",
    "\n",
    "print(\"\\nQuantized Data (4-bit):\", q_data)\n",
    "print(\"Scale:\", scale, \"Zero Point:\", zp)\n",
    "print(\"Dequantized Data:\", dq_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I84kcoO9llcX",
   "metadata": {
    "id": "I84kcoO9llcX"
   },
   "source": [
    "In the code above, we:\n",
    "1. **Compute** the range of the data ($t_\\text{min}$ and $t_\\text{max}$).\n",
    "2. **Derive** the scale and zero-point for a given bit precision (e.g., 4 bits gives 16 levels).\n",
    "3. **Round** the data to these discrete levels.\n",
    "4. **Dequantize** by reversing the process.\n",
    "\n",
    "This is a simplistic demonstration of what quantization does.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WcYEbnttllcY",
   "metadata": {
    "id": "WcYEbnttllcY"
   },
   "source": [
    "### LoRA Conceptual Example\n",
    "Let’s do a toy example of applying a low-rank update to a matrix in PyTorch. Note: This is a contrived example just to illustrate the idea of $W + AB^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "J-eB1VgfllcY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {},
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1737810934130,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "J-eB1VgfllcY",
    "outputId": "6e843147-bdad-47a0-ed73-4b87dfaf57e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original W:\n",
      " tensor([[-1.6709,  0.1817, -0.9945, -0.0473, -0.1308, -0.8295],\n",
      "        [ 0.3378, -0.5218,  0.5219,  0.4855, -0.3505,  0.7716],\n",
      "        [ 0.5225,  0.9816,  0.5470,  1.4447, -0.7710, -0.5612],\n",
      "        [ 0.0310,  1.7092,  0.3299,  0.2574,  0.5099, -1.2652],\n",
      "        [-1.0065, -0.6935, -0.6856,  0.9836,  1.0478,  1.3924],\n",
      "        [-0.0584, -2.2209,  0.7246, -0.8486, -0.7438,  0.7352]])\n",
      "\n",
      "LoRA update Delta_W = A*B:\n",
      " tensor([[ 4.5739e-01, -2.7724e-01, -3.3288e-03,  3.6313e-01,  2.5559e-01,\n",
      "          1.1802e+00],\n",
      "        [ 6.7996e-01, -5.5937e-01,  3.8879e-02,  2.6342e-01,  4.8200e-01,\n",
      "          1.8959e+00],\n",
      "        [-6.5047e-02, -2.4404e-01,  8.4853e-02, -5.8380e-01,  1.6010e-01,\n",
      "          1.0445e-01],\n",
      "        [-1.0409e+00,  8.5025e-01, -5.7706e-02, -4.1470e-01, -7.3367e-01,\n",
      "         -2.8966e+00],\n",
      "        [ 8.0745e-01, -5.1029e-01,  3.3829e-04,  6.0184e-01,  4.6567e-01,\n",
      "          2.1035e+00],\n",
      "        [-2.6922e-01,  2.0129e-01, -9.3858e-03, -1.4218e-01, -1.7685e-01,\n",
      "         -7.3128e-01]])\n",
      "\n",
      "W_new = W + Delta_W:\n",
      " tensor([[-1.2135e+00, -9.5578e-02, -9.9783e-01,  3.1582e-01,  1.2483e-01,\n",
      "          3.5071e-01],\n",
      "        [ 1.0177e+00, -1.0812e+00,  5.6078e-01,  7.4888e-01,  1.3149e-01,\n",
      "          2.6676e+00],\n",
      "        [ 4.5747e-01,  7.3757e-01,  6.3185e-01,  8.6094e-01, -6.1094e-01,\n",
      "         -4.5671e-01],\n",
      "        [-1.0100e+00,  2.5594e+00,  2.7220e-01, -1.5730e-01, -2.2378e-01,\n",
      "         -4.1618e+00],\n",
      "        [-1.9910e-01, -1.2038e+00, -6.8526e-01,  1.5855e+00,  1.5134e+00,\n",
      "          3.4959e+00],\n",
      "        [-3.2765e-01, -2.0196e+00,  7.1524e-01, -9.9074e-01, -9.2063e-01,\n",
      "          3.9345e-03]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "d = 6  # dimension\n",
    "r = 2  # rank\n",
    "\n",
    "# Original weight matrix W\n",
    "W = torch.randn(d, d)\n",
    "\n",
    "# LoRA matrices\n",
    "A = torch.randn(d, r)\n",
    "B = torch.randn(r, d)\n",
    "\n",
    "# Low-rank update Delta W = A.mm(B)\n",
    "Delta_W = A.mm(B)\n",
    "\n",
    "# New weight\n",
    "W_new = W + Delta_W\n",
    "\n",
    "print(\"Original W:\\n\", W)\n",
    "print(\"\\nLoRA update Delta_W = A*B:\\n\", Delta_W)\n",
    "print(\"\\nW_new = W + Delta_W:\\n\", W_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NEOv3ficllcY",
   "metadata": {
    "id": "NEOv3ficllcY"
   },
   "source": [
    "Here, we are simulating the **LoRA** technique. In an actual training scenario, $W$ (the base model weights) would be frozen, and only $A$ and $B$ would be updated during backpropagation. Then the updated weight would be $W + AB^T$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PRSpIZSrllcY",
   "metadata": {
    "id": "PRSpIZSrllcY"
   },
   "source": [
    "<a id=\"mock_calcs\"></a>\n",
    "# 6. Example Calculations\n",
    "Let’s do a small conceptual calculation:\n",
    "- Suppose you have a single weight $w = 0.78$ in float32.\n",
    "- You decide to quantize to 2 bits (4 levels: $\\{0, 1, 2, 3\\}$).\n",
    "- If your range is [0, 1], then scale $\\delta = (1 - 0)/3 = 0.3333$, zero-point $\\alpha = 0$.\n",
    "- Quantized value $w_q = \\text{round}((0.78 - 0)/0.3333) = \\text{round}(2.34) = 2$.\n",
    "- Dequantized $w_{approx} = 2 \\times 0.3333 = 0.6667$.\n",
    "\n",
    "Here, you see that you lost some precision (went from 0.78 to ~0.67), but you gained efficiency.\n",
    "\n",
    "### Terms:\n",
    "- **Weight**: The parameter in the neural network.\n",
    "- **Bias**: An additional parameter added to the weighted sum before passing through an activation.\n",
    "- **Rank**: The dimension of the subspace for the LoRA update.\n",
    "- **Scale & Zero Point**: Used in quantization to map continuous ranges to discrete integer levels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_N3UD5rWllcY",
   "metadata": {
    "id": "_N3UD5rWllcY"
   },
   "source": [
    "<a id=\"step_by_step\"></a>\n",
    "# 7. Step-by-Step Example from Scratch\n",
    "We’ll build a **tiny** neural network and do the following:\n",
    "1. **Initialize** the network.\n",
    "2. **Quantize** its parameters.\n",
    "3. **Apply** a LoRA-like update.\n",
    "\n",
    "Note: This is a conceptual demonstration and not a full-blown training session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fYp8IhR-llcY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {},
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1737810934372,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "fYp8IhR-llcY",
    "outputId": "daec787b-2f07-48b1-9686-a9ac60af409d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model parameters (fc1 weight):\n",
      " Parameter containing:\n",
      "tensor([[-0.4468, -0.0502,  0.4679,  0.1325],\n",
      "        [-0.4030,  0.0108, -0.3204,  0.0597],\n",
      "        [ 0.1513,  0.4134,  0.4890, -0.0888],\n",
      "        [ 0.4202,  0.0199,  0.3172,  0.4601]], requires_grad=True)\n",
      "\n",
      "Quantized + Dequantized model parameters (fc1 weight):\n",
      " Parameter containing:\n",
      "tensor([[-0.4468, -0.0725,  0.4890,  0.1147],\n",
      "        [-0.3844, -0.0101, -0.3220,  0.0523],\n",
      "        [ 0.1771,  0.4266,  0.4890, -0.0725],\n",
      "        [ 0.4266, -0.0101,  0.3018,  0.4890]], requires_grad=True)\n",
      "\n",
      "After LoRA-like update (fc1 weight):\n",
      " Parameter containing:\n",
      "tensor([[ 4.7902,  4.0630, -2.1885,  1.8887],\n",
      "        [ 1.7763,  1.6905, -1.3997,  0.8723],\n",
      "        [-1.5059, -0.9240,  1.4538, -0.3012],\n",
      "        [-2.0380, -2.0002,  1.7736,  0.3468]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TinyNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(TinyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Step 1: Initialize network\n",
    "model = TinyNet(input_dim=4, hidden_dim=4, output_dim=2)\n",
    "print(\"Original model parameters (fc1 weight):\\n\", model.fc1.weight)\n",
    "\n",
    "# Step 2: Quantize the fc1 weight\n",
    "fc1_weight = model.fc1.weight.data\n",
    "q_fc1_weight, scale, zp = simple_quantize(fc1_weight, num_bits=4)\n",
    "model.fc1.weight.data = simple_dequantize(q_fc1_weight, scale, zp)\n",
    "\n",
    "print(\"\\nQuantized + Dequantized model parameters (fc1 weight):\\n\", model.fc1.weight)\n",
    "\n",
    "# Step 3: Apply a LoRA-like update\n",
    "r = 2\n",
    "A = torch.randn(model.fc1.weight.data.shape[0], r)\n",
    "B = torch.randn(r, model.fc1.weight.data.shape[1])\n",
    "Delta = A.mm(B)\n",
    "model.fc1.weight.data += Delta\n",
    "print(\"\\nAfter LoRA-like update (fc1 weight):\\n\", model.fc1.weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zCWLSPx_llcY",
   "metadata": {
    "id": "zCWLSPx_llcY"
   },
   "source": [
    "We first **initialized** a tiny network. Then, we **quantized** its `fc1` weight parameter to 4 bits. Next, we **dequantized** it back and replaced the parameter in the model, simulating storing it in quantized form. Finally, we added a LoRA-like low-rank update to illustrate how we can adapt a quantized model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cxpLwVbbllcY",
   "metadata": {
    "id": "cxpLwVbbllcY"
   },
   "source": [
    "<a id=\"illustrative_problem\"></a>\n",
    "# 8. Illustrative Problem It Solves\n",
    "For instance, if you want to run a **Large Language Model** on a **resource-limited device** (like a smaller GPU or edge device), you can’t afford storing massive 32-bit floats for billions of parameters. By using 4-bit quantization, you drastically reduce memory.\n",
    "\n",
    "But you also want to **adapt** or fine-tune the model for a new language or domain. Full fine-tuning of billions of parameters is also expensive. Instead, you apply **LoRA** (only a small set of trainable parameters), and combine that with the quantized model. This is **QLoRA**, which solves the problem of running and updating large models cheaply.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W312Kte6llcY",
   "metadata": {
    "id": "W312Kte6llcY"
   },
   "source": [
    "<a id=\"realworld_problem\"></a>\n",
    "# 9. Real-World Problems Solved\n",
    "1. **Domain adaptation**: Fine-tune a large model on a specialized domain (e.g., medical data) using LoRA, while storing the base model in 4-bit.\n",
    "2. **Edge deployment**: Deploy on embedded devices or small data centers with limited GPU memory.\n",
    "3. **Cost reduction**: Lower the hardware requirement means cheaper training and inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rLcxYOh_llcY",
   "metadata": {
    "id": "rLcxYOh_llcY"
   },
   "source": [
    "<a id=\"use_tech\"></a>\n",
    "# 10. How to Solve a Real-World Problem Using This Tech\n",
    "1. **Pick a big pretrained model** (like a 7B or 13B parameter language model).\n",
    "2. **Quantize** the model using a 4-bit or 8-bit quantization tool.\n",
    "3. **Apply LoRA**: Freeze the base weights and add low-rank adapters.\n",
    "4. **Train** the LoRA parameters with a small learning rate on your domain data.\n",
    "5. **Deploy**: Use the quantized model + LoRA parameters in production. You only need to load the small LoRA overhead in full precision. Everything else is compressed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39O76xi3llcY",
   "metadata": {
    "id": "39O76xi3llcY"
   },
   "source": [
    "<a id=\"questions\"></a>\n",
    "# 11. What Other Questions Can You Ask? (Points to Ponder)\n",
    "1. *How do we choose the right number of bits for quantization?*\n",
    "2. *How do we select the rank $r$ for LoRA?*\n",
    "3. *When does quantization error significantly degrade performance?*\n",
    "4. *How does outlier-aware quantization improve results?*\n",
    "5. *How do we handle gradient updates in QLoRA for the quantized weights?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WZ3ediinllcY",
   "metadata": {
    "id": "WZ3ediinllcY"
   },
   "source": [
    "<a id=\"answers\"></a>\n",
    "# 12. Answers to the Questions (with Code Examples)\n",
    "### 1. How do we choose the right number of bits for quantization?\n",
    "- Typically, we try 8-bit or 4-bit because they balance memory savings and accuracy. 2-bit might be too aggressive. The best way is to **experiment** on a validation set.\n",
    "\n",
    "### 2. How do we select the rank $r$ for LoRA?\n",
    "- This depends on how complex the new task is. A higher rank means more parameters to learn, but better capacity. A typical range is $r \\in [4, 64]$ for LLM fine-tuning.\n",
    "\n",
    "### 3. When does quantization error significantly degrade performance?\n",
    "- If your network is very sensitive to small changes in weights or if you quantize to too few bits (2-bit or 3-bit). Large networks tend to be somewhat robust to quantization, but tasks requiring very high precision might suffer.\n",
    "\n",
    "### 4. How does outlier-aware quantization improve results?\n",
    "- Some weights (outliers) can be very large, messing up min-max scaling. Outlier-aware quantization uses separate scaling for outliers or specialized algorithms to reduce the impact of these extreme values.\n",
    "\n",
    "### 5. How do we handle gradient updates in QLoRA for the quantized weights?\n",
    "- In practice, the gradients are computed in higher precision. We don’t update the quantized weights directly. Instead, we update the LoRA parameters. The quantized weights remain mostly unchanged.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9p-Xk3CllcY",
   "metadata": {
    "id": "f9p-Xk3CllcY"
   },
   "source": [
    "<a id=\"easy_sample\"></a>\n",
    "# 13. A Sample Exercise\n",
    "Below is a **template** code you can run and fill in the TODO items. It demonstrates building a small network, quantizing it, and applying a LoRA update. Complete the TODOs to see how it works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "qm4AhANVllcY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {},
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1737810934372,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "qm4AhANVllcY",
    "outputId": "74348b89-d355-41c1-f1af-d41964ac5fd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original params: Parameter containing:\n",
      "tensor([[-0.5225, -0.0100,  0.1037],\n",
      "        [ 0.1947, -0.1677, -0.2551]], requires_grad=True)\n",
      "\n",
      "Updated params after LoRA-like update (complete the TODOs first): Parameter containing:\n",
      "tensor([[-0.1693,  1.4029, -1.0591],\n",
      "        [ 0.3295,  0.3458, -0.6773]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Complete and run this code.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def my_quantize(tensor, num_bits=4):\n",
    "    # TODO: Implement a simple min-max quantization\n",
    "    # 1. Find the min and max of the tensor\n",
    "    # 2. Compute scale = (max - min) / (levels - 1)\n",
    "    # 3. Compute zero_point = min\n",
    "    # 4. Quantize by rounding\n",
    "    tensor_min = tensor.min()\n",
    "    tensor_max = tensor.max()\n",
    "    scale = (tensor_max - tensor_min) / (2 ** num_bits - 1)\n",
    "    zero_point = tensor_min\n",
    "    quantized_tensor = torch.round((tensor - zero_point) / scale)\n",
    "    return quantized_tensor, scale, zero_point\n",
    "\n",
    "def my_dequantize(quantized_tensor, scale, zero_point):\n",
    "    # TODO: Implement the reverse process\n",
    "    return quantized_tensor * scale + zero_point\n",
    "\n",
    "# Define a simple linear layer\n",
    "linear = nn.Linear(3, 2, bias=False)\n",
    "\n",
    "# Print original parameters\n",
    "print(\"Original params:\", linear.weight)\n",
    "\n",
    "# TODO: Call my_quantize on linear.weight.data\n",
    "q_weight, scale, zp = my_quantize(linear.weight.data, num_bits=4)\n",
    "\n",
    "# TODO: Dequantize\n",
    "dq_weight = my_dequantize(q_weight, scale, zp)\n",
    "\n",
    "# TODO: Assign dq_weight back to linear.weight.data\n",
    "linear.weight.data = dq_weight\n",
    "\n",
    "# Create LoRA-like update\n",
    "r = 1\n",
    "A = torch.randn(linear.weight.size(0), r)\n",
    "B = torch.randn(r, linear.weight.size(1))\n",
    "Delta = A.mm(B)\n",
    "\n",
    "# TODO: Add Delta to linear.weight.data\n",
    "linear.weight.data += Delta\n",
    "\n",
    "# Print updated parameters\n",
    "print(\"\\nUpdated params after LoRA-like update (complete the TODOs first):\", linear.weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jup7H0O-llcZ",
   "metadata": {
    "id": "jup7H0O-llcZ"
   },
   "source": [
    "By filling in the `TODO`s, you’ll practice implementing your own min-max quantization logic, dequantization, and finally adding a LoRA-like update.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cUFg0tBMllcZ",
   "metadata": {
    "id": "cUFg0tBMllcZ"
   },
   "source": [
    "<a id=\"glossary\"></a>\n",
    "# 14. Glossary\n",
    "- **Quantization**: Reducing the number of bits used to represent numbers in neural networks.\n",
    "- **LoRA (Low-Rank Adaptation)**: A method that updates only a small set of parameters through low-rank factorization.\n",
    "- **QLoRA**: 4-bit quantization + LoRA to fine-tune large models efficiently.\n",
    "- **Scale**: Factor used to scale floating-point range into a smaller discrete range.\n",
    "- **Zero Point (Offset)**: A shift added so that the minimum value can map to 0 in quantized integer space.\n",
    "- **Rank**: The number of columns in matrix A (and rows in matrix B) for LoRA.\n",
    "- **Parameter-Efficient Fine-Tuning**: Methods that avoid updating all model parameters.\n",
    "- **Outlier-Aware Quantization**: Specialized approach for dealing with extremely large or small weight values.\n",
    "- **Min-Max Quantization**: The simplest form of quantization using the min and max of the data.\n",
    "- **bitsandbytes**: A popular library for 8-bit and 4-bit optimization and quantization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eC3J5_kNllcZ",
   "metadata": {
    "id": "eC3J5_kNllcZ"
   },
   "source": [
    "## Conclusion\n",
    "With these examples and explanations, you should now have a comprehensive understanding of **Quantization**, **LoRA**, and **QLoRA**, along with their theory, mathematics, and practical implementation details. Happy exploring and coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "TY5MppxPllcZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1737810934372,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "TY5MppxPllcZ",
    "outputId": "14630296-c87e-43de-eca2-9c50d645b6a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Acknowledgement +++\n",
      "Executed on: 2025-01-29 01:41:51.289597\n",
      "In Google Colab: No\n",
      "System info: Linux 6.8.0-51-generic\n",
      "Node name: alimuhammad-Inspiron-7559\n",
      "MAC address: 20:47:47:74:94:05\n",
      "IP address: 127.0.1.1\n",
      "Signing off, Ali Muhammad Asad\n"
     ]
    }
   ],
   "source": [
    "import os, sys, platform, datetime, uuid, socket\n",
    "\n",
    "def signoff(name=\"Anonymous\"):\n",
    "    colab_check = \"Yes\" if 'google.colab' in sys.modules else \"No\"\n",
    "    mac_addr = ':'.join(format((uuid.getnode() >> i) & 0xff, '02x') for i in reversed(range(0, 48, 8)))\n",
    "    print(\"+++ Acknowledgement +++\")\n",
    "    print(f\"Executed on: {datetime.datetime.now()}\")\n",
    "    print(f\"In Google Colab: {colab_check}\")\n",
    "    print(f\"System info: {platform.system()} {platform.release()}\")\n",
    "    print(f\"Node name: {platform.node()}\")\n",
    "    print(f\"MAC address: {mac_addr}\")\n",
    "    try:\n",
    "        print(f\"IP address: {socket.gethostbyname(socket.gethostname())}\")\n",
    "    except:\n",
    "        print(\"IP address: Unknown\")\n",
    "    print(f\"Signing off, {name}\")\n",
    "\n",
    "signoff(\"Ali Muhammad Asad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b959f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "name": "QLoRA_Quantization_Explainer"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
