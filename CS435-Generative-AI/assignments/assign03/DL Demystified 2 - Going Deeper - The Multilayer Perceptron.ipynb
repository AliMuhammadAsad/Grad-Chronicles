{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "s8eBPTNs4HTj",
      "metadata": {
        "id": "s8eBPTNs4HTj"
      },
      "outputs": [],
      "source": [
        "#################################################################\n",
        "#                                                               #\n",
        "#  CS435 Generative AI: Security, Ethics and Governance         #\n",
        "#                                                               #\n",
        "#  Instructor: Dr. Adnan Masood                                 #\n",
        "#  Contact:    adnanmasood@gmail.com                            #\n",
        "#                                                               #\n",
        "#  Notebook is MIT Licensed                                     #\n",
        "#################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u5U5eqRW4HTk",
      "metadata": {
        "id": "u5U5eqRW4HTk"
      },
      "source": [
        "# Multilayer Perceptron (MLP) - Comprehensive Explanation\n",
        "\n",
        "---\n",
        "## Table of Contents\n",
        "1. [Introduction](#introduction)\n",
        "2. [Building an Intuitive Understanding](#5levels)\n",
        "3. [Intuition & History](#intuition)\n",
        "4. [Mathematics Behind MLP](#math)\n",
        "5. [Illustrative Example (With Code)](#illustrative)\n",
        "6. [Example Calculations](#mock)\n",
        "7. [Step-by-Step Implementation from Scratch](#scratch)\n",
        "8. [Illustrative Problem MLP Solves](#illustrative_problem)\n",
        "9. [Practical Real-World Problem](#practical)\n",
        "10. [How to Solve a Real-World Problem Using MLP](#solve_real)\n",
        "11. [Questions to Ponder](#questions_ponder)\n",
        "12. [Answers & Code Examples](#answers_code)\n",
        "13. [A Sample Exercise](#easy_code)\n",
        "14. [Glossary](#glossary)\n",
        "\n",
        "---\n",
        "<a id=\"introduction\"></a>\n",
        "## 1. Introduction\n",
        "Welcome to our deep dive into **Multilayer Perceptrons (MLPs)**. In this Jupyter notebook, we will explore MLPs from the very basics to advanced concepts, walking through intuition, history, mathematics, code examples, and practical applications. Let's begin!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NyfXwQeq4HTl",
      "metadata": {
        "id": "NyfXwQeq4HTl"
      },
      "source": [
        "<a id=\"5levels\"></a>\n",
        "## 2. Building an Intuitive Understanding\n",
        "We will explain MLP at five distinct levels of complexity. Feel free to read through them all or skip to your level of comfort.\n",
        "\n",
        "- **What is an MLP?**\n",
        "  - An MLP is like a group of friends solving a puzzle together by passing notes. Each friend does a little work, then hands off the result to the next friend.\n",
        "- **Why is it important?**\n",
        "  - MLPs help computers learn patterns—like recognizing digits or understanding if an email is spam.\n",
        "\n",
        "- **A bit more detail**:\n",
        "  - An MLP has layers of tiny calculators called neurons. These neurons take numbers, do math, and pass results to the next layer.\n",
        "- **Goal**:\n",
        "  - Train the MLP so it can make predictions or decisions (like classifying images).\n",
        "\n",
        "- **Architecture**:\n",
        "  - Consists of an input layer, hidden layer(s), and an output layer.\n",
        "- **Learning**:\n",
        "  - We \"teach\" the network by showing examples. We measure errors with a loss function, then adjust the network's parameters (weights and biases) to reduce errors.\n",
        "\n",
        "- **Components**:\n",
        "  - Activation functions: Sigmoid, ReLU, Tanh, etc.\n",
        "  - Optimization algorithms: Gradient Descent, Stochastic Gradient Descent, Adam, etc.\n",
        "- **Training Procedure**:\n",
        "  - Forward pass, compute loss, backward pass (backpropagation), update parameters.\n",
        "\n",
        "- **Advanced Topics**:\n",
        "  - Convergence properties, theoretical underpinnings (Universal Approximation Theorem), interpretability, and regularization (L2, Dropout).\n",
        "- **Research Scope**:\n",
        "  - Investigating ways to improve generalization, reduce overfitting, and design deeper architectures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-USqh5p44HTl",
      "metadata": {
        "id": "-USqh5p44HTl"
      },
      "source": [
        "<a id=\"intuition\"></a>\n",
        "## 3. Intuition & Brief History\n",
        "**Intuition**: An MLP is like a decision-making pipeline. Each layer refines the information received from the previous layer. Think of it as a multi-step question-and-answer process:\n",
        "1. Input layer collects data.\n",
        "2. Each hidden layer processes, extracts patterns, and compresses/expands relevant features.\n",
        "3. The output layer makes a final decision or prediction.\n",
        "\n",
        "**Brief History**:\n",
        "- **1943**: McCulloch and Pitts propose the first mathematical model of a neuron.\n",
        "- **1958**: Perceptron invented by Frank Rosenblatt.\n",
        "- **1970s**: Introduction of multi-layer networks, but training them was difficult.\n",
        "- **1986**: Backpropagation popularized by Rumelhart, Hinton, and Williams.\n",
        "- **2006**: Deep learning renaissance begins with efficient training of deeper networks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y9k50hhK4HTl",
      "metadata": {
        "id": "Y9k50hhK4HTl"
      },
      "source": [
        "<a id=\"math\"></a>\n",
        "## 4. Mathematics Behind MLP\n",
        "An MLP is made up of layers of **neurons**. Each neuron does a simple math operation:\n",
        "\n",
        "1. Takes inputs $x_1, x_2, \\ldots, x_n$.\n",
        "2. Multiplies each $x_i$ by a **weight** $w_i$.\n",
        "3. Adds a **bias** $b$ to that sum.\n",
        "4. Passes the result through an **activation function** (like a squishing function that keeps numbers in a certain range).\n",
        "\n",
        "Mathematically:\n",
        "$$\n",
        "z = w_1x_1 + w_2x_2 + \\cdots + w_n x_n + b\n",
        "$$\n",
        "Then,\n",
        "$$\n",
        "\\text{output} = \\sigma(z)\n",
        "$$\n",
        "where $\\sigma$ is an activation function (e.g., $\\sigma(z) = \\text{ReLU}(z) = \\max(0, z)$, or a sigmoid, etc.).\n",
        "\n",
        "### Forward Pass\n",
        "For each layer, we calculate the output of every neuron in that layer and pass it to the next layer.\n",
        "\n",
        "### Backpropagation\n",
        "To train, we compare the network's prediction to the **target** and compute a **loss** (or error):\n",
        "$$\n",
        "L = \\frac{1}{2}(\\hat{y} - y)^2\n",
        "$$\n",
        "where $\\hat{y}$ is the predicted output and $y$ is the true label. Then we adjust weights by taking partial derivatives of $L$ w.r.t. $w$ (chain rule) so that we nudge them in the direction that reduces the loss.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ipQp82Pp4HTl",
      "metadata": {
        "id": "ipQp82Pp4HTl"
      },
      "source": [
        "<a id=\"illustrative\"></a>\n",
        "## 5. Illustrative Example with Code\n",
        "We’ll build a tiny MLP that learns the XOR function (an exclusive OR truth table). XOR’s outputs are tricky for a single neuron, but with a small hidden layer, it works!\n",
        "\n",
        "XOR Truth Table:\n",
        "| x1 | x2 | x1 XOR x2 |\n",
        "|----|----|-----------|\n",
        "|  0 |  0 |     0     |\n",
        "|  0 |  1 |     1     |\n",
        "|  1 |  0 |     1     |\n",
        "|  1 |  1 |     0     |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "Bs3bY0Um4HTl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {},
        "id": "Bs3bY0Um4HTl",
        "outputId": "710e1377-c0e8-40d0-8b14-2b67b44d37e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.2696367200181884\n",
            "Epoch 2000, Loss: 0.14613094454674397\n",
            "Epoch 4000, Loss: 0.13288176632884824\n",
            "Epoch 6000, Loss: 0.12955145571861026\n",
            "Epoch 8000, Loss: 0.12814004233797888\n",
            "\n",
            "Final predictions after training:\n",
            "[[0]\n",
            " [0]\n",
            " [1]\n",
            " [1]]\n"
          ]
        }
      ],
      "source": [
        "# Let's implement a tiny MLP for XOR in Python (using only NumPy):\n",
        "import numpy as np\n",
        "\n",
        "# XOR dataset\n",
        "X = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "              [1, 0],\n",
        "              [1, 1]])\n",
        "y = np.array([[0],\n",
        "              [1],\n",
        "              [1],\n",
        "              [0]])\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "# Initialize weights\n",
        "W1 = np.random.randn(2, 2)  # 2 inputs -> 2 hidden\n",
        "b1 = np.zeros((1, 2))\n",
        "W2 = np.random.randn(2, 1)  # 2 hidden -> 1 output\n",
        "b2 = np.zeros((1, 1))\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "for epoch in range(10000):\n",
        "    # Forward pass\n",
        "    z1 = np.dot(X, W1) + b1   # shape: (4, 2)\n",
        "    a1 = sigmoid(z1)         # shape: (4, 2)\n",
        "    z2 = np.dot(a1, W2) + b2 # shape: (4, 1)\n",
        "    a2 = sigmoid(z2)         # shape: (4, 1)\n",
        "\n",
        "    # Compute loss (Mean Squared Error)\n",
        "    loss = np.mean((y - a2)**2)\n",
        "\n",
        "    # Backpropagation\n",
        "    dLoss_da2 = -(y - a2)\n",
        "    dLoss_dz2 = dLoss_da2 * sigmoid_derivative(z2)\n",
        "\n",
        "    dLoss_dW2 = np.dot(a1.T, dLoss_dz2)\n",
        "    dLoss_db2 = np.sum(dLoss_dz2, axis=0, keepdims=True)\n",
        "\n",
        "    dLoss_da1 = np.dot(dLoss_dz2, W2.T)\n",
        "    dLoss_dz1 = dLoss_da1 * sigmoid_derivative(z1)\n",
        "\n",
        "    dLoss_dW1 = np.dot(X.T, dLoss_dz1)\n",
        "    dLoss_db1 = np.sum(dLoss_dz1, axis=0, keepdims=True)\n",
        "\n",
        "    # Gradient Descent update\n",
        "    W2 -= learning_rate * dLoss_dW2\n",
        "    b2 -= learning_rate * dLoss_db2\n",
        "    W1 -= learning_rate * dLoss_dW1\n",
        "    b1 -= learning_rate * dLoss_db1\n",
        "\n",
        "    # Print loss every 2000 epochs\n",
        "    if epoch % 2000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "print(\"\\nFinal predictions after training:\")\n",
        "predictions = a2 > 0.5\n",
        "print(predictions.astype(int))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H7K6EUOQ4HTm",
      "metadata": {
        "id": "H7K6EUOQ4HTm"
      },
      "source": [
        "<a id=\"mock\"></a>\n",
        "## 6. Example Calculations\n",
        "- **Weights (W)**: Numbers used to multiply each input.\n",
        "- **Bias (b)**: A number added to the weighted input before activation.\n",
        "- **Activation Function ($\\sigma$)**: A function applied to the sum of weighted inputs, e.g. sigmoid.\n",
        "- **Forward Pass**: Computing the output for each layer.\n",
        "- **Loss**: The measure of error (e.g., Mean Squared Error).\n",
        "- **Backward Pass** (Backpropagation): Updating weights to minimize the loss.\n",
        "\n",
        "Example of a single neuron calculation:\n",
        "$$\n",
        "z = w_1x_1 + w_2x_2 + b\n",
        "$$\n",
        "$$\n",
        "\\text{output} = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sT6sy7d-4HTm",
      "metadata": {
        "id": "sT6sy7d-4HTm"
      },
      "source": [
        "<a id=\"scratch\"></a>\n",
        "## 7. Step-by-Step Example from Scratch\n",
        "1. **Define the Network Architecture**: Decide how many neurons in each layer (input, hidden, output).\n",
        "2. **Initialize Weights & Biases**: Usually random small values.\n",
        "3. **Define Activation Function**: Sigmoid, ReLU, etc.\n",
        "4. **Forward Pass**: Multiply inputs by weights, add bias, apply activation.\n",
        "5. **Calculate Loss**: Compare prediction with the true label.\n",
        "6. **Backpropagation**: Compute gradient of the loss w.r.t. weights and biases.\n",
        "7. **Update Weights**: Adjust parameters to reduce loss.\n",
        "8. **Repeat** until convergence or max epochs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uvY92TFZ4HTm",
      "metadata": {
        "id": "uvY92TFZ4HTm"
      },
      "source": [
        "<a id=\"illustrative_problem\"></a>\n",
        "## 8. What Illustrative Problem Does MLP Solve?\n",
        "We used **XOR** as a classic example because a single perceptron (without hidden layers) *cannot* solve XOR. A small MLP with a hidden layer *can* solve it, showcasing why multiple layers are powerful."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cavxr1Ix4HTm",
      "metadata": {
        "id": "cavxr1Ix4HTm"
      },
      "source": [
        "<a id=\"practical\"></a>\n",
        "## 9. Practical, Real-World Problem\n",
        "MLPs can handle a variety of tasks, for example:\n",
        "- Handwritten digit recognition (MNIST dataset)\n",
        "- Spam detection in emails\n",
        "- Credit score prediction\n",
        "- Basic image classification\n",
        "\n",
        "For instance, with digit recognition, each pixel is an input. The MLP learns weights (important pixels) and biases to identify the digit accurately."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CrAZZV0d4HTm",
      "metadata": {
        "id": "CrAZZV0d4HTm"
      },
      "source": [
        "<a id=\"solve_real\"></a>\n",
        "## 10. How to Solve a Real-World Problem Using This Tech\n",
        "1. Collect or gather a labeled dataset (e.g., images, text, or numeric data).\n",
        "2. Preprocess and split data into training and testing sets.\n",
        "3. Define your MLP architecture (number of layers, neurons, activation).\n",
        "4. Train the MLP using forward pass + backprop.\n",
        "5. Evaluate on the test set. If performance is low, tune hyperparameters or gather more data.\n",
        "6. Deploy the trained model to make predictions in the real world."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9PGVDRPH4HTm",
      "metadata": {
        "id": "9PGVDRPH4HTm"
      },
      "source": [
        "<a id=\"questions_ponder\"></a>\n",
        "## 11. What Other Questions Can You Ask?\n",
        "1. **How many hidden layers do we need and why?**\n",
        "2. **What activation function is best for my problem?**\n",
        "3. **How do we deal with overfitting and underfitting?**\n",
        "4. **What is the difference between MLP and other deep architectures (CNN, RNN)?**\n",
        "5. **How does learning rate affect training?**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WSHkJgmQ4HTm",
      "metadata": {
        "id": "WSHkJgmQ4HTm"
      },
      "source": [
        "<a id=\"answers_code\"></a>\n",
        "## 12. Questions & Answers with Code Examples\n",
        "### Q1: How many hidden layers do we need?\n",
        "**A1**: There's no one-size-fits-all. Simple problems might need 1 or 2 hidden layers, but more complex tasks might need deeper networks. Experimentation is key.\n",
        "\n",
        "### Q2: Which activation function is best?\n",
        "**A2**: Sigmoid is good for small networks, ReLU often works best for deeper networks because it helps avoid vanishing gradients.\n",
        "\n",
        "### Q3: How do we handle overfitting?\n",
        "**A3**: Use regularization (e.g., L2, dropout), gather more data, or reduce network complexity.\n",
        "\n",
        "### Q4: MLP vs CNN vs RNN?\n",
        "**A4**: MLP is fully connected, CNN is specialized for images (exploits spatial structure), RNN is for sequential data.\n",
        "\n",
        "### Q5: How does learning rate affect training?\n",
        "**A5**: A high learning rate can overshoot minima; a low learning rate can be very slow or get stuck."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5IFL-hEU4HTm",
      "metadata": {
        "id": "5IFL-hEU4HTm"
      },
      "source": [
        "<a id=\"easy_code\"></a>\n",
        "## 13. A Sample Exercise\n",
        "Below is a simple code example using **Keras** (TensorFlow) to classify the Iris dataset. Some sections are marked with **TODO** for you to fill in.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "pYipyQiM4HTm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {},
        "id": "pYipyQiM4HTm",
        "outputId": "6029ddc3-7af6-404d-d400-5efe40afb8b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 93.33%\n"
          ]
        }
      ],
      "source": [
        "# PLEASE COMPLETE THE TODO SECTIONS\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data  # shape (150, 4)\n",
        "y = iris.target.reshape(-1, 1)  # shape (150, 1)\n",
        "\n",
        "# One-hot encode target\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y = encoder.fit_transform(y)  # shape (150, 3)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = 'GPU' if tf.config.list_physical_devices('GPU') else 'CPU'\n",
        "\n",
        "print(f\"Training on {device}\")\n",
        "\n",
        "with tf.device(device):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(8, activation='relu', input_shape=(4,)))\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=0)\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "# # TODO: Create a Sequential model\n",
        "# model = Sequential()\n",
        "\n",
        "# # TODO: Add a Dense hidden layer with, say, 8 neurons and activation='relu'\n",
        "# model.add(Dense(8, activation='relu', input_shape=(4,)))\n",
        "\n",
        "# # TODO: Add another Dense hidden layer with, say, 8 neurons and activation='relu'\n",
        "# model.add(Dense(8, activation='relu'))\n",
        "\n",
        "# # TODO: Add the output layer with 3 neurons (because 3 classes) and activation='softmax'\n",
        "# model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# # Compile the model\n",
        "# # TODO: Choose an optimizer, e.g., 'adam', and a loss, e.g., 'categorical_crossentropy'\n",
        "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# # Train the model\n",
        "# model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=0)\n",
        "\n",
        "# # Evaluate\n",
        "# loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "# print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4TZOb8jb4HTn",
      "metadata": {
        "id": "4TZOb8jb4HTn"
      },
      "source": [
        "<a id=\"glossary\"></a>\n",
        "## 14. Glossary\n",
        "- **Activation Function**: A function (Sigmoid, ReLU) that adds non-linearity to the neuron’s output.\n",
        "- **Backpropagation**: Algorithm to compute gradients for all weights in a network, used to train MLP.\n",
        "- **Bias**: A constant term added before activation.\n",
        "- **Epoch**: One complete pass through the entire training dataset.\n",
        "- **Gradient Descent**: An optimization method that updates parameters in the direction opposite the gradient of the loss.\n",
        "- **Hidden Layer**: The layers between input and output.\n",
        "- **Loss Function**: Measures how far the network's predictions are from the true labels.\n",
        "- **Neuron**: The basic unit in a network that computes weighted sums and applies an activation.\n",
        "- **Weight**: A parameter that scales input data in a neuron.\n",
        "- **MLP**: A type of neural network with multiple layers of perceptrons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ZbkIPwU34HTn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbkIPwU34HTn",
        "outputId": "c06d4754-e64f-4ab6-b56f-b5ac220b444c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+++ Acknowledgement +++\n",
            "Executed on: 2025-01-28 12:46:42.148449\n",
            "In Google Colab: Yes\n",
            "System info: Linux 6.1.85+\n",
            "Node name: 6ecffb779a71\n",
            "MAC address: 02:42:ac:1c:00:0c\n",
            "IP address: 172.28.0.12\n",
            "Signing off, Ali Muhammad Asad\n"
          ]
        }
      ],
      "source": [
        "import os, sys, platform, datetime, uuid, socket\n",
        "\n",
        "def signoff(name=\"Anonymous\"):\n",
        "    colab_check = \"Yes\" if 'google.colab' in sys.modules else \"No\"\n",
        "    mac_addr = ':'.join(format((uuid.getnode() >> i) & 0xff, '02x') for i in reversed(range(0, 48, 8)))\n",
        "    print(\"+++ Acknowledgement +++\")\n",
        "    print(f\"Executed on: {datetime.datetime.now()}\")\n",
        "    print(f\"In Google Colab: {colab_check}\")\n",
        "    print(f\"System info: {platform.system()} {platform.release()}\")\n",
        "    print(f\"Node name: {platform.node()}\")\n",
        "    print(f\"MAC address: {mac_addr}\")\n",
        "    try:\n",
        "        print(f\"IP address: {socket.gethostbyname(socket.gethostname())}\")\n",
        "    except:\n",
        "        print(\"IP address: Unknown\")\n",
        "    print(f\"Signing off, {name}\")\n",
        "\n",
        "signoff(\"Ali Muhammad Asad\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gXOJwOqr8vxB"
      },
      "id": "gXOJwOqr8vxB",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "name": "Multilayer_Perceptron_Dr_Adnan_Masood",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}