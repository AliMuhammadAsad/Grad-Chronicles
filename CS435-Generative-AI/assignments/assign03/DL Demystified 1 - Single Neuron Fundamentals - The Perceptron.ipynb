{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "w8HkY3EGT2Z5",
   "metadata": {
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1737815635970,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "w8HkY3EGT2Z5"
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "#                                                               #\n",
    "#  CS435 Generative AI: Security, Ethics and Governance         #\n",
    "#                                                               #\n",
    "#  Instructor: Dr. Adnan Masood                                 #\n",
    "#  Contact:    adnanmasood@gmail.com                            #\n",
    "#                                                               #\n",
    "#  Notebook is MIT Licensed                                     #\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7xQSR6hQT0sh",
   "metadata": {
    "id": "7xQSR6hQT0sh"
   },
   "source": [
    "# **Perceptron Explained **\n",
    "\n",
    "Welcome, everyone! In this Jupyter notebook, we will explore the Perceptron algorithm in detail We will cover its **history**, the **math** behind it, **intuitive examples**, **code demonstrations**, practical problem-solving applications, and more.\n",
    "\n",
    "Let's jump right in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eYSXQ0T0sj",
   "metadata": {
    "id": "70eYSXQ0T0sj"
   },
   "source": [
    "## Building the Intuition\n",
    "\n",
    "- Think of the Perceptron as a simple decision maker. It takes some numbers (like test scores), adds them up, and then decides if the result is above or below a certain mark (like a passing grade). If it's above, it says \"Yes\"; if it's below, it says \"No.\"\n",
    "\n",
    "- A Perceptron is a small machine that takes inputs (like features about something), multiplies each input by a number (called a weight), adds them up, then adds a little extra number (called bias). Finally, it passes the result through a step function which outputs either 0 or 1.\n",
    "\n",
    "- This means the Perceptron is deciding whether something meets a condition or not, much like flipping a switch.\n",
    "\n",
    "- A Perceptron is one of the earliest forms of a neural network unit. It takes an input vector $\\mathbf{x} = (x_1, x_2, ..., x_n)$. Each input $x_i$ is multiplied by a weight $w_i$, and a bias term $b$ is added. We then apply an activation function $\\phi$, typically a step function:\n",
    "  $$\n",
    "  \\text{output} = \\phi\\left(\\sum_{i=1}^{n} w_i x_i + b\\right)\n",
    "  $$\n",
    "- If the sum is greater than some threshold (often 0 when using a bias), the Perceptron outputs 1; otherwise, 0.\n",
    "\n",
    "- The Perceptron is a linear binary classifier. It is trained using the Perceptron Learning Algorithm (PLA), which updates weights based on misclassifications:\n",
    "  $$\n",
    "  w_i \\leftarrow w_i + \\eta (t - o) x_i,\\quad b \\leftarrow b + \\eta (t - o)\n",
    "  $$\n",
    "  where $\\eta$ is the learning rate, $t$ is the target label, $o$ is the output, and $x_i$ is the input.\n",
    "\n",
    "- Convergence is guaranteed if the data is linearly separable.\n",
    "\n",
    "- The Perceptron can be seen as a foundational model for computational learning theory. Under the Probably Approximately Correct (PAC) framework, the Perceptron algorithm can learn any linearly separable function in a finite number of steps proportional to the margin and data dimension (Novikoff bound).\n",
    "\n",
    "- Extensions of the Perceptron, such as the kernel Perceptron, leverage kernel functions to deal with non-linearly separable data in a higher-dimensional feature space. This forms the bedrock of many advanced techniques in modern machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Lhd6ImwuT0sj",
   "metadata": {
    "id": "Lhd6ImwuT0sj"
   },
   "source": [
    "## **2. Brief History and Invention**\n",
    "\n",
    "The Perceptron was invented by **Frank Rosenblatt** in 1957 at the Cornell Aeronautical Laboratory. It was inspired by the earlier work of Warren McCulloch and Walter Pitts on simplified neurons. Rosenblatt's Perceptron gained significant attention as a promising step toward \"thinking machines.\" However, it was later criticized (notably by Minsky and Papert) for its inability to solve certain problems (like the XOR problem) due to its linear nature.\n",
    "\n",
    "Despite setbacks, the Perceptron laid the groundwork for future developments in **artificial neural networks** and **deep learning**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6vbiaupJT0sj",
   "metadata": {
    "id": "6vbiaupJT0sj"
   },
   "source": [
    "## **3. Underlying Technology (How It Works)**\n",
    "\n",
    "**Structure**:\n",
    "- Inputs: $x_1, x_2, ..., x_n$\n",
    "- Weights: $w_1, w_2, ..., w_n$\n",
    "- Bias: $b$\n",
    "- Activation function: typically a step function\n",
    "\n",
    "**Working**:\n",
    "1. Compute weighted sum: $z = \\sum_{i=1}^n w_i x_i + b$.\n",
    "2. Pass $z$ through an activation function (often a Heaviside step function):\n",
    "   $$\n",
    "   \\phi(z) =\n",
    "   \\begin{cases}\n",
    "     1 & z \\ge 0,\\\\\n",
    "     0 & z < 0.\n",
    "   \\end{cases}\n",
    "   $$\n",
    "3. The output is a **binary** decision (0 or 1, or sometimes -1 or +1).\n",
    "\n",
    "**Intuitive Explanation**:\n",
    "- Each weight $w_i$ tells us how important the corresponding input $x_i$ is.\n",
    "- The bias $b$ is like a baseline that shifts the decision boundary.\n",
    "- If the total influence (weighted sum + bias) is large enough, the Perceptron outputs 1 (true/positive). Otherwise, it outputs 0 (false/negative).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7Kvm3xhxT0sk",
   "metadata": {
    "id": "7Kvm3xhxT0sk"
   },
   "source": [
    "## **4. Math Behind the Perceptron (ELI5)**\n",
    "\n",
    "**Equation**:\n",
    "$$\n",
    "\\text{Output} = \\phi\\left(\\sum_{i=1}^{n} w_i x_i + b\\right) \\quad\\text{where}\\quad \\phi(z) =\n",
    "\\begin{cases}\n",
    "1 & z \\ge 0,\\\\\n",
    "0 & z < 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- The weights $w_i$ and bias $b$ are parameters that we adjust during **training**.\n",
    "- If we think about a 2D plane (just 2 inputs $x_1$ and $x_2$), the decision boundary is a line given by $w_1 x_1 + w_2 x_2 + b = 0$.\n",
    "- Training involves moving this line around until the Perceptron correctly classifies the training data.\n",
    "\n",
    "## **5. A Quick Illustrative Example**\n",
    "Let's consider a super-simple dataset:\n",
    "- We want the Perceptron to learn the **AND** logical function.\n",
    "  - (0, 0) -> 0\n",
    "  - (0, 1) -> 0\n",
    "  - (1, 0) -> 0\n",
    "  - (1, 1) -> 1\n",
    "\n",
    "Below is some sample code (ELI5 style) demonstrating how to train a Perceptron on this small dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "qDtRucnGT0sk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1737815636276,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "qDtRucnGT0sk",
    "outputId": "95f575dc-5cb1-493a-ca5c-c805daf2eef2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained weights: [0.19671415 0.0617357 ]\n",
      "Trained bias: -0.2523114618993075\n",
      "Input: [0 0], Predicted: 0, True: 0\n",
      "Input: [0 1], Predicted: 0, True: 0\n",
      "Input: [1 0], Predicted: 0, True: 0\n",
      "Input: [1 1], Predicted: 1, True: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define our input data (X) and labels (y)\n",
    "X = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "y = np.array([0, 0, 0, 1])  # AND function labels\n",
    "\n",
    " # Initialize weights and bias\n",
    "np.random.seed(42)  # For reproducibility\n",
    "w = np.random.randn(2)  # random weights\n",
    "b = np.random.randn()   # random bias\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 10\n",
    "\n",
    "def step_function(z):\n",
    "    return 1 if z >= 0 else 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, x in enumerate(X):\n",
    "        # Compute weighted sum\n",
    "        z = np.dot(w, x) + b\n",
    "        # Perceptron output\n",
    "        prediction = step_function(z)\n",
    "        # Calculate error\n",
    "        error = y[i] - prediction\n",
    "        # Update weights and bias\n",
    "        w += learning_rate * error * x\n",
    "        b += learning_rate * error\n",
    "\n",
    "print(\"Trained weights:\", w)\n",
    "print(\"Trained bias:\", b)\n",
    "\n",
    "# Test the perceptron\n",
    "for i, x in enumerate(X):\n",
    "    z = np.dot(w, x) + b\n",
    "    prediction = step_function(z)\n",
    "    print(f\"Input: {x}, Predicted: {prediction}, True: {y[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qRnAg6h_T0sl",
   "metadata": {
    "id": "qRnAg6h_T0sl"
   },
   "source": [
    "### **Mock Calculation Example**\n",
    "Let's do a tiny mock calculation for a single input from the AND dataset, say \\(x = (1, 1)\\), with **imagined** weights and bias:\n",
    "- Suppose \\(w = (0.5, 0.5)\\) and \\(b = -0.8\\).\n",
    "- Weighted sum: \\(z = 0.5*1 + 0.5*1 - 0.8 = 1 - 0.8 = 0.2\\).\n",
    "- Since \\(z = 0.2 \\ge 0\\), the step function returns 1.\n",
    "- If the correct label was 1, there's no error and no update. If the label was 0, we'd need to adjust weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HvB2KtHBT0sl",
   "metadata": {
    "id": "HvB2KtHBT0sl"
   },
   "source": [
    "## **6. Step-by-Step Perceptron Creation from Scratch**\n",
    "1. **Gather Data**: Create or collect labeled training examples (e.g., (features) -> (label)).\n",
    "2. **Initialize Parameters**: Choose random weights and a random bias.\n",
    "3. **Define an Activation Function**: Often a step function.\n",
    "4. **Compute Output**: For each training example, compute $z = w \\cdot x + b$. Output $\\phi(z)$.\n",
    "5. **Compare Output with True Label**: If there's a mismatch (error), update weights and bias:\n",
    "   $$\n",
    "   w_i = w_i + \\eta (t - o)x_i,\\quad b = b + \\eta (t - o)\n",
    "   $$\n",
    "6. **Repeat**: Keep looping over the dataset until the errors are minimized or you reach a max number of epochs.\n",
    "\n",
    "This is the **Perceptron Learning Algorithm**!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "WVx8fp9_T0sl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1737815636276,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "WVx8fp9_T0sl",
    "outputId": "fe1690b6-3fef-4608-e9e6-96c0cd9c7e5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned weights: [0.86405235 0.10015721]\n",
      "Learned bias: -0.7212620158942606\n",
      "Predictions on AND data: [0 0 1 1]\n",
      "True labels: [0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PerceptronScratch:\n",
    "    def __init__(self, learning_rate=0.1, epochs=10):\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def step_function(self, z):\n",
    "        return 1 if z >= 0 else 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # X is shape (num_samples, num_features)\n",
    "        # y is shape (num_samples,)\n",
    "        num_samples, num_features = X.shape\n",
    "        # Initialize weights and bias randomly\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_features)\n",
    "        self.b = np.random.randn()\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            for i in range(num_samples):\n",
    "                z = np.dot(self.w, X[i]) + self.b\n",
    "                prediction = self.step_function(z)\n",
    "                error = y[i] - prediction\n",
    "                # Update rule\n",
    "                self.w += self.lr * error * X[i]\n",
    "                self.b += self.lr * error\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.w) + self.b\n",
    "        # Vectorized step function\n",
    "        return np.array([1 if val >= 0 else 0 for val in z])\n",
    "\n",
    "# Let's test our from-scratch Perceptron with the AND data again.\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 0, 0, 1])\n",
    "\n",
    "perceptron = PerceptronScratch(learning_rate=0.1, epochs=10)\n",
    "perceptron.fit(X, y)\n",
    "print(\"Learned weights:\", perceptron.w)\n",
    "print(\"Learned bias:\", perceptron.b)\n",
    "\n",
    "predictions = perceptron.predict(X)\n",
    "print(\"Predictions on AND data:\", predictions)\n",
    "print(\"True labels:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VTa0HgHZT0sl",
   "metadata": {
    "id": "VTa0HgHZT0sl"
   },
   "source": [
    "## **7. Illustrative Problem It Solves**\n",
    "- **Logical Gates**: As demonstrated, a Perceptron can learn logical operations like AND, OR, etc.\n",
    "- **Linear Classification**: For instance, determining if a point is above or below a line in 2D.\n",
    "\n",
    "## **8. Practical Problem It Solves**\n",
    "- **Spam Detection (Simple Version)**: Suppose we have features like `contains_word_free = 1/0`, `contains_link = 1/0`, etc. A Perceptron can combine these features with weights. If the sum is above a threshold, label the email as spam.\n",
    "- **Image Classification (Basic)**: If your images are small and data is linearly separable, a Perceptron can do a crude classification.\n",
    "\n",
    "## **9. Solving a Real World Problem**\n",
    "If we had a labeled dataset of small images or textual features, the Perceptron could be quickly trained to do a binary classification (e.g., \"Cat vs. Not Cat\"). Of course, for complex tasks, we now use deeper neural networks, but the Perceptron is the conceptual building block behind them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t2pfCCP0T0sl",
   "metadata": {
    "id": "t2pfCCP0T0sl"
   },
   "source": [
    "## **10. Questions to Illustrate the Use\n",
    "1. **What happens if the data is not linearly separable?**\n",
    "2. **How does the Perceptron update its weights?**\n",
    "3. **Why does the bias term matter?**\n",
    "4. **What is the decision boundary for a Perceptron with two inputs?**\n",
    "5. **How many epochs are enough?**\n",
    "6. **Can we use a different activation function?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UhuB4kuGT0sl",
   "metadata": {
    "id": "UhuB4kuGT0sl"
   },
   "source": [
    "## **11. Answers**\n",
    "\n",
    "1. **Non-linearly separable data**: The plain Perceptron can't perfectly classify data that's not linearly separable. It will keep trying to update weights forever (or until you stop). We need multiple perceptrons (multi-layer) or other methods (like kernels or advanced architectures) for non-linear boundaries.\n",
    "2. **Weight update**: If an example is misclassified, weights are nudged in a direction that makes the output more correct. The update rule is $w_i = w_i + \\eta (t - o) x_i$, and $b = b + \\eta (t - o)$.\n",
    "3. **Bias term**: The bias shifts the decision boundary away from the origin. Without bias, your decision boundary always has to pass through the origin (when all inputs are zero).\n",
    "4. **Decision boundary (2 inputs)**: For $x_1$ and $x_2$, the boundary is the line $w_1 x_1 + w_2 x_2 + b = 0$.\n",
    "5. **How many epochs?**: There's no fixed rule. Often, we set a maximum number of epochs. If data is linearly separable, theoretically it will converge. Practically, we stop when misclassifications are zero or below some threshold.\n",
    "6. **Different activation function**: Yes! We can replace the step function with sigmoid, ReLU, or others. This transforms the Perceptron into different forms of neural network units (like logistic regression or modern neuron units).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "JBT8v0dST0sl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 171,
     "status": "ok",
     "timestamp": 1737815693017,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "JBT8v0dST0sl",
    "outputId": "52a5c4ed-224f-447d-b67b-abe24fb2eb28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 1 1 1]\n",
      "True labels: [0 1 1 1]\n",
      "TODO exercise is set up. Please open the SimplePerceptron class and fill in the missing parts!\n"
     ]
    }
   ],
   "source": [
    "# 12. Code Example with TODO items\n",
    "# Let's create a small exercise for students:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class SimplePerceptron:\n",
    "    def __init__(self, lr=0.01, epochs=5):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def step_function(self, z):\n",
    "        return 1 if z >= 0 else 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # TODO 1: Initialize self.w and self.b\n",
    "        # hint: use np.random.randn for random initialization\n",
    "        num_features = X.shape[1]\n",
    "        self.w = np.random.randn(X.shape[1])\n",
    "        self.b = np.random.randn()\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            for i, x in enumerate(X):\n",
    "                # TODO 2: Compute the weighted sum z\n",
    "                # z = ?\n",
    "                z = np.dot(self.w, x) + self.b\n",
    "\n",
    "                # TODO 3: Apply the step function to get the prediction\n",
    "                # pred = ?\n",
    "                pred = self.step_function(z)\n",
    "\n",
    "                # TODO 4: Compute the error\n",
    "                # error = ?\n",
    "                error = y[i] - pred\n",
    "\n",
    "                # TODO 5: Update the weights and bias using the perceptron rule\n",
    "                # self.w += ?\n",
    "                # self.b += ?\n",
    "                self.w += self.lr * error * x\n",
    "                self.b += self.lr * error\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        # We'll do the step function for each input in X\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            z = np.dot(self.w, x) + self.b\n",
    "            predictions.append(self.step_function(z))\n",
    "        return np.array(predictions)\n",
    "\n",
    "# Let's create a small dataset for the exercise - the OR function.\n",
    "X_todo = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "y_todo = np.array([0, 1, 1, 1])  # OR function\n",
    "\n",
    "perceptron_todo = SimplePerceptron(lr=0.1, epochs=10)\n",
    "\n",
    "############################################\n",
    "# Students will fill in the TODOs above.  #\n",
    "# After filling in, they can run:         #\n",
    "############################################\n",
    "perceptron_todo.fit(X_todo, y_todo)\n",
    "preds = perceptron_todo.predict(X_todo)\n",
    "print(\"Predictions:\", preds)\n",
    "print(\"True labels:\", y_todo)\n",
    "\n",
    "print(\"TODO exercise is set up. Please open the SimplePerceptron class and fill in the missing parts!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9VO94UzXT0sm",
   "metadata": {
    "id": "9VO94UzXT0sm"
   },
   "source": [
    "## **13. Glossary**\n",
    "\n",
    "- **Weights (\\(w_i\\))**: The parameters that scale the importance of each input.\n",
    "- **Bias (\\(b\\))**: A constant term added to shift the decision boundary.\n",
    "- **Activation Function**: A function (like step, sigmoid, ReLU) that decides the output based on the weighted sum.\n",
    "- **Linearly Separable**: A dataset is linearly separable if there exists a hyperplane (line in 2D, plane in 3D, etc.) that can perfectly separate the classes.\n",
    "- **Epoch**: One full pass over the entire training dataset.\n",
    "- **Learning Rate (\\(\\eta\\))**: A small number controlling how much we adjust weights/bias on each error.\n",
    "- **Perceptron Learning Algorithm**: The procedure to update weights/bias to reduce misclassification.\n",
    "- **Decision Boundary**: In 2D, it's a line; in nD, it's a hyperplane where \\(w\\cdot x + b = 0\\).\n",
    "- **Neural Network**: A collection of interconnected perceptrons (neurons) in multiple layers (multi-layer perceptrons, deep networks, etc.).\n",
    "\n",
    "Happy coding!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "PCs34W0cV5oq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 163,
     "status": "ok",
     "timestamp": 1737815697049,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "PCs34W0cV5oq",
    "outputId": "7204adfa-2087-4abf-8acd-a524e9f1b1ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Acknowledgement +++\n",
      "Executed on: 2025-01-27 15:13:03.599349\n",
      "In Google Colab: No\n",
      "System info: Linux 6.8.0-51-generic\n",
      "Node name: alimuhammad-Inspiron-7559\n",
      "MAC address: 20:47:47:74:94:05\n",
      "IP address: 127.0.1.1\n",
      "Signing off, Ali Muhammad Asad\n"
     ]
    }
   ],
   "source": [
    "import os, sys, platform, datetime, uuid, socket\n",
    "\n",
    "def signoff(name=\"Anonymous\"):\n",
    "    colab_check = \"Yes\" if 'google.colab' in sys.modules else \"No\"\n",
    "    mac_addr = ':'.join(format((uuid.getnode() >> i) & 0xff, '02x') for i in reversed(range(0, 48, 8)))\n",
    "    print(\"+++ Acknowledgement +++\")\n",
    "    print(f\"Executed on: {datetime.datetime.now()}\")\n",
    "    print(f\"In Google Colab: {colab_check}\")\n",
    "    print(f\"System info: {platform.system()} {platform.release()}\")\n",
    "    print(f\"Node name: {platform.node()}\")\n",
    "    print(f\"MAC address: {mac_addr}\")\n",
    "    try:\n",
    "        print(f\"IP address: {socket.gethostbyname(socket.gethostname())}\")\n",
    "    except:\n",
    "        print(\"IP address: Unknown\")\n",
    "    print(f\"Signing off, {name}\")\n",
    "\n",
    "signoff(\"Ali Muhammad Asad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a54168",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "description": "A comprehensive notebook on the Perceptron, taught by Dr. Adnan Masood.",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "name": "Perceptron_Explained.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
