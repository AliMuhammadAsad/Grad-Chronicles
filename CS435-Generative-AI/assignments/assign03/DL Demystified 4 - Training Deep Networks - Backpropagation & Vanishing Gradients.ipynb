{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fZXER-4NzuZF",
   "metadata": {
    "executionInfo": {
     "elapsed": 212,
     "status": "ok",
     "timestamp": 1737814927314,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "fZXER-4NzuZF"
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "#                                                               #\n",
    "#  CS435 Generative AI: Security, Ethics and Governance         #\n",
    "#                                                               #\n",
    "#  Instructor: Dr. Adnan Masood                                 #\n",
    "#  Contact:    adnanmasood@gmail.com                            #\n",
    "#                                                               #\n",
    "#  Notebook is MIT Licensed                                     #\n",
    "#################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XelEmWgrzuZH",
   "metadata": {
    "id": "XelEmWgrzuZH"
   },
   "source": [
    "# Teaching Backpropagation and Vanishing Gradients in Neural Networks\n",
    "**Instructor:** Dr. Adnan Masood\n",
    "\n",
    "Welcome everyone! In this session, we will dive deep into two key concepts in Neural Networks:\n",
    "1. **Backpropagation** (often shortened to *backprop*)\n",
    "2. **Vanishing Gradients**\n",
    "\n",
    "We will explore these topics at **five different levels** (from very simple to more advanced), building from intuitive explanations all the way to code and mock calculations.\n",
    "We'll also provide a **brief history**, **underlying math**, a **step-by-step example** of how to create the technology from scratch, and how it can be used to solve both illustrative and practical, real-world problems. Finally, we’ll have a **Q&A section**, some code examples with **TODOs**, and a **Glossary** of terms at the end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9lnRnHFzuZH",
   "metadata": {
    "id": "b9lnRnHFzuZH"
   },
   "source": [
    "## 1. Building an Intuitive Understanding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dfgoVHzuZH",
   "metadata": {
    "id": "79dfgoVHzuZH"
   },
   "source": [
    "**What is Backpropagation?**\n",
    "- Imagine you are learning to shoot basketball hoops. Each time you shoot, you see how far you missed and try to correct your aim. Over time, you adjust your arm's angle, strength, etc. until you improve.\n",
    "- Similarly, in a neural network, backpropagation is the way a computer learns from mistakes. It sees how \"off\" its guess was (the miss) and adjusts all the \"knobs\" (weights and biases) so the next guess is better.\n",
    "\n",
    "**What is Vanishing Gradient?**\n",
    "- If you had to pass a message through a big chain of friends, sometimes the message gets quieter or lost by the time it reaches the last friend. That's what happens in a very deep neural network: the \"fix-it\" signal (gradient) can get so small (vanish) that the first layers barely learn anything.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ToOhKleOzuZH",
   "metadata": {
    "id": "ToOhKleOzuZH"
   },
   "source": [
    "\n",
    "**Backpropagation** is:\n",
    "- A method to train multi-layer neural networks.\n",
    "- It uses the *chain rule* from calculus to figure out how to change each weight in the network in order to reduce the error.\n",
    "- The network's output is compared to the correct result, we get the error (loss), and then we move backwards updating weights to minimize this error.\n",
    "\n",
    "**Vanishing Gradient** is:\n",
    "- An issue that occurs in deep networks (many layers), especially with certain activation functions like the sigmoid.\n",
    "- Because the gradient is multiplied many times as it goes back through layers, it can become extremely small, leading the earliest layers to stop learning effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Fy0TZvwKzuZI",
   "metadata": {
    "id": "Fy0TZvwKzuZI"
   },
   "source": [
    "**Backpropagation**:\n",
    "- Is the algorithm that makes deep learning feasible. Without it, it would be extremely difficult to train multi-layer networks.\n",
    "- Uses gradient descent (or variants) to iteratively update weights.\n",
    "- The gradient of the loss function w.r.t. each weight is computed using partial derivatives. This leverages the chain rule.\n",
    "\n",
    "**Vanishing Gradient**:\n",
    "- Happens when partial derivatives of activation functions become too small.\n",
    "- Example: Sigmoid activation $\\sigma(x) = \\frac{1}{1+e^{-x}}$. The derivative is $\\sigma'(x) = \\sigma(x)(1-\\sigma(x))$. For large positive or negative $x$, the slope is nearly 0.\n",
    "- Repeated multiplication of small derivatives across many layers yields a near-zero gradient, stalling training.\n",
    "- Modern techniques to mitigate it: ReLU activation, batch normalization, careful initialization, skip connections (ResNets), etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9_irqMaTzuZI",
   "metadata": {
    "id": "9_irqMaTzuZI"
   },
   "source": [
    "\n",
    "- Derives from the **reverse-mode differentiation** technique.\n",
    "- If we define a loss function $L$ and a chain of transformations (layers) $f^{(1)}, f^{(2)}, \\dots, f^{(n)}$, the backprop step is essentially computing $\\frac{\\partial L}{\\partial w_i}$ for each parameter $w_i$ efficiently.\n",
    "- Complexity is roughly linear in the number of parameters (rather than exponential if you tried naive forward differences).\n",
    "\n",
    "**Vanishing Gradient**:\n",
    "- A major challenge in training Recurrent Neural Networks (RNNs) and older feedforward models with many layers.\n",
    "- Often tackled by using gating mechanisms (e.g. LSTM, GRU in RNNs) or architectures designed to have shortcuts.\n",
    "- When the gradient is too small, it doesn’t effectively update earlier weights. In extreme cases, those weights become “frozen.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rg5dkFjRzuZI",
   "metadata": {
    "id": "rg5dkFjRzuZI"
   },
   "source": [
    "\n",
    "- **Historical context**: Backprop was popularized in the 1980s by Rumelhart, Hinton, and Williams. The concept dates back earlier in control theory and was re-discovered for neural networks. It laid the foundation for modern deep learning.\n",
    "- **Mathematical foundation**: Based on applying the chain rule to compute gradients of parameterized compositions of functions. Reverse-mode auto-differentiation offers a computationally efficient solution.\n",
    "- **Vanishing and Exploding Gradients**: Exploding gradients also appear in deep networks. Techniques like gradient clipping, careful initialization, orthogonal or identity-based initialization in RNNs, etc. are used. In deep feed-forward networks, skip connections (e.g., ResNet) help preserve gradient flow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zr0pEnwszuZI",
   "metadata": {
    "id": "zr0pEnwszuZI"
   },
   "source": [
    "## 2. Brief History and Underlying Technology\n",
    "- Backpropagation was described in a famous 1986 paper by **Rumelhart, Hinton, and Williams**: \"Learning representations by back-propagating errors\".\n",
    "- The core idea, *gradient-based optimization*, was well-known in mathematics, but its application to multi-layer neural networks revolutionized the field.\n",
    "- Early networks faced challenges like computational limits and lack of big datasets. In the mid-2000s, with GPUs and large datasets, backprop-based deep neural networks resurged.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0YLNGL1SzuZI",
   "metadata": {
    "id": "0YLNGL1SzuZI"
   },
   "source": [
    "## 3. Intuition Behind Backpropagation\n",
    "- You have a neural network making predictions $y_{pred}$ for a target $y_{true}$.\n",
    "- Compute the **loss** $L(y_{pred}, y_{true})$. Often Mean Squared Error (MSE) or Cross-Entropy Loss.\n",
    "- We want to adjust each weight and bias so the loss is minimized.\n",
    "- We measure **how much** each weight contributed to the error. The chain rule helps us compute these contributions.\n",
    "- Then we tweak the weights in the direction that reduces the error (this is the essence of gradient descent).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3HWSACeHzuZI",
   "metadata": {
    "id": "3HWSACeHzuZI"
   },
   "source": [
    "## 4. Math of Backprop\n",
    "### 4.1 Single Neuron Example\n",
    "- Suppose we have a neuron with input $x$, weight $w$, bias $b$, and output $\\hat{y}$.\n",
    "- The neuron’s output: $\\hat{y} = \\sigma(wx + b)$, where $\\sigma$ is an activation function (e.g. sigmoid).\n",
    "- Loss function example: $L = \\frac{1}{2}(y_{true} - \\hat{y})^2$ (MSE).\n",
    "- We want $\\frac{\\partial L}{\\partial w}$ and $\\frac{\\partial L}{\\partial b}$.\n",
    "\n",
    "**Step by step**:\n",
    "1. $ \\frac{\\partial L}{\\partial \\hat{y}} = \\hat{y} - y_{true}$ because of the derivative of $\\frac{1}{2}(error)^2$ w.r.t. $error$.\n",
    "2. $ \\frac{\\partial \\hat{y}}{\\partial z} = \\sigma'(z)$ where $z = wx + b$.\n",
    "3. $ \\frac{\\partial z}{\\partial w} = x$ and $ \\frac{\\partial z}{\\partial b} = 1$.\n",
    "\n",
    "**Chain rule**:\n",
    "- $ \\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial z} \\times \\frac{\\partial z}{\\partial w}$\n",
    "- $ \\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial z} \\times \\frac{\\partial z}{\\partial b}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4QntSQYozuZI",
   "metadata": {
    "id": "4QntSQYozuZI"
   },
   "source": [
    "## 5. Illustrative Example with Code\n",
    "We’ll build a simple 2-layer neural network to illustrate backprop.\n",
    "### 5.1 Example Calculations\n",
    "Assume we have inputs $x_1, x_2$ and a single hidden layer with weights $W^{(1)}$ and output layer with weights $W^{(2)}$. We'll do a forward pass, compute the loss, then show how partial derivatives are computed.\n",
    "We'll keep it symbolic here. In code, we'll do the actual numeric steps.\n",
    "\n",
    "- **Weights**: $w_1, w_2, ...$\n",
    "- **Biases**: $b_1, b_2, ...$\n",
    "- We compute the forward pass: $\\mathbf{h} = \\sigma(W^{(1)}x + b^{(1)})$ -> $\\hat{y} = \\sigma(W^{(2)} \\mathbf{h} + b^{(2)})$\n",
    "- Then the loss $L = f(\\hat{y}, y_{true})$.\n",
    "- Using the chain rule, we find partial derivatives of $L$ wrt each parameter. Then we do gradient descent: $ \\theta \\leftarrow \\theta - \\alpha \\frac{\\partial L}{\\partial \\theta}$, where $\\alpha$ is the learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XEGPApaxzuZI",
   "metadata": {
    "id": "XEGPApaxzuZI"
   },
   "source": [
    "### 5.2 Step by Step Example (From Scratch)\n",
    "1. **Initialize** random weights: $W^{(1)}$, $b^{(1)}$, $W^{(2)}$, $b^{(2)}$.\n",
    "2. **Forward pass**: compute hidden layer, then output, then the loss.\n",
    "3. **Backward pass**: compute partial derivatives of loss wrt each weight and bias.\n",
    "4. **Update** the weights using gradient descent.\n",
    "5. **Repeat** until convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7-_kk1RQzuZI",
   "metadata": {
    "id": "7-_kk1RQzuZI"
   },
   "source": [
    "## 6. Illustrative Problem Solved\n",
    "A simple example is to classify points as either inside or outside a circle (a basic classification). By adjusting the network’s weights via backprop, we can teach the neural network to recognize circular boundaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yGpFel26zuZI",
   "metadata": {
    "id": "yGpFel26zuZI"
   },
   "source": [
    "## 7. Real-World Problem\n",
    "Backprop is used everywhere in deep learning, for example:\n",
    "- **Image classification** (e.g. is this a cat or a dog?)\n",
    "- **Speech recognition** (transcribing speech to text)\n",
    "- **Natural language processing** (LLMs, text generation, translations)\n",
    "In all these tasks, the same principle applies: compute the error, backpropagate, adjust weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WclfGddEzuZJ",
   "metadata": {
    "id": "WclfGddEzuZJ"
   },
   "source": [
    "## 8. How to Solve a Real-World Problem Using This Tech\n",
    "1. Collect data (inputs and correct outputs).\n",
    "2. Design a neural network architecture.\n",
    "3. Initialize weights.\n",
    "4. Compute forward pass -> compute loss.\n",
    "5. Backprop and update weights.\n",
    "6. Repeat until loss is minimal or acceptable.\n",
    "7. Use the trained network for predictions on new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GILAbXPczuZJ",
   "metadata": {
    "id": "GILAbXPczuZJ"
   },
   "source": [
    "## 9. Questions to Ponder\n",
    "1. Why do we need an activation function?\n",
    "2. How does the choice of activation function affect vanishing gradients?\n",
    "3. How does learning rate influence training?\n",
    "4. How do we mitigate vanishing gradient problems?\n",
    "5. What are alternative optimizers to vanilla gradient descent?\n",
    "\n",
    "### Answers\n",
    "1. **Need for activation**: Without it, the network would be just a linear mapping, no matter how many layers. Activations add non-linearity.\n",
    "2. **Effect on vanishing gradients**: Sigmoid or tanh saturate for large inputs, making gradients small. ReLU avoids saturation in positive region.\n",
    "3. **Learning rate**: Too high -> might overshoot minima. Too low -> very slow convergence. Must pick carefully.\n",
    "4. **Mitigating vanishing gradients**: Use ReLU, batch normalization, skip connections, better initializations.\n",
    "5. **Alternative optimizers**: Momentum, RMSProp, Adam, etc. They can handle varying learning rates, accelerate convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XGvCmaHozuZJ",
   "metadata": {
    "id": "XGvCmaHozuZJ"
   },
   "source": [
    "## 10. Code Examples\n",
    "### 10.1 Minimal Working Example (All Completed)\n",
    "Below is a simple neural network from scratch in Python (using NumPy) that learns a small function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "kt1n9UXkzuZJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1116,
     "status": "ok",
     "timestamp": 1737815317344,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "kt1n9UXkzuZJ",
    "outputId": "1c2a191f-9a22-4bf0-ed4f-ee8afe6dae87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2558\n",
      "Epoch 2000, Loss: 0.2494\n",
      "Epoch 4000, Loss: 0.2454\n",
      "Epoch 6000, Loss: 0.2046\n",
      "Epoch 8000, Loss: 0.1532\n",
      "Final predictions:\n",
      "[[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "# Let's build and train a simple 2-layer neural network to learn XOR, for example.\n",
    "# We'll show how backprop is implemented manually.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# XOR data\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "np.random.seed(42)\n",
    "# Initialize weights for layer1 (2 -> 2), layer2 (2 -> 1)\n",
    "W1 = np.random.randn(2, 2)\n",
    "b1 = np.zeros((1, 2))\n",
    "W2 = np.random.randn(2, 1)\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "lr = 0.1  # learning rate\n",
    "epochs = 10000\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Compute loss (mean squared error)\n",
    "    loss = np.mean((y - a2) ** 2)\n",
    "\n",
    "    # Backprop\n",
    "    # dL/da2\n",
    "    dL_da2 = 2*(a2 - y) / y.shape[0]\n",
    "    # dL/dz2 = dL/da2 * da2/dz2\n",
    "    dL_dz2 = dL_da2 * sigmoid_deriv(z2)\n",
    "\n",
    "    # dL/dW2 = a1^T * dL_dz2\n",
    "    dW2 = np.dot(a1.T, dL_dz2)\n",
    "    db2 = np.sum(dL_dz2, axis=0, keepdims=True)\n",
    "\n",
    "    # dL/dz1 = dL/da1 * da1/dz1\n",
    "    # but dL/da1 = dL_dz2 * W2^T\n",
    "    dL_da1 = np.dot(dL_dz2, W2.T)\n",
    "    dL_dz1 = dL_da1 * sigmoid_deriv(z1)\n",
    "\n",
    "    dW1 = np.dot(X.T, dL_dz1)\n",
    "    db1 = np.sum(dL_dz1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update parameters\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "\n",
    "    if i % 2000 == 0:\n",
    "        print(f\"Epoch {i}, Loss: {loss:.4f}\")\n",
    "\n",
    "print(\"Final predictions:\")\n",
    "print(a2.round())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DNtkk6ZXzuZJ",
   "metadata": {
    "id": "DNtkk6ZXzuZJ"
   },
   "source": [
    "### 10.2 A Sample Exercise\n",
    "Below is a skeleton code for you to fill in. We’ll compute a similar network for a simple function. Follow the `TODO` comments to complete the missing lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tmip67ZvzuZJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "error",
     "timestamp": 1737814928432,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "tmip67ZvzuZJ",
    "outputId": "9eb70be2-a75a-4fbf-a03d-a51356d207bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.34477440568223433\n",
      "Epoch 1000, Loss: 0.09469907283773513\n",
      "Epoch 2000, Loss: 0.034526774761455825\n",
      "Epoch 3000, Loss: 0.016221608911122266\n",
      "Epoch 4000, Loss: 0.009673113121277846\n",
      "Trained!\n",
      "Test predictions: [[0.02746471]\n",
      " [0.08113197]\n",
      " [0.08197225]\n",
      " [0.88825806]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Complete the code and run!\n",
    "import numpy as np\n",
    "\n",
    "# Suppose we want to learn the AND function\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "np.random.seed(0)\n",
    "W1 = np.random.randn(2, 2)\n",
    "b1 = np.zeros((1, 2))\n",
    "W2 = np.random.randn(2, 1)\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    # TODO: implement derivative of sigmoid\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "lr = 0.1\n",
    "epochs = 5000\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Forward pass\n",
    "    # TODO: compute z1, a1, z2, a2 using W1, b1, W2, b2\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Loss\n",
    "    loss = np.mean((y - a2)**2)\n",
    "\n",
    "    # Backprop\n",
    "    # TODO: fill in the partial derivatives and parameter updates\n",
    "    dL_da2 = 2*(a2 - y) / y.shape[0]\n",
    "    dL_dz2 = dL_da2 * sigmoid_deriv(z2)\n",
    "    dW2 = np.dot(a1.T, dL_dz2)\n",
    "    db2 = np.sum(dL_dz2, axis=0, keepdims=True)\n",
    "\n",
    "    dL_da1 = np.dot(dL_dz2, W2.T)\n",
    "    dL_dz1 = dL_da1 * sigmoid_deriv(z1)\n",
    "    dW1 = np.dot(X.T, dL_dz1)\n",
    "    db1 = np.sum(dL_dz1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update\n",
    "    # TODO: update W2, b2, W1, b1 with lr * dW, dB\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Epoch {i}, Loss: {loss}\")\n",
    "\n",
    "print(\"Trained!\")\n",
    "print(\"Test predictions:\", a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cgyz4D6uzuZJ",
   "metadata": {
    "id": "cgyz4D6uzuZJ"
   },
   "source": [
    "## 11. Glossary\n",
    "- **Backpropagation**: Algorithm for computing gradients of a neural network’s parameters in reverse order of layers.\n",
    "- **Vanishing Gradients**: Situation where gradients become very small in early layers, hindering learning.\n",
    "- **Chain Rule**: A fundamental calculus rule used to compute the derivative of composite functions.\n",
    "- **Weight, Bias**: Learnable parameters of a neuron/network. The network adjusts these via training.\n",
    "- **Activation Function**: A non-linear function (sigmoid, ReLU, etc.) applied at each neuron.\n",
    "- **Loss Function**: Measures the difference between predictions and ground truth.\n",
    "- **Gradient**: The direction of steepest ascent for a function. We often move in the negative gradient direction to minimize loss.\n",
    "- **Learning Rate**: Hyperparameter that controls how big a step to take when updating weights.\n",
    "- **MSE (Mean Squared Error)**: A common loss function, average of the squares of errors.\n",
    "- **Cross-Entropy**: Another common loss function, especially for classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T2AU9CN6zuZJ",
   "metadata": {
    "id": "T2AU9CN6zuZJ"
   },
   "source": [
    "End of Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "byMfSwuVzuZJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 150,
     "status": "ok",
     "timestamp": 1737815293537,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "byMfSwuVzuZJ",
    "outputId": "07a2538f-2fcf-46a0-fb83-afa17e6f8557"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Acknowledgement +++\n",
      "Executed on: 2025-01-28 18:02:12.673953\n",
      "In Google Colab: No\n",
      "System info: Linux 6.8.0-51-generic\n",
      "Node name: alimuhammad-Inspiron-7559\n",
      "MAC address: 20:47:47:74:94:05\n",
      "IP address: 127.0.1.1\n",
      "Signing off, Ali Muhammad Asad\n"
     ]
    }
   ],
   "source": [
    "import os, sys, platform, datetime, uuid, socket\n",
    "\n",
    "def signoff(name=\"Anonymous\"):\n",
    "    colab_check = \"Yes\" if 'google.colab' in sys.modules else \"No\"\n",
    "    mac_addr = ':'.join(format((uuid.getnode() >> i) & 0xff, '02x') for i in reversed(range(0, 48, 8)))\n",
    "    print(\"+++ Acknowledgement +++\")\n",
    "    print(f\"Executed on: {datetime.datetime.now()}\")\n",
    "    print(f\"In Google Colab: {colab_check}\")\n",
    "    print(f\"System info: {platform.system()} {platform.release()}\")\n",
    "    print(f\"Node name: {platform.node()}\")\n",
    "    print(f\"MAC address: {mac_addr}\")\n",
    "    try:\n",
    "        print(f\"IP address: {socket.gethostbyname(socket.gethostname())}\")\n",
    "    except:\n",
    "        print(\"IP address: Unknown\")\n",
    "    print(f\"Signing off, {name}\")\n",
    "\n",
    "signoff(\"Ali Muhammad Asad\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
