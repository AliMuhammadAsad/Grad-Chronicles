{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01_E-tANwoM7",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1737813614265,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "01_E-tANwoM7"
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "#                                                               #\n",
    "#  CS435 Generative AI: Security, Ethics and Governance         #\n",
    "#                                                               #\n",
    "#  Instructor: Dr. Adnan Masood                                 #\n",
    "#  Contact:    adnanmasood@gmail.com                            #\n",
    "#                                                               #\n",
    "#  Notebook is MIT Licensed                                     #\n",
    "#################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497y6YYmwoM8",
   "metadata": {
    "id": "497y6YYmwoM8"
   },
   "source": [
    "# Recurrent Neural Networks (RNNs)\n",
    "Welcome to this comprehensive notebook on **Recurrent Neural Networks (RNNs)**. We will explore RNNs from multiple perspectives (five distinct levels) and build an intuition of how they work, their history, the math behind them, and how to code a simple RNN from scratch.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Building an Intuitive Understanding](#five-levels)\n",
    "2. [Intuition](#intuition)\n",
    "3. [Brief History](#history)\n",
    "4. [RNN Underlying Technology](#technology)\n",
    "5. [Math Behind RNNs](#math)\n",
    "6. [Example with Intuitive Code](#example)\n",
    "7. [Example Calculations](#mock-calculations)\n",
    "8. [Step-by-Step Example from Scratch](#step-by-step)\n",
    "9. [Illustrative Problem It Solves](#illustrative-problem)\n",
    "10. [Real World Problem](#real-world)\n",
    "11. [How to Solve a Real World Problem Using RNN](#solve-real-world)\n",
    "12. [Points to Ponder (Questions)](#points-to-ponder)\n",
    "13. [Answers & Code Examples](#answers)\n",
    "14. [A Sample Exercise](#todo-sample)\n",
    "15. [Glossary](#glossary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3xTXC89DwoM9",
   "metadata": {
    "id": "3xTXC89DwoM9",
    "tags": []
   },
   "source": [
    "<a id=\"five-levels\"></a>\n",
    "## 1. Building an Intuitive Understanding\n",
    "\n",
    "Think of RNNs like remembering what happened in the previous sentence when you read a book. They use their \"memory\" to keep track of information from past steps to understand what should come next.\n",
    "\n",
    "RNNs are special neural networks that process information one step at a time and remember what they have seen before. This memory lets them handle tasks like predicting the next word in a sentence or classifying text.\n",
    "\n",
    "RNNs process sequences of data (e.g., text, audio, or time series) by maintaining a hidden state that updates after each new data point. This hidden state is what allows them to keep information about previous elements in the sequence.\n",
    "\n",
    "RNNs implement a feedback loop in their architecture. At each time step, an RNN cell takes the current input and the hidden state from the previous time step to produce a new hidden state. However, training vanilla RNNs can lead to issues like vanishing or exploding gradients.\n",
    "\n",
    "More complex variants, such as LSTM (Long Short-Term Memory) networks and GRU (Gated Recurrent Units), address vanishing and exploding gradients by introducing gating mechanisms. These gates control the flow of information to and from the hidden state, enabling the network to \"forget\" or \"remember\" selectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zZWvD5HbwoM9",
   "metadata": {
    "id": "zZWvD5HbwoM9",
    "tags": []
   },
   "source": [
    "<a id=\"intuition\"></a>\n",
    "## 2. Intuition\n",
    "If you are reading a sentence in English, the words that came before might change how you interpret the next word. That is exactly the idea of RNNs – they remember what came before. They use a hidden state (like a short-term memory) to keep track of previous inputs and use this memory to help predict the next output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8E2YusrlwoM9",
   "metadata": {
    "id": "8E2YusrlwoM9",
    "tags": []
   },
   "source": [
    "<a id=\"history\"></a>\n",
    "## 3. Brief History\n",
    "RNNs have been around for decades. Early foundational work was done by **John Hopfield** in the 1980s with Hopfield networks. In 1989, **Williams and Zipser** studied gradient-based learning methods for recurrent networks. **Sepp Hochreiter** and **Jürgen Schmidhuber** introduced the LSTM (Long Short-Term Memory) in 1997 to address the vanishing gradient problem. Later modifications like **GRUs** (by Kyunghyun Cho and others) further optimized the gating mechanisms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1uk_Tp_2woM9",
   "metadata": {
    "id": "1uk_Tp_2woM9"
   },
   "source": [
    "<a id=\"technology\"></a>\n",
    "## 4. Underlying Technology\n",
    "- **Hidden State**: Each time step's memory\n",
    "- **Weights**: Matrices that transform input and hidden state\n",
    "- **Activation Function**: Often a $\\tanh$ or ReLU\n",
    "- **Loss Function**: Determines the error\n",
    "- **Backpropagation Through Time (BPTT)**: Specialized training method that unrolls the RNN across time steps\n",
    "\n",
    "RNNs differ from feed-forward networks by introducing feedback loops. This allows them to operate on sequences of varying lengths and maintain a form of memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rDFOJQ4DwoM9",
   "metadata": {
    "id": "rDFOJQ4DwoM9"
   },
   "source": [
    "<a id=\"math\"></a>\n",
    "## 5. Math Behind RNNs\n",
    "**Hidden State Update**:\n",
    "$$\n",
    "h_t = \\sigma (W_{xh} x_t + W_{hh} h_{t-1} + b_h)\n",
    "$$\n",
    "where:\n",
    "- $h_t$ is the hidden state at time $t$\n",
    "- $x_t$ is the input at time $t$\n",
    "- $W_{xh}$ and $W_{hh}$ are weight matrices\n",
    "- $b_h$ is the bias term\n",
    "- $\\sigma$ is an activation function (often $\\tanh$ or ReLU)\n",
    "\n",
    "**Output**:\n",
    "$$\n",
    "y_t = W_{hy} h_t + b_y\n",
    "$$\n",
    "where:\n",
    "- $y_t$ is the output at time $t$\n",
    "- $W_{hy}$ is the weight matrix for the hidden-to-output transformation\n",
    "- $b_y$ is the output bias\n",
    "\n",
    "The key point is that $h_t$ depends on $h_{t-1}$, so the network can remember information from previous time steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rf56o7Z2woM9",
   "metadata": {
    "id": "rf56o7Z2woM9"
   },
   "source": [
    "<a id=\"example\"></a>\n",
    "## 6. Example with Intuitive Code\n",
    "Imagine we have a sequence of letters `H E L L O`, and we want to predict the next letter in the sequence. We'll do a simple RNN approach:\n",
    "1. Convert letters to numbers (e.g., H=0, E=1, L=2, O=3).\n",
    "2. Pass them through an RNN.\n",
    "3. Predict the next letter.\n",
    "\n",
    "We'll use PyTorch to illustrate, but the idea is the same in other frameworks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Aq_dbsXQwoM-",
   "metadata": {
    "id": "Aq_dbsXQwoM-"
   },
   "source": [
    "### Here's a small code snippet to show a tiny RNN in action (no training, just forward pass):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "q2A4Fp0MwoM-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10380,
     "status": "ok",
     "timestamp": 1737813624805,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "execution_count": null,
    "id": "q2A4Fp0MwoM-",
    "outputId": "d46ea4a5-f415-497c-ad6d-e8c1ac77a33a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output (probabilities): tensor([[-1.6606, -0.9803, -1.5722, -1.4819]], grad_fn=<LogSoftmaxBackward0>)\n",
      "Hidden state: tensor([[ 0.0999,  0.0120,  0.2132, -0.3944, -0.2753,  0.2929,  0.0459, -0.1475]],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Let's define a simple RNN module.\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Weight matrices\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        hidden = torch.tanh(hidden)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "# Sample usage\n",
    "input_size = 4  # Suppose we have 4 possible letters\n",
    "hidden_size = 8\n",
    "output_size = 4  # Predict among 4 letters\n",
    "\n",
    "rnn = SimpleRNN(input_size, hidden_size, output_size)\n",
    "hidden = rnn.init_hidden()\n",
    "\n",
    "# Let's create a dummy input for 'H' (assuming one-hot encoding)\n",
    "h_letter = torch.tensor([[1, 0, 0, 0]], dtype=torch.float)\n",
    "\n",
    "output, hidden = rnn(h_letter, hidden)\n",
    "print(\"Output (probabilities):\", output)\n",
    "print(\"Hidden state:\", hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4O96m4gPwoM-",
   "metadata": {
    "id": "4O96m4gPwoM-"
   },
   "source": [
    "<a id=\"mock-calculations\"></a>\n",
    "## 7. Example Calculations\n",
    "Let's do a small, simplified mock calculation (ignoring actual matrix shapes for simplicity):\n",
    "\n",
    "1. **Weights** ($W_{xh}$, $W_{hh}$) are randomly initialized. Suppose we have a hidden size of 2.\n",
    "2. **Bias** ($b_h$) is also randomly initialized.\n",
    "3. **Input** at time $t$ is $x_t$. Let's say $x_t = [1, 0, 0, 0]$ for the letter 'H'.\n",
    "4. **Hidden State** at time $t-1$ is $h_{t-1}$. Suppose it starts as $[0, 0]$.\n",
    "5. The new hidden state: $h_t = \\tanh(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$.\n",
    "6. The output: $y_t = W_{hy} h_t + b_y$ -> we then apply something like softmax.\n",
    "7. **During training**, we compare the predicted output to the actual label (the next letter) and adjust weights via BPTT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4IHuqs_4woM-",
   "metadata": {
    "id": "4IHuqs_4woM-"
   },
   "source": [
    "<a id=\"step-by-step\"></a>\n",
    "## 8. Step-by-Step Example from Scratch\n",
    "1. **Prepare Data**: Convert text sequence into numeric form. (One-hot or embedded vectors)\n",
    "2. **Initialize RNN**: Decide input size, hidden size, output size.\n",
    "3. **Forward Pass**: For each time step:\n",
    "   - Compute new hidden state with old hidden state and current input.\n",
    "   - Compute output (prediction).\n",
    "4. **Loss Calculation**: Compare predictions with target labels.\n",
    "5. **Backpropagation Through Time (BPTT)**: Unroll the RNN and compute gradients for each time step.\n",
    "6. **Update Weights**: Adjust using optimizer (e.g. SGD, Adam).\n",
    "7. **Repeat** until predictions converge or we reach a desired accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JOI494erwoM-",
   "metadata": {
    "id": "JOI494erwoM-"
   },
   "source": [
    "<a id=\"illustrative-problem\"></a>\n",
    "## 9. Illustrative Problem It Solves\n",
    "RNNs are great for **sequence prediction**: for example, predicting the next word in a sentence. If your sentence is “I love going to the …,” the RNN will use context from the entire sentence to guess the next word (maybe “park” or “movies”).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1TWK0u8IwoM-",
   "metadata": {
    "id": "1TWK0u8IwoM-"
   },
   "source": [
    "<a id=\"real-world\"></a>\n",
    "## 10. Real World Problem\n",
    "A **real world** application is **sentiment analysis** on social media posts. RNNs can read each post word by word, maintain context, and classify if it's positive, negative, or neutral. They can also do **language translation** by reading one language's words and predicting the corresponding words in another language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UHDtUrQpwoM-",
   "metadata": {
    "id": "UHDtUrQpwoM-"
   },
   "source": [
    "<a id=\"solve-real-world\"></a>\n",
    "## 11. How to Solve a Real World Problem Using RNN\n",
    "1. **Data Collection**: Gather text data. Example: thousands of tweets labeled with sentiment.\n",
    "2. **Data Preprocessing**: Clean text, remove punctuation, convert to tokens.\n",
    "3. **Vectorization**: Use embeddings or one-hot encoding.\n",
    "4. **Build/Use RNN**: LSTM or GRU or Vanilla RNN.\n",
    "5. **Train**: With a labeled dataset, minimizing a loss function.\n",
    "6. **Evaluate**: Use accuracy, F1-score, etc.\n",
    "7. **Deploy**: Serve the model via an API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PHKprGdYwoM-",
   "metadata": {
    "id": "PHKprGdYwoM-"
   },
   "source": [
    "<a id=\"points-to-ponder\"></a>\n",
    "## 12. Points to Ponder (Questions)\n",
    "1. **How do RNNs handle long sequences?**\n",
    "2. **What is the vanishing gradient problem?**\n",
    "3. **How do LSTMs and GRUs differ from vanilla RNNs?**\n",
    "4. **What is BPTT (Backpropagation Through Time)?**\n",
    "5. **What are possible solutions to exploding gradients?**\n",
    "6. **Why do we often use embeddings instead of one-hot vectors?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qWKEJGSzwoM-",
   "metadata": {
    "id": "qWKEJGSzwoM-"
   },
   "source": [
    "<a id=\"answers\"></a>\n",
    "## 13. Answers\n",
    "1. **How do RNNs handle long sequences?**  \n",
    "   - They use hidden states to pass information forward. However, for very long sequences, they often rely on LSTM/GRU to mitigate forgetting.\n",
    "\n",
    "2. **What is the vanishing gradient problem?**  \n",
    "   - When gradients become extremely small during backprop, updates become negligible, and the network stops learning long-range dependencies.\n",
    "\n",
    "3. **How do LSTMs and GRUs differ from vanilla RNNs?**  \n",
    "   - They have gating mechanisms (like forget gate, input gate) to better control what is remembered and what is forgotten.\n",
    "\n",
    "4. **What is BPTT (Backpropagation Through Time)?**  \n",
    "   - It's the method of training RNNs by unfolding them across time steps and computing gradients at each step.\n",
    "\n",
    "5. **What are possible solutions to exploding gradients?**  \n",
    "   - Gradient clipping, using LSTM or GRU, proper initialization.\n",
    "\n",
    "6. **Why do we often use embeddings instead of one-hot vectors?**  \n",
    "   - Embeddings capture semantic relationships and reduce dimensionality compared to sparse one-hot vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GwCBiBntwoM_",
   "metadata": {
    "id": "GwCBiBntwoM_"
   },
   "source": [
    "<a id=\"todo-sample\"></a>\n",
    "## 14. A Sample Exercise\n",
    "Below is a minimal example of training a small RNN on a toy sequence task (predicting the next digit in a repeating sequence). Please complete the TODO sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lq5tWgctwoM_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "error",
     "timestamp": 1737813624805,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "execution_count": null,
    "id": "lq5tWgctwoM_",
    "outputId": "7a9c764e-fe46-4065-d5fe-7ac6a4dd38b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Loss: 0.3404\n",
      "Epoch 20/50, Loss: 0.1173\n",
      "Epoch 30/50, Loss: 0.0528\n",
      "Epoch 40/50, Loss: 0.0305\n",
      "Epoch 50/50, Loss: 0.0211\n",
      "Test input: [2,3], Predicted next number: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Toy dataset: A repeating sequence of [0,1,2,3], predict next in sequence\n",
    "# We'll create a small dataset of sequences of length 3, label is the 4th.\n",
    "\n",
    "sequence = [0, 1, 2, 3]\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "#TODO - There is an error in the following code, can you figure out the problem and fix it?\n",
    "# HINT - It's an IndexError: list index out of range. This means you're trying to access an element of\n",
    "#        the sequence list using an index that's larger than the list's size.\n",
    "# for i in range(len(sequence) - 1):\n",
    "#     data.append(sequence[i:i+2])  # e.g. [0,1], [1,2], [2,3]\n",
    "#     labels.append(sequence[i+2])  # e.g. next number\n",
    "\n",
    "for i in range(len(sequence) - 2):\n",
    "    data.append(sequence[i:i+2])  # e.g. [0,1], [1,2], [2,3]\n",
    "    labels.append(sequence[i+2])  # e.g. next number\n",
    "\n",
    "# Convert to tensors\n",
    "data_tensors = torch.tensor(data, dtype=torch.long)\n",
    "labels_tensors = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "class ToyRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ToyRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # We will use an embedding for input\n",
    "        self.embedding = nn.Embedding(input_size, input_size)\n",
    "\n",
    "        # RNN cell\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "\n",
    "        # Final output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        embedded = self.embedding(x)\n",
    "        # shape: (batch_size, seq_length, input_size)\n",
    "\n",
    "        # TODO: Initialize hidden state (size: 1 x batch_size x hidden_size)\n",
    "        # hidden = ???\n",
    "        hidden = torch.zeros(1, embedded.size(0), self.hidden_size)\n",
    "\n",
    "        # RNN forward\n",
    "        out, hidden = self.rnn(embedded, hidden)\n",
    "        # out shape: (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        # We only want the last time step's output\n",
    "        # TODO: Extract the last step's output from 'out'.\n",
    "        # last_out = ??? # shape: (batch_size, hidden_size)\n",
    "        last_out = out[:, -1, :]\n",
    "\n",
    "        # Pass through fully connected layer\n",
    "        logits = self.fc(last_out)\n",
    "        return logits\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 4  # digits 0,1,2,3\n",
    "hidden_size = 8\n",
    "output_size = 4\n",
    "lr = 0.01\n",
    "epochs = 50\n",
    "\n",
    "# Model, loss, optimizer\n",
    "model = ToyRNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward\n",
    "    logits = model(data_tensors)\n",
    "    loss = criterion(logits, labels_tensors)\n",
    "\n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test: let's predict the sequence [2,3] -> what is next?\n",
    "test_input = torch.tensor([[2, 3]], dtype=torch.long)\n",
    "with torch.no_grad():\n",
    "    prediction = model(test_input)\n",
    "    pred_label = torch.argmax(prediction, dim=1)\n",
    "    print(\"Test input: [2,3], Predicted next number:\", pred_label.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cs_n16vgwoM_",
   "metadata": {
    "id": "cs_n16vgwoM_"
   },
   "source": [
    "### Explanation:\n",
    "- **Embedding**: We turn the integer inputs (0,1,2,3) into embedding vectors.\n",
    "- **RNN**: Processes each sequence step by step.\n",
    "- **Hidden State**: Re-initialized for each sample.\n",
    "- **Fully Connected Layer**: Maps final hidden state to an output (one of 0,1,2,3).\n",
    "\n",
    "Fill the `TODO` comments (we already gave the typical solutions, but keep them to practice)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xKr2kXi_woM_",
   "metadata": {
    "id": "xKr2kXi_woM_"
   },
   "source": [
    "<a id=\"glossary\"></a>\n",
    "## 15. Glossary\n",
    "**RNN (Recurrent Neural Network)**: A type of neural network designed for sequential data processing.\n",
    "\n",
    "**Hidden State**: A vector that stores information from previous time steps.\n",
    "\n",
    "**Weight Matrices ($W_{xh}$, $W_{hh}$, $W_{hy}$)**: Parameters that transform input and hidden state.\n",
    "\n",
    "**Bias ($b_h$, $b_y$)**: Additional parameter added to the transformation for shifting.\n",
    "\n",
    "**Activation Function ($\\sigma$)**: A function (e.g. $\\tanh$, ReLU) applied after the linear transformation.\n",
    "\n",
    "**Backpropagation Through Time (BPTT)**: The process of training RNNs by unrolling them over time steps.\n",
    "\n",
    "**Vanishing Gradient**: A problem where gradients become extremely small, hindering learning.\n",
    "\n",
    "**Exploding Gradient**: A problem where gradients become extremely large, causing unstable updates.\n",
    "\n",
    "**Embedding**: A dense representation of input tokens (words, digits, etc.) in a lower-dimensional space.\n",
    "\n",
    "---\n",
    "**End of Notebook**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "-hVS6V5BwoM_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1737813786193,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "-hVS6V5BwoM_",
    "outputId": "48ad7475-903f-4aad-8356-e6e77737cd44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Acknowledgement +++\n",
      "Executed on: 2025-01-28 18:11:44.760650\n",
      "In Google Colab: No\n",
      "System info: Linux 6.8.0-51-generic\n",
      "Node name: alimuhammad-Inspiron-7559\n",
      "MAC address: 20:47:47:74:94:05\n",
      "IP address: 127.0.1.1\n",
      "Signing off, Ali Muhammad Asad\n"
     ]
    }
   ],
   "source": [
    "import os, sys, platform, datetime, uuid, socket\n",
    "\n",
    "def signoff(name=\"Anonymous\"):\n",
    "    colab_check = \"Yes\" if 'google.colab' in sys.modules else \"No\"\n",
    "    mac_addr = ':'.join(format((uuid.getnode() >> i) & 0xff, '02x') for i in reversed(range(0, 48, 8)))\n",
    "    print(\"+++ Acknowledgement +++\")\n",
    "    print(f\"Executed on: {datetime.datetime.now()}\")\n",
    "    print(f\"In Google Colab: {colab_check}\")\n",
    "    print(f\"System info: {platform.system()} {platform.release()}\")\n",
    "    print(f\"Node name: {platform.node()}\")\n",
    "    print(f\"MAC address: {mac_addr}\")\n",
    "    try:\n",
    "        print(f\"IP address: {socket.gethostbyname(socket.gethostname())}\")\n",
    "    except:\n",
    "        print(\"IP address: Unknown\")\n",
    "    print(f\"Signing off, {name}\")\n",
    "\n",
    "signoff(\"Ali Muhammad Asad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9c9d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
