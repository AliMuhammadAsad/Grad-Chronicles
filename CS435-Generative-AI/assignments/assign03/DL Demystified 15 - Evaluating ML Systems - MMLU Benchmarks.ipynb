{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hbnzf9A2d4GJ",
   "metadata": {
    "executionInfo": {
     "elapsed": 187,
     "status": "ok",
     "timestamp": 1737808680316,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "hbnzf9A2d4GJ"
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "#                                                               #\n",
    "#  CS435 Generative AI: Security, Ethics and Governance         #\n",
    "#                                                               #\n",
    "#  Instructor: Dr. Adnan Masood                                 #\n",
    "#  Contact:    adnanmasood@gmail.com                            #\n",
    "#                                                               #\n",
    "#  Notebook is MIT Licensed                                     #\n",
    "#################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Lhs3hqdOd4GM",
   "metadata": {
    "id": "Lhs3hqdOd4GM"
   },
   "source": [
    "# **LLM Evaluations with MMLU and Other Benchmarks**\n",
    "### *A Comprehensive, Step-by-Step Explanation and Example with PyTorch*\n",
    "By: **Dr. Adnan Masood**\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "1. **Building an Intuitive Understanding** (From \"Curious Adventurer\" to \"Doctoral Explorer\")\n",
    "2. **Intuition Behind LLM Evaluation**\n",
    "3. **Brief History and Underlying Tech**\n",
    "4. **Math Behind It**\n",
    "5. **Illustrative Example (Code + Explanation)**\n",
    "6. **Example Calculations** (weights, biases, etc.)\n",
    "7. **Step-by-Step Example: Building From Scratch**\n",
    "8. **Illustrative Problem It Solves**\n",
    "9. **Practical, Real-World Problem It Solves**\n",
    "10. **How to Solve a Real-World Problem Using This Tech**\n",
    "11. **Questions to Ask & Their Answers (with Code)**\n",
    "12. **A Sample Exercise**\n",
    "13. **Glossary**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FuU0z5LCd4GN",
   "metadata": {
    "id": "FuU0z5LCd4GN"
   },
   "source": [
    "## 1. **Building an Intuitive Understanding**\n",
    "\n",
    "We'll explain **LLM (Large Language Model) Evaluations** with a focus on the **MMLU benchmark** (Massive Multitask Language Understanding) and other benchmarks, at different depths. Think of it as layering new insights each time.\n",
    "\n",
    "- **What's an LLM?** A Large Language Model is like a very big \"predictive text\" that can answer questions, write stories, or have a conversation.\n",
    "- **What's MMLU?** It's a big test for these models. It has lots of questions on different subjects (math, history, science, etc.), and we see how well the model answers.\n",
    "- **Why do we test them?** Just like you take tests at school, we give tests to AI models to check what they know and how good they are at answering questions.\n",
    "\n",
    "- **LLMs** use huge amounts of text to \"learn\" patterns in language. They can complete sentences or answer queries.\n",
    "- **Evaluation**: We measure how well the model does on different benchmarks (like MMLU). MMLU is a set of exam-style questions across many academic subjects.\n",
    "- **Goal**: If an LLM can answer a wide variety of questions correctly, it shows it \"understands\" or can replicate knowledge from various fields.\n",
    "\n",
    "- **LLMs** rely on deep neural networks (e.g., Transformers) that learn word relationships.\n",
    "- **Benchmarks** like MMLU test the model’s ability to handle tasks across numerous academic disciplines, checking both factual recall and reasoning.\n",
    "- **Metrics**: Typically, we measure accuracy, or how often the model picks the correct answer. We may also measure other metrics like F1-score, perplexity, etc.\n",
    "- **Why MMLU?** MMLU covers 57 subjects, reflecting real-world academic tests. It’s a stress test for a model’s multitask knowledge.\n",
    "\n",
    "- **Transformer Architecture**: The LLM is built on multi-head self-attention layers, enabling it to weigh different parts of text differently.\n",
    "- **Fine-tuning & Prompt Engineering**: MMLU is used as a downstream evaluation benchmark. Models can be either fine-tuned or prompted to perform well.\n",
    "- **Domain Generalization**: The ability of a model to handle varied subjects with minimal parameter adjustments is key to success on MMLU.\n",
    "- **Evaluation Nuance**: We also consider calibrations (confidence measures), multi-step reasoning, and chain-of-thought prompts.\n",
    "\n",
    "- **Detailed Architecture**: LLMs typically rely on pretraining objectives (e.g., masked language modeling, next-token prediction) on massive corpora. They exploit emergent capabilities when scaled.\n",
    "- **Evaluation Techniques**: MMLU involves multiple-choice questions that gauge knowledge from high school to professional exam level. The choice of prompting strategy (few-shot, zero-shot, chain-of-thought) can drastically affect performance.\n",
    "- **Research Implications**: Because MMLU covers broad disciplines, it challenges the model’s reasoning, knowledge retrieval, and ability to handle out-of-distribution questions.\n",
    "- **Limitations**: Models might achieve good MMLU performance yet fail in interpretability, calibration, or real-world application constraints.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yWzMtTJYd4GN",
   "metadata": {
    "id": "yWzMtTJYd4GN"
   },
   "source": [
    "## 2. **Intuition Behind LLM Evaluation**\n",
    "\n",
    "- Large Language Models learn patterns by seeing tons of text examples.\n",
    "- We want to test if they can answer new questions they haven’t seen before.\n",
    "- Think of MMLU as a giant trivia quiz for the model—covering many domains. If the model truly \"understands\" or at least can replicate knowledge well, it will do well on these varied questions.\n",
    "- Evaluation helps us measure progress and compare different models or versions of models.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MQvTdV1Md4GN",
   "metadata": {
    "id": "MQvTdV1Md4GN"
   },
   "source": [
    "## 3. **Brief History, Invention, and Underlying Tech**\n",
    "\n",
    "- **Deep Neural Networks**: Before LLMs, we had simpler neural nets. But with the introduction of Transformers (Vaswani et al., 2017), we could handle large contexts.\n",
    "- **Pretraining & Fine-tuning**: GPT (Generative Pretrained Transformer) models from OpenAI, BERT from Google, and others popularized large-scale pretraining.\n",
    "- **MMLU**: Proposed as a comprehensive benchmark covering 57 subjects, from elementary to professional knowledge. The idea is to test a model’s broad knowledge.\n",
    "- **Underlying Tech**: High-dimensional vector representations (embeddings), attention mechanisms, massive parallel computing with GPUs/TPUs.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KlqxhcpMd4GO",
   "metadata": {
    "id": "KlqxhcpMd4GO"
   },
   "source": [
    "## 4. **Math Behind It**\n",
    "\n",
    "**Core Concept**: We use a neural network (often a Transformer) which does something like this:\n",
    "\n",
    "1. **Embedding**: Convert each word into a vector of numbers. Suppose a word `w` is mapped to a vector $\\mathbf{x}$ of size d.\n",
    "2. **Attention**: The model calculates how important each word is to every other word in the input. This can be written (in simplified form) as:\n",
    "$$\n",
    "  \\text{Attention}(Q, K, V) = \\text{softmax}\\Big(\\frac{Q K^T}{\\sqrt{d_k}}\\Big) V,\n",
    "$$\n",
    "   where\n",
    "   - $Q$ = Query matrix\n",
    "   - $K$ = Key matrix\n",
    "   - $V$ = Value matrix\n",
    "   - $d_k$ = dimension of the key vectors\n",
    "\n",
    "3. **Feed-Forward**: After attention, we pass the result through linear layers with activation functions (like ReLU or GELU).\n",
    "$$\n",
    "  \\mathbf{z} = \\text{ReLU}(W_2 \\,(\\text{ReLU}(W_1 \\, \\mathbf{h} + b_1)) + b_2)\n",
    "$$\n",
    "   This is a typical 2-layer feed-forward used in Transformers.\n",
    "\n",
    "4. **Output**: Eventually, the model learns to predict the next word or fill in masked words. For multiple choice, it can rank which answer is likely correct.\n",
    "\n",
    "5. **Evaluation**: We check if the highest probability answer is the correct one. Accuracy is calculated as:\n",
    "$$\n",
    "  \\text{Accuracy} = \\frac{\\text{Number of correct answers}}{\\text{Total questions}}.\n",
    "$$\n",
    "\n",
    "In MMLU, we have many different subjects, so we get an overall accuracy across all tasks.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ad1o5ogDd4GO",
   "metadata": {
    "id": "Ad1o5ogDd4GO"
   },
   "source": [
    "## 5. **Illustrative Example (Code + Explanation)**\n",
    "\n",
    "Imagine a simpler scenario. We'll create a toy neural network that tries to choose the correct label for small text inputs. Instead of 57 subjects, we'll just do a mini classification example to illustrate how we might evaluate correctness.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b3XTd2zd4GO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20600,
     "status": "ok",
     "timestamp": 1737808701116,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "7b3XTd2zd4GO",
    "outputId": "3ed177fd-0ad4-4d36-b80a-ab1e63392f55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Loss: 0.6232\n",
      "Epoch [10/20], Loss: 0.5231\n",
      "Epoch [15/20], Loss: 0.3873\n",
      "Epoch [20/20], Loss: 0.2444\n",
      "Training Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# We'll simulate a tiny dataset of text, each labeled as category 0 or 1.\n",
    "# Then we'll build a simple feed-forward network to classify them.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Suppose we have a small vocabulary and each word is a simple integer.\n",
    "# We'll create a synthetic dataset (text_samples, labels) just for illustration.\n",
    "\n",
    "text_samples = [\n",
    "    [1, 2, 3],   # e.g. \"I love math\"\n",
    "    [4, 5, 6],   # e.g. \"Dogs are cute\"\n",
    "    [1, 5, 7],   # e.g. \"I are cats\" (nonsense example)\n",
    "    [2, 3, 7],   # e.g. \"love math cats\" (nonsense)\n",
    "]\n",
    "\n",
    "# Labels: 0 or 1\n",
    "labels = [0, 1, 1, 0]\n",
    "\n",
    "# Convert to tensors\n",
    "text_tensor = torch.tensor(text_samples, dtype=torch.long)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=8, embed_dim=4, hidden_dim=8, num_classes=2):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # Simple feed-forward\n",
    "        self.fc1 = nn.Linear(embed_dim * 3, hidden_dim)  # 3 words each of embed_dim\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        embeds = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
    "        # Flatten\n",
    "        embeds = embeds.view(embeds.size(0), -1)  # (batch_size, seq_len*embed_dim)\n",
    "        out = self.fc1(embeds)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Initialize model\n",
    "model = SimpleClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# We'll do a simple training loop\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(text_tensor)\n",
    "    loss = criterion(outputs, labels_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate accuracy on the training set (since we have a tiny dataset)\n",
    "with torch.no_grad():\n",
    "    outputs = model(text_tensor)\n",
    "    _, predicted = torch.max(outputs, dim=1)\n",
    "    correct = (predicted == labels_tensor).sum().item()\n",
    "    accuracy = correct / len(labels_tensor) * 100\n",
    "    print(f\"Training Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G5911Emfd4GP",
   "metadata": {
    "id": "G5911Emfd4GP"
   },
   "source": [
    "**Interpretation:**\n",
    "- We used a dummy dataset and a small embedding + feed-forward network.\n",
    "- The model tries to classify text samples into 0 or 1.\n",
    "- We measure the accuracy. This is analogous (in a very tiny way) to how we would see if our model picks the correct answer among choices.\n",
    "\n",
    "For MMLU, we’d do something similar but with a much larger dataset and more sophisticated prompts or fine-tuning strategies.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NplZ50_qd4GP",
   "metadata": {
    "id": "NplZ50_qd4GP"
   },
   "source": [
    "## 6. **Example Calculations**\n",
    "\n",
    "### **Weights ($W$)**\n",
    "- These are the parameters the network learns. For example, in our code, `self.fc1.weight` and `self.fc2.weight` are weight matrices.\n",
    "- In a Transformer, we have weights in the embedding layer, attention matrices $Q, K, V$, and feed-forward layers.\n",
    "\n",
    "### **Bias ($b$)**\n",
    "- Added to the linear transformation. For instance, `self.fc1.bias`.\n",
    "- Helps the model shift predictions up or down.\n",
    "\n",
    "### **Matrix Multiplication**\n",
    "- If we have $x$ as an input vector (e.g., embeddings), a linear layer does $W x + b$. Suppose $W$ is 2x3 and $x$ is 3x1, the result is a 2x1 vector.\n",
    "\n",
    "### **Softmax**\n",
    "- Used to convert raw scores into probabilities. For classification, we check which probability is highest.\n",
    "\n",
    "### **Loss Calculation**\n",
    "- If the correct label is 1, but the model's probability for label 1 is 0.2, it gets penalized. The network adjusts weights via backpropagation to reduce this penalty.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iOUBGk58d4GP",
   "metadata": {
    "id": "iOUBGk58d4GP"
   },
   "source": [
    "## 7. **Step-by-Step Example: Creating the Technology from Scratch**\n",
    "\n",
    "**Step 1: Collect or define a dataset**\n",
    "- For MMLU, it’s a big set of multiple-choice questions across 57 subjects.\n",
    "- For our mini example, we just made a few text samples.\n",
    "\n",
    "**Step 2: Build or define the model architecture**\n",
    "- This could be a Transformer-based model for large-scale tasks.\n",
    "- In our mini example, it’s an embedding + feed-forward.\n",
    "\n",
    "**Step 3: Choose a training objective**\n",
    "- For classification, it’s usually cross-entropy.\n",
    "- For MMLU or multiple-choice QA, we compare the logits for each choice.\n",
    "\n",
    "**Step 4: Train the model**\n",
    "- Provide data in batches, do forward pass, compute loss, backpropagate, update weights.\n",
    "\n",
    "**Step 5: Evaluate**\n",
    "- Compare model predictions vs. correct answers.\n",
    "- Calculate accuracy or other metrics.\n",
    "\n",
    "**Step 6: Iterate**\n",
    "- Tweak hyperparameters, architecture, or training steps to improve performance.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dLx-_Qd4GP",
   "metadata": {
    "id": "b1dLx-_Qd4GP"
   },
   "source": [
    "## 8. **Illustrative Problem It Solves**\n",
    "\n",
    "**In Academics**:\n",
    "- MMLU can test if a model \"knows\" or can replicate knowledge from, say, chemistry or math. If it can pass those tests, it implies broad coverage.\n",
    "\n",
    "**In Our Mini Example**:\n",
    "- Our tiny classifier can differentiate text into categories (0 or 1). This is akin to seeing if it can pick the right label from a set of choices.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PoET4U_td4GP",
   "metadata": {
    "id": "PoET4U_td4GP"
   },
   "source": [
    "## 9. **Practical, Real-World Problem It Solves**\n",
    "\n",
    "**Chatbots and Virtual Assistants**:\n",
    "- Tools like ChatGPT are evaluated on tasks akin to MMLU to see if they can answer professional-level and academic queries reliably.\n",
    "\n",
    "**Diagnostics**:\n",
    "- If a model is deployed in a tutoring app, we want to ensure it can handle different subject questions well. MMLU-like evaluations help gauge that.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dC3rnZbd4GP",
   "metadata": {
    "id": "9dC3rnZbd4GP"
   },
   "source": [
    "## 10. **How to Solve a Real-World Problem Using This Tech**\n",
    "\n",
    "1. **Identify the Task**: e.g., build a Q&A system for a medical domain.\n",
    "2. **Collect Data**: gather medical questions and correct answers.\n",
    "3. **Train or Fine-tune**: pick a large pre-trained LLM, fine-tune it or prompt it to handle medical Q&A.\n",
    "4. **Evaluate**: use an MMLU-like benchmark (or specialized medical exam questions) to see how well the system does.\n",
    "5. **Deploy & Monitor**: put the system in a real setting, monitor performance, gather feedback for improvements.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2KqOW2zjd4GP",
   "metadata": {
    "id": "2KqOW2zjd4GP"
   },
   "source": [
    "## 11. **Questions to Ask & Their Answers (with Code Examples)**\n",
    "\n",
    "### Q1. *How do we add new subjects or tasks to an MMLU-like benchmark?*\n",
    "**Answer**: You’d create or source multiple-choice questions from the new subject, standardize them (similar to the existing format), and then incorporate them into the evaluation pipeline.\n",
    "\n",
    "### Q2. *What if my model is performing poorly on certain subjects?*\n",
    "**Answer**: You can gather more training data relevant to that subject, do targeted fine-tuning, or employ prompt engineering to help the model reason more effectively.\n",
    "\n",
    "### Q3. *How do we measure success on MMLU?*\n",
    "**Answer**: Typically, we measure the percentage of questions answered correctly across all subjects. We may also look at performance by subject.\n",
    "\n",
    "### Q4. *Can we do a quick code snippet that checks multiple-choice answers?*\n",
    "**Answer**: Yes, see below for a minimal illustration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "OmpGh1Vjd4GP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1737808701116,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "OmpGh1Vjd4GP",
    "outputId": "5cdb1fa0-de0d-41dc-b34a-8603e3cb574d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: tensor([ 2.5861, -0.3854, -1.7413])\n",
      "Best choice index: 0\n"
     ]
    }
   ],
   "source": [
    "# Minimal multiple-choice style check\n",
    "import torch\n",
    "\n",
    "# Suppose we have 1 question with 3 possible answers.\n",
    "question_embedding = torch.randn(1, 4)  # random question representation\n",
    "choice_embeddings = torch.randn(3, 4)  # 3 choices, each a 4-d vector\n",
    "\n",
    "# Let's do a simple dot product to see which answer is most similar to the question.\n",
    "scores = torch.matmul(choice_embeddings, question_embedding.transpose(0, 1))\n",
    "scores = scores.view(-1)  # flatten\n",
    "best_choice = torch.argmax(scores).item()\n",
    "print(\"Scores:\", scores)\n",
    "print(\"Best choice index:\", best_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vVDGuVhEd4GQ",
   "metadata": {
    "id": "vVDGuVhEd4GQ"
   },
   "source": [
    "**Interpretation:** In a real scenario, the model outputs logits for each choice, and the highest logit is the predicted answer.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y-JBM_WXd4GQ",
   "metadata": {
    "id": "y-JBM_WXd4GQ"
   },
   "source": [
    "## 12. **A Sample Exercise**\n",
    "\n",
    "Below is a more elaborate example. The idea is to have students:\n",
    "1. **Complete the TODOs**.\n",
    "2. **Run the cells**.\n",
    "3. **Observe the results**.\n",
    "\n",
    "We'll build a very small text classification model (similar to before but with some missing pieces for you to fill). We’ll imagine each piece of text is an exam question, and the label is the correct subject category (just for demonstration).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "NywCW7ENd4GQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1737808701116,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "NywCW7ENd4GQ",
    "outputId": "51b47491-a0d2-4aca-b4d2-b3edb8bc92f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Loss: 0.5779\n",
      "Epoch [10/20], Loss: 0.4137\n",
      "Epoch [15/20], Loss: 0.2359\n",
      "Epoch [20/20], Loss: 0.0914\n",
      "Final Training Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# TODO CODE EXAMPLE\n",
    "# Follow the hints and fill in the missing pieces marked as TODO.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "################################\n",
    "# 1. Prepare some synthetic data\n",
    "################################\n",
    "\n",
    "# Suppose we have 6 text samples with 2 different \"subject categories\" (label 0 or 1)\n",
    "text_samples = [\n",
    "    [1, 2, 2, 3],   # e.g. question tokens\n",
    "    [2, 2, 3, 1],\n",
    "    [4, 1, 5, 5],\n",
    "    [4, 1, 1, 5],\n",
    "    [7, 2, 2, 3],\n",
    "    [7, 1, 1, 2]\n",
    "]\n",
    "labels = [0, 0, 1, 1, 0, 1]\n",
    "\n",
    "# Convert to tensors\n",
    "text_tensor = torch.tensor(text_samples, dtype=torch.long)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "################################\n",
    "# 2. Define a simple network\n",
    "################################\n",
    "\n",
    "class MyTinyModel(nn.Module):\n",
    "    def __init__(self, vocab_size=10, embed_dim=4, hidden_dim=8, num_classes=2):\n",
    "        super(MyTinyModel, self).__init__()\n",
    "        # TODO: Create an embedding layer called self.embedding\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "\n",
    "        # We assume each sample has length 4, so final embedding size is embed_dim * 4\n",
    "        # TODO: Create a linear layer self.fc1 from embed_dim*4 -> hidden_dim\n",
    "        self.fc1 = nn.Linear(embed_dim*4, hidden_dim)\n",
    "\n",
    "        # TODO: Create a second linear layer self.fc2 from hidden_dim -> num_classes\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        embeds = self.embedding(x)\n",
    "        # flatten\n",
    "        embeds = embeds.view(embeds.size(0), -1)\n",
    "        out = self.fc1(embeds)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "################################\n",
    "# 3. Training loop\n",
    "################################\n",
    "\n",
    "# TODO: Instantiate the model, define criterion (CrossEntropyLoss), and optimizer (Adam or SGD)\n",
    "model = MyTinyModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# TODO: define num_epochs\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(text_tensor)\n",
    "    loss = criterion(outputs, labels_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "with torch.no_grad():\n",
    "    outputs = model(text_tensor)\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    correct = (preds == labels_tensor).sum().item()\n",
    "    accuracy = correct / len(labels_tensor) * 100\n",
    "print(f\"Final Training Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s8Ri22LFd4GQ",
   "metadata": {
    "id": "s8Ri22LFd4GQ"
   },
   "source": [
    "### Hints:\n",
    "- The embedding layer is `nn.Embedding(num_embeddings, embedding_dim)`.\n",
    "- The linear layers are `nn.Linear(in_features, out_features)`.\n",
    "- The CrossEntropyLoss expects raw logits, so don’t apply softmax inside the model (PyTorch automatically does it in `nn.CrossEntropyLoss`).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dsEBHaSd4GQ",
   "metadata": {
    "id": "0dsEBHaSd4GQ"
   },
   "source": [
    "## 13. **Glossary**\n",
    "\n",
    "- **LLM (Large Language Model)**: A neural network model trained on massive text data, capable of generating or understanding human-like text.\n",
    "- **MMLU (Massive Multitask Language Understanding)**: A comprehensive benchmark testing LLMs across 57 subjects.\n",
    "- **Benchmark**: A standard test set or suite of tasks for comparing model performance.\n",
    "- **Transformer**: A neural network architecture using attention mechanisms to handle sequential data.\n",
    "- **Attention**: Method to focus on different parts of input text selectively.\n",
    "- **Weights and Biases**: Parameters the network learns. Weights are multiplicative factors, biases are additive offsets.\n",
    "- **Loss Function**: A measure of how far off the model’s predictions are from the correct labels.\n",
    "- **Accuracy**: Ratio of correct predictions to total predictions.\n",
    "- **Logits**: The raw, unnormalized scores output by a model before a softmax.\n",
    "- **Embeddings**: Vector representations of words or tokens.\n",
    "- **Fine-tuning**: Adapting a pre-trained model to a specific task.\n",
    "- **Chain-of-thought**: A method of prompting that shows the model detailed reasoning steps.\n",
    "\n",
    "---\n",
    "### End of Notebook\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ly8cRWVDd4GQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 176,
     "status": "ok",
     "timestamp": 1737808701291,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "ly8cRWVDd4GQ",
    "outputId": "b2b9768f-267f-4675-a4f0-82771d78073d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Acknowledgement +++\n",
      "Executed on: 2025-01-29 01:47:11.438630\n",
      "In Google Colab: No\n",
      "System info: Linux 6.8.0-51-generic\n",
      "Node name: alimuhammad-Inspiron-7559\n",
      "MAC address: 20:47:47:74:94:05\n",
      "IP address: 127.0.1.1\n",
      "Signing off, Ali Muhammad Asad\n"
     ]
    }
   ],
   "source": [
    "import os, sys, platform, datetime, uuid, socket\n",
    "\n",
    "def signoff(name=\"Anonymous\"):\n",
    "    colab_check = \"Yes\" if 'google.colab' in sys.modules else \"No\"\n",
    "    mac_addr = ':'.join(format((uuid.getnode() >> i) & 0xff, '02x') for i in reversed(range(0, 48, 8)))\n",
    "    print(\"+++ Acknowledgement +++\")\n",
    "    print(f\"Executed on: {datetime.datetime.now()}\")\n",
    "    print(f\"In Google Colab: {colab_check}\")\n",
    "    print(f\"System info: {platform.system()} {platform.release()}\")\n",
    "    print(f\"Node name: {platform.node()}\")\n",
    "    print(f\"MAC address: {mac_addr}\")\n",
    "    try:\n",
    "        print(f\"IP address: {socket.gethostbyname(socket.gethostname())}\")\n",
    "    except:\n",
    "        print(\"IP address: Unknown\")\n",
    "    print(f\"Signing off, {name}\")\n",
    "\n",
    "signoff(\"Ali Muhammad Asad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4de02c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
