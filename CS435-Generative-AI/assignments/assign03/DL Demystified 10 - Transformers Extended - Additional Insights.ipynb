{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 623,
     "status": "ok",
     "timestamp": 1737812439105,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "o1R1YQOdsSK6"
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "#                                                               #\n",
    "#  CS435 Generative AI: Security, Ethics and Governance         #\n",
    "#                                                               #\n",
    "#  Instructor: Dr. Adnan Masood                                 #\n",
    "#  Contact:    adnanmasood@gmail.com                            #\n",
    "#                                                               #\n",
    "#  Notebook is MIT Licensed                                     #\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5sqVd2psSK7"
   },
   "source": [
    "# Transformers and Attention\n",
    "\n",
    "Welcome to today's lesson! In this notebook, we will explore **Transformers and Attention Mechanisms** in detail. This will include everything from basic intuition to building a simple transformer from scratch in PyTorch. We'll go step by step to understand the concepts, math, and code behind these technologies.\n",
    "\n",
    "---\n",
    "\n",
    "## Building an Intuitive Understanding\n",
    "To make it easier to grasp the material, we'll explain it at five levels:\n",
    "1. What is it and why does it matter?\n",
    "2. Where it came from and how it works at a high level.\n",
    "3. Understanding the math behind attention and transformers.\n",
    "4. Writing PyTorch code to implement and experiment.\n",
    "5. Building a transformer from scratch.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2or-8XCAsSK8"
   },
   "source": [
    "## 1. Attention in Transformers?\n",
    "Imagine reading a book and trying to summarize it. A transformer is like a super-smart assistant that not only reads the book but also pays attention to the important parts, understands their relationships, and creates a summary that's easy to understand.\n",
    "\n",
    "Why is this important? Because machines need to understand relationships between words to translate languages, generate text, or even answer questions like humans!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIyoGAVcsSK8"
   },
   "source": [
    "Transformers were introduced in 2017 in a research paper titled \"Attention is All You Need.\" Before transformers, models like RNNs and LSTMs struggled to handle long-term dependencies in text. Transformers solved this by using \"self-attention\" to focus on the most relevant parts of the input.\n",
    "\n",
    "Key ideas of transformers:\n",
    "1. **Self-Attention**: Helps the model focus on important words in a sentence.\n",
    "2. **Positional Encoding**: Adds information about the order of words.\n",
    "3. **Multi-Head Attention**: Looks at the input from multiple perspectives.\n",
    "4. **Feedforward Networks**: Processes attention outputs for prediction.\n",
    "5. **Layer Normalization**: Stabilizes training.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gM67hBMfsSK8"
   },
   "source": [
    "## Understanding Attention\n",
    "Let's break down the self-attention mechanism step by step.\n",
    "\n",
    "### Self-Attention Formula\n",
    "The main idea is to compute a weighted sum of values where the weights are based on pairwise word similarity.\n",
    "For each word in a sentence, we compute:\n",
    "\n",
    "$$ Attention(Q, K, V) = Softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V $$\n",
    "\n",
    "Where:\n",
    "- $Q$: Query\n",
    "- $K$: Key\n",
    "- $V$: Value\n",
    "- $d_k$: Dimension of the key vectors\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zsHvS9msSK8"
   },
   "source": [
    "### Breaking it Down\n",
    "1. $Q$ and $K$ are compared to measure similarity.\n",
    "2. The result is scaled by $\\sqrt{d_k}$ to stabilize gradients.\n",
    "3. We apply $\\text{Softmax}$ to ensure weights sum to 1.\n",
    "4. Multiply weights with $V$ to get the output.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5790,
     "status": "ok",
     "timestamp": 1737812445204,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "xwYptTKOsSK8",
    "outputId": "b31f4e1b-e1e6-4d2d-ff9b-81a8c7ac7f90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights: tensor([[0.6698, 0.3302],\n",
      "        [0.3302, 0.6698]])\n",
      "Output: tensor([[6.6976, 3.3024],\n",
      "        [3.3024, 6.6976]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example Inputs\n",
    "queries = torch.tensor([[1.0, 0.0], [0.0, 1.0]])  # Q\n",
    "keys = torch.tensor([[1.0, 0.0], [0.0, 1.0]])     # K\n",
    "values = torch.tensor([[10.0, 0.0], [0.0, 10.0]]) # V\n",
    "\n",
    "# Compute Attention Scores\n",
    "scores = torch.mm(queries, keys.T) / torch.sqrt(torch.tensor(keys.shape[-1], dtype=torch.float32))\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "output = torch.mm(weights, values)\n",
    "\n",
    "print(\"Attention Weights:\", weights)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZ6Lls8bsSK9"
   },
   "source": [
    "---\n",
    "\n",
    "## Building Multi-Head Attention\n",
    "Below, we create a simple multi-head attention layer in PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1737812445204,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "lcK1nbewsSK9"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert embed_size % num_heads == 0, \"Embedding size must be divisible by number of heads.\"\n",
    "\n",
    "        self.head_dim = embed_size // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.query = torch.nn.Linear(embed_size, embed_size)\n",
    "        self.key = torch.nn.Linear(embed_size, embed_size)\n",
    "        self.value = torch.nn.Linear(embed_size, embed_size)\n",
    "        self.fc_out = torch.nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, seq_length, embed_size = x.shape\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "\n",
    "        # Reshape for multi-heads\n",
    "        Q = Q.view(N, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(N, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(N, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        # Concatenate heads\n",
    "        output = output.transpose(1, 2).contiguous().view(N, seq_length, embed_size)\n",
    "\n",
    "        return self.fc_out(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1737812445205,
     "user": {
      "displayName": "Adnan Masood",
      "userId": "04515194584513400333"
     },
     "user_tz": 300
    },
    "id": "10Qs7SnFsSK9",
    "outputId": "c5488460-8bdf-4f75-8d04-f4bac53cc335"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Acknowledgement +++\n",
      "Executed on: 2025-01-29 01:29:33.567065\n",
      "In Google Colab: No\n",
      "System info: Linux 6.8.0-51-generic\n",
      "Node name: alimuhammad-Inspiron-7559\n",
      "MAC address: 20:47:47:74:94:05\n",
      "IP address: 127.0.1.1\n",
      "Signing off, Ali Muhammad Asad\n"
     ]
    }
   ],
   "source": [
    "import os, sys, platform, datetime, uuid, socket\n",
    "\n",
    "def signoff(name=\"Anonymous\"):\n",
    "    colab_check = \"Yes\" if 'google.colab' in sys.modules else \"No\"\n",
    "    mac_addr = ':'.join(format((uuid.getnode() >> i) & 0xff, '02x') for i in reversed(range(0, 48, 8)))\n",
    "    print(\"+++ Acknowledgement +++\")\n",
    "    print(f\"Executed on: {datetime.datetime.now()}\")\n",
    "    print(f\"In Google Colab: {colab_check}\")\n",
    "    print(f\"System info: {platform.system()} {platform.release()}\")\n",
    "    print(f\"Node name: {platform.node()}\")\n",
    "    print(f\"MAC address: {mac_addr}\")\n",
    "    try:\n",
    "        print(f\"IP address: {socket.gethostbyname(socket.gethostname())}\")\n",
    "    except:\n",
    "        print(\"IP address: Unknown\")\n",
    "    print(f\"Signing off, {name}\")\n",
    "\n",
    "signoff(\"Ali Muhammad Asad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
