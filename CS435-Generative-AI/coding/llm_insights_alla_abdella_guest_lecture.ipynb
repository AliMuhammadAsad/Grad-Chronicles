{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eLabQ16w2i3"
      },
      "source": [
        "## LLM Insights - Guest Lecture by Dr. Alla Abdella\n",
        "Welcome to this Jupyter Notebook demonstrating embeddings, retrieval with RAG, and usage of LLM-based pipelines with LangChain. We'll explore how to set up local LLMs, build a vector store, and create multi-step pipelines with memory and prompt templates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_1"
      },
      "source": [
        "# Embeddings & Retrieval: A Hands-On LangChain Demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c63ys5C5Jtjg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5c0a6025-457f-4567-d740-a3d954c75af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "source": [
        "# Install Core LangChain and Related Libraries\n",
        "!pip install -U langchain langchainhub langchain-nomic langchain_community langchain-groq tiktoken chromadb langgraph\n",
        "\n",
        "# Install Sentence Embedding Models\n",
        "!pip install sentence-transformers\n",
        "\n",
        "# Install LLM Libraries\n",
        "!pip install transformers gpt4all anthropic\n",
        "\n",
        "# Install Additional Data Processing and Visualization Libraries\n",
        "!pip install pandas scikit-learn matplotlib plotly\n",
        "\n",
        "# Install Streamlit for Web Apps\n",
        "!pip install streamlit\n",
        "\n",
        "# Install Tavily for API Integrations\n",
        "!pip install tavily-python\n",
        "\n",
        "# Upgrade Specific Tools and Libraries\n",
        "!pip install --upgrade langchain\n",
        "!pip install --upgrade --quiet langchain-text-splitters\n",
        "!pip install llama-index\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_2"
      },
      "source": [
        "**Explanation of Installed Libraries:**  \n",
        "- **langchain, langchainhub, etc.**: Libraries that help in chaining different steps of an LLM pipeline—like retrieving documents, running prompts, storing conversation memory, etc.\n",
        "- **sentence-transformers**: Convert sentences into vectors so we can compare how similar they are.\n",
        "- **transformers, gpt4all, anthropic**: Different libraries for advanced AI language models.\n",
        "- **pandas, scikit-learn, matplotlib, plotly**: Data analysis and visualization tools.\n",
        "- **streamlit**: Build web apps easily.\n",
        "- **tavily-python**: Helps with certain API integrations.\n",
        "- **tiktoken**: A library from OpenAI to manage text tokens.\n",
        "- **llama-index**: Assists in building large language model indexes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M7YZPrZLN0m"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import SequentialChain\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "from langchain_groq import ChatGroq\n",
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import GPT4AllEmbeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = \"\"\n",
        "os.environ[\"GROQ_API_KEY\"] = \"\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = \"\"\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"HF_TOKEN\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_3"
      },
      "source": [
        "**What this code does:**\n",
        "- **Imports** a variety of components from the LangChain ecosystem.\n",
        "- **ChatGroq** integrates with a particular LLM service.\n",
        "- **Sets environment variables** so we don’t have to manually input keys or tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "I16ErBrBKZsT"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "# Initialize the SentenceTransformer model.\n",
        "model = SentenceTransformer(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_4"
      },
      "source": [
        "**Explanation:**  \n",
        "We set up `all-MiniLM-L6-v2`, which is a well-known model for generating sentence embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "0DQioC4cKDiR"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Use the model to encode a simple string.\n",
        "model.encode(\"Hello Students\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6jmirRbKDoq"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Let's see the length of the vector.\n",
        "len(model.encode(\"Hello Students\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_5"
      },
      "source": [
        "**What’s happening here?**\n",
        "- We encode the phrase \"Hello Students\" into a numerical vector.\n",
        "- We check the **length** of that vector.\n",
        "- It’s usually 384 for `all-MiniLM-L6-v2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r6B_txXOXOh"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 1. Initialize a pre-trained model\n",
        "# model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "# model = SentenceTransformer(model_name)\n",
        "\n",
        "# 2. Define sentences related to Urdu students and Pakistani culture\n",
        "sentences = [\n",
        "    # Greetings\n",
        "    \"Assalam-o-Alaikum! How are you today?\",\n",
        "    \"Good morning! I hope you are doing well.\",\n",
        "    \"Hi there! How has your day been?\",\n",
        "\n",
        "    # Technology\n",
        "    \"Learning Python is essential for students.\",\n",
        "    \"Programming in Python is very interesting.\",\n",
        "    \"Python is a useful language for students.\",\n",
        "\n",
        "    # Food\n",
        "    \"Biryani is a delicious dish from Pakistan.\",\n",
        "    \"I love eating spicy biryani with friends.\",\n",
        "    \"Pakistani biryani is the best comfort food.\",\n",
        "]\n",
        "\n",
        "# 3. Compute embeddings\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "# 4. Apply PCA to reduce to 2D for visualization\n",
        "pca = PCA(n_components=2)\n",
        "reduced_embeddings = pca.fit_transform(embeddings)\n",
        "\n",
        "# 5. Plot the PCA results\n",
        "plt.figure(figsize=(12, 8))\n",
        "colors = ['red', 'green', 'blue']  # Unique color for each category\n",
        "categories = ['Greetings', 'Technology', 'Food']  # Labels for the legend\n",
        "\n",
        "category_size = 3  # Number of sentences per category\n",
        "num_categories = len(sentences) // category_size\n",
        "\n",
        "for i in range(num_categories):\n",
        "    start_idx = i * category_size\n",
        "    end_idx = start_idx + category_size\n",
        "    cluster = reduced_embeddings[start_idx:end_idx]\n",
        "    label_group = sentences[start_idx:end_idx]\n",
        "    color = colors[i % len(colors)]\n",
        "    plt.scatter(cluster[:, 0], cluster[:, 1], color=color, label=categories[i])\n",
        "\n",
        "    # Annotate each point\n",
        "    for j, txt in enumerate(label_group):\n",
        "        plt.annotate(txt, (cluster[j, 0], cluster[j, 1]), fontsize=9, alpha=0.7)\n",
        "\n",
        "plt.title(\"PCA of Sentence Embeddings\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.legend(title=\"Categories\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_6"
      },
      "source": [
        "**Step-by-step**:\n",
        "1. We have a list of sentences in different categories.\n",
        "2. We turn them into embeddings using our model.\n",
        "3. We apply **PCA** to reduce to 2D.\n",
        "4. We color them by category to see if similar topics cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZEOh5d_P_gq"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "sentence = \"I love math\"\n",
        "# Simple example of tokenization\n",
        "tokens =  [\"I\", \"love\", \"math\"]\n",
        "tokens2id = {token: i for i, token in enumerate(tokens)}\n",
        "tokens2id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_7"
      },
      "source": [
        "**Explanation:**\n",
        "A quick illustration of how tokenization might assign IDs to tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZSI21qFQjpS"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# #!pip install llama_index==0.10.18\n",
        "# \"\"\"\n",
        "# This is a simple application for sentence embeddings: semantic search\n",
        "#\n",
        "# We have a corpus with various sentences. Then, for a given query sentence,\n",
        "# we want to find the most similar sentence in this corpus.\n",
        "#\n",
        "# This script outputs for various queries the top 5 most similar sentences in the corpus.\n",
        "# \"\"\"\n",
        "\n",
        "# import torch\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from mpl_toolkits.mplot3d import Axes3D  # Required for 3D plotting\n",
        "# from sklearn.decomposition import PCA\n",
        "# import plotly.graph_objects as go\n",
        "# # from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
        "\n",
        "# embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "# #embedder = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "# corpus = [\n",
        "#     \"A man is eating food.\",\n",
        "#     \"A man is eating a piece of bread.\",\n",
        "#     \"The girl is carrying a baby.\",\n",
        "#     \"A man is riding a horse.\",\n",
        "#     \"A woman is playing violin.\",\n",
        "#     \"Two men pushed carts through the woods.\",\n",
        "#     \"A man is riding a white horse on an enclosed ground.\",\n",
        "#     \"A monkey is playing drums.\",\n",
        "#     \"A cheetah is running behind its prey.\",\n",
        "# ]\n",
        "# # corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
        "#\n",
        "# # queries = [\n",
        "# #     \"A man is eating pasta.\",\n",
        "# # ]\n",
        "#\n",
        "# # top_k = min(2, len(corpus))\n",
        "# query_embeddings_list = []\n",
        "# results = {}  # To store top-2 indices for each query\n",
        "#\n",
        "# for query in queries:\n",
        "#     query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
        "#     # similarity_scores = torch.matmul(query_embedding, corpus_embeddings.T)[0]\n",
        "#     # scores, indices = torch.topk(similarity_scores, k=top_k)\n",
        "#\n",
        "#     print(\"\\nQuery:\", query)\n",
        "#     print(\"Top 5 most similar sentences in corpus:\")\n",
        "#\n",
        "#     for score, idx in zip(scores, indices):\n",
        "#         print(corpus[idx], f\"(Score: {score:.4f})\")\n",
        "#\n",
        "#     query_embeddings_list.append(query_embedding[0].numpy())\n",
        "#     results[query] = {\n",
        "#         'top2_indices': indices[:2].numpy(),\n",
        "#         'similarity_scores': scores[:2].numpy()\n",
        "#     }\n",
        "#\n",
        "# # 3D PCA plotting omitted here.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_8"
      },
      "source": [
        "**Explanation:**\n",
        "Commented-out example showing how to do semantic search with embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr54wLLnKDsM"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "!pip install colab-xterm\n",
        "%load_ext colabxterm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_9"
      },
      "source": [
        "**What is `colab-xterm`?**\n",
        "It allows a terminal-like interface in Google Colab for commands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xswRAXRKDvC"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "%xterm\n",
        "# curl -fsSL https://ollama.com/install.sh | sh\n",
        "# ollama serve &\n",
        "# ollama run llama3.2:3b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_10"
      },
      "source": [
        "**Explanation**:\n",
        "- `%xterm` opens xterm in Colab.\n",
        "- Commented lines show how to install/run `ollama`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHNEaRxTZuYv"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "import json\n",
        "\n",
        "### Index\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import GPT4AllEmbeddings\n",
        "\n",
        "urls = [\n",
        "     \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "     \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "]\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=250,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"rag-chroma1\",\n",
        "    embedding=GPT4AllEmbeddings(),\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever(k=5)\n",
        "query = \"What are the critical features of a gun\"\n",
        "documents = retriever.invoke(query)\n",
        "\n",
        "context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(documents)])\n",
        "\n",
        "prompt = f\"\"\"\n",
        "You are an expert in Generative AI and autonomous agent systems. Below is the context retrieved from relevant documents. Use this context to provide a detailed and accurate answer to the user's query.\n",
        "If you don't know the answer, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Query:\n",
        "{query}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "url = 'http://localhost:11434/api/generate'\n",
        "payload = {\n",
        "    \"model\": \"llama3.2:3b\",\n",
        "    \"prompt\": prompt,\n",
        "    \"num_predict\": 2000,\n",
        "    \"temperature\": 0.0,\n",
        "    \"stream\": True\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=payload, stream=True)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    print(\"Model Response:\")\n",
        "    assembled_response = \"\"\n",
        "    for line in response.iter_lines(decode_unicode=True):\n",
        "        if line.strip():\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "                if \"response\" in data:\n",
        "                    assembled_response += data[\"response\"]\n",
        "                    print(data[\"response\"], end='', flush=True)\n",
        "                if data.get(\"done\", False):\n",
        "                    break\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error decoding JSON: {e}\")\n",
        "else:\n",
        "    print(f\"Error: {response.status_code} - {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_11"
      },
      "source": [
        "**Explanation**:\n",
        "We load web documents, split them, embed them, store in Chroma, retrieve top results, then prompt a local LLM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LChxSmEZzCE"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "retriever.invoke(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_12"
      },
      "source": [
        "**Explanation**:\n",
        "Just shows the raw retrieved doc chunks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4wlf3aqNoRn"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#export LANGCHAIN_TRACING_V2=true\n",
        "from langsmith import traceable\n",
        "from langchain_groq import ChatGroq\n",
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import GPT4AllEmbeddings\n",
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = \"\"\n",
        "os.environ[\"GROQ_API_KEY\"] = \"\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = \"\"\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"HF_TOKEN\"] = \"\"\n",
        "\n",
        "### LLM\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Here we configure a ChatGroq instance.\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.3-70b-versatile\",#\"mixtral-8x7b-32768\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "chat = llm\n",
        "\n",
        "# Re-initialize the ChatGroq LLM.\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "@traceable\n",
        "def get_messages():\n",
        "    messages = [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful AI assistant with deep expertise in AI, data engineering, and healthcare.\"\n",
        "        ),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"\"\" Please give me advices on how i start learing about the data science filed\\n\\nPlease provide markdown that demonstrates:\\n  1. Use of syntax highlighting for different programming languages\\n  2. Colored code blocks\\n  3. Varied text styling (bold, italic, headers)\\n  4. Include at least one example of colored terminal/output text\\n\"\"\"\n",
        "        ),\n",
        "    ]\n",
        "    return messages\n",
        "\n",
        "@traceable(run_type=\"llm\")\n",
        "def invoke_llm(messages):\n",
        "    return llm.invoke(messages)\n",
        "\n",
        "@traceable\n",
        "def parse_output(response):\n",
        "    return response.content\n",
        "\n",
        "@traceable\n",
        "def run_pipeline():\n",
        "    messages = get_messages()\n",
        "    response = invoke_llm(messages)\n",
        "    result = parse_output(response)\n",
        "    return result\n",
        "\n",
        "result = run_pipeline()\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "display(Markdown(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_17"
      },
      "source": [
        "**Explanation**:\n",
        "- We decorate functions with `@traceable` to track them.\n",
        "- Build messages, call the LLM, parse output, display the result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtCdKlj7PDW9"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcMd672WFqY_"
      },
      "source": [
        "# Today's Agenda\n",
        "\n",
        "1. **Concepts of embeddings and embedding layers**  \n",
        "2. **Cost function(s) for training an embedding model**  \n",
        "3. **Practical demonstration in Python using Sentence Transformers**  \n",
        "4. **Sample training data in multiple formats**  \n",
        "5. **Location of the embedding layer in BERT-based models**  \n",
        "6. **Example code with multilingual sentences (English, Urdu, Arabic)**  \n",
        "7. **PCA visualization to show similar sentences clustering**  \n",
        "8. **Build a simple RAG (Retrieval-Augmented Generation)**  \n",
        "9. **Build Habib Conversational Agent**  \n",
        "10. **LangChain Chains**  \n",
        "11. **LangChain Memory**\n",
        "\n",
        "---\n",
        "\n",
        "## Slide 1: Introduction to Embeddings\n",
        "\n",
        "**Slide Content:**\n",
        "- **Definition**: An embedding is a dense vector representation of text in a continuous vector space.\n",
        "- **Key Idea**:\n",
        "  - Each dimension captures latent semantic or syntactic info.\n",
        "- **Why embeddings?**\n",
        "  - They enable semantic similarity, context, relationships.\n",
        "\n",
        "---\n",
        "\n",
        "## Slide 2: What is an Embedding Layer?\n",
        "\n",
        "**Slide Content:**\n",
        "- **Embedding Layer**: A trainable layer that maps tokens to vectors.\n",
        "- **In BERT-like models**:\n",
        "  - The embedding layer is part of the initial component.\n",
        "  - Token embeddings + position embeddings.\n",
        "\n",
        "---\n",
        "\n",
        "## Slide 3: Cost Functions for Training Embeddings\n",
        "\n",
        "**Slide Content:**\n",
        "- Objective: close embeddings for similar texts, far for dissimilar.\n",
        "- Common Losses:\n",
        "  1. Triplet Loss\n",
        "  2. Contrastive Loss\n",
        "  3. Multiple Negative Ranking Loss\n",
        "\n",
        "---\n",
        "\n",
        "## Slide 4: Sample Training Data Formats\n",
        "\n",
        "**Slide Content:**\n",
        "1. Triplet (anchor, positive, negative)\n",
        "2. Pair with label\n",
        "3. Pair with similarity score\n",
        "\n",
        "---\n",
        "\n",
        "## Slide 5: Where is the Embedding Layer in BERT?\n",
        "\n",
        "**Slide Content:**\n",
        "- BERT Architecture (simplified): Input Embeddings -> Transformer Layers -> Pooler.\n",
        "- Embedding layer is at the start.\n",
        "- Sentence-Transformers adds a pooling layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLIDE_6_Markdown"
      },
      "source": [
        "## Slide 6:  Detailed Python Example 1\n",
        "\n",
        "**Code Explanation**  \n",
        "1. Demonstrates corpus, query, and top-k similarity search.\n",
        "2. Uses embeddings from a model.\n",
        "3. Visualizes results with PCA.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLIDE_6_Code"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "!pip install sentence-transformers scikit-learn matplotlib --quiet\n",
        "\n",
        "\"\"\"\n",
        "Simple application for sentence embeddings: semantic search\n",
        "\n",
        "We have a corpus with various sentences. Then, for a given query,\n",
        "we want to find the most similar ones.\n",
        "\"\"\"\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.decomposition import PCA\n",
        "import plotly.graph_objects as go\n",
        "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
        "\n",
        "# embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embedder = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "corpus = [\n",
        "    \"A man is eating food.\",\n",
        "    \"A man is eating a piece of bread.\",\n",
        "    \"The girl is carrying a baby.\",\n",
        "    \"A man is riding a horse.\",\n",
        "    \"A woman is playing violin.\",\n",
        "    \"Two men pushed carts through the woods.\",\n",
        "    \"A man is riding a white horse on an enclosed ground.\",\n",
        "    \"A monkey is playing drums.\",\n",
        "    \"A cheetah is running behind its prey.\",\n",
        "]\n",
        "corpus_embeddings = embedder.get_text_embedding_batch(corpus, show_progress=True)\n",
        "\n",
        "if isinstance(corpus_embeddings, list):\n",
        "    corpus_embeddings = torch.tensor(corpus_embeddings)\n",
        "\n",
        "queries = [\n",
        "    \"A man is eating pasta.\",\n",
        "]\n",
        "\n",
        "top_k = min(2, len(corpus))\n",
        "query_embeddings_list = []\n",
        "results = {}\n",
        "\n",
        "for query in queries:\n",
        "    query_embedding = embedder.get_text_embedding_batch([query], show_progress=True)\n",
        "    if isinstance(query_embedding, list):\n",
        "        query_embedding = torch.tensor(query_embedding)\n",
        "\n",
        "    similarity_scores = torch.matmul(query_embedding, corpus_embeddings.T)[0]\n",
        "    scores, indices = torch.topk(similarity_scores, k=top_k)\n",
        "\n",
        "    print(\"\\nQuery:\", query)\n",
        "    print(\"Top 5 most similar sentences in corpus:\")\n",
        "\n",
        "    for score, idx in zip(scores, indices):\n",
        "        print(corpus[idx], f\"(Score: {score:.4f})\")\n",
        "\n",
        "    query_embeddings_list.append(query_embedding[0].numpy())\n",
        "    results[query] = {\n",
        "        'top2_indices': indices[:2].numpy(),\n",
        "        'similarity_scores': scores[:2].numpy()\n",
        "    }\n",
        "\n",
        "corpus_np = corpus_embeddings.cpu().detach().numpy()\n",
        "all_embeddings = np.vstack([corpus_np, np.array(query_embeddings_list)])\n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "all_embeddings_3d = pca.fit_transform(all_embeddings)\n",
        "\n",
        "num_corpus = corpus_np.shape[0]\n",
        "corpus_3d = all_embeddings_3d[:num_corpus]\n",
        "queries_3d = all_embeddings_3d[num_corpus:]\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(corpus_3d[:, 0], corpus_3d[:, 1], corpus_3d[:, 2], c='blue', marker='o', s=80, label='Corpus')\n",
        "\n",
        "for i, txt in enumerate(corpus):\n",
        "    ax.text(corpus_3d[i, 0], corpus_3d[i, 1], corpus_3d[i, 2], txt, size=9, zorder=1, color='k')\n",
        "\n",
        "ax.scatter(queries_3d[:, 0], queries_3d[:, 1], queries_3d[:, 2], c='red', marker='^', s=120, label='Query')\n",
        "\n",
        "for i, query in enumerate(queries):\n",
        "    q_point = queries_3d[i]\n",
        "    top2_indices = results[query]['top2_indices']\n",
        "    for idx in top2_indices:\n",
        "        c_point = corpus_3d[idx]\n",
        "        ax.plot([q_point[0], c_point[0]], [q_point[1], c_point[1]], [q_point[2], c_point[2]], 'g--', linewidth=1.5)\n",
        "\n",
        "ax.set_xlabel(\"PCA Component 1\")\n",
        "ax.set_ylabel(\"PCA Component 2\")\n",
        "ax.set_zlabel(\"PCA Component 3\")\n",
        "ax.set_title(\"3D Visualization of Sentence Embeddings\\nLines connect each query to its 2 most similar corpus sentences\")\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLIDE_7_Markdown"
      },
      "source": [
        "## Slide 7:  Detailed Python Example 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLIDE_7_Code"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "# !pip install sentence-transformers scikit-learn matplotlib plotly --quiet\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 1. Initialize a pre-trained model\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "# 2. Define sentences related to Urdu students and Pakistani culture\n",
        "sentences = [\n",
        "    \"Assalam-o-Alaikum! How are you today?\",\n",
        "    \"Good morning! I hope you are doing well.\",\n",
        "    \"Hi there! How has your day been?\",\n",
        "    \"Learning Python is essential for students.\",\n",
        "    \"Programming in Python is very interesting.\",\n",
        "    \"Python is a useful language for students.\",\n",
        "    \"Biryani is a delicious dish from Pakistan.\",\n",
        "    \"I love eating spicy biryani with friends.\",\n",
        "    \"Pakistani biryani is the best comfort food.\",\n",
        "]\n",
        "\n",
        "# 3. Compute embeddings\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "# 4. Apply PCA to reduce to 2D\n",
        "pca = PCA(n_components=2)\n",
        "reduced_embeddings = pca.fit_transform(embeddings)\n",
        "\n",
        "# 5. Plot results\n",
        "plt.figure(figsize=(12, 8))\n",
        "colors = ['red', 'green', 'blue',  'orange', 'cyan']\n",
        "categories = ['Greetings', 'Sports', 'Questions', 'Technology', 'Food']\n",
        "\n",
        "category_size = 3\n",
        "num_categories = len(sentences) // category_size\n",
        "\n",
        "for i in range(num_categories):\n",
        "    start_idx = i * category_size\n",
        "    end_idx = start_idx + category_size\n",
        "    cluster = reduced_embeddings[start_idx:end_idx]\n",
        "    label_group = sentences[start_idx:end_idx]\n",
        "    color = colors[i % len(colors)]\n",
        "    plt.scatter(cluster[:, 0], cluster[:, 1], color=color, label=categories[i])\n",
        "    for j, txt in enumerate(label_group):\n",
        "        plt.annotate(txt, (cluster[j, 0], cluster[j, 1]), fontsize=9, alpha=0.7)\n",
        "\n",
        "plt.title(\"PCA of Sentence Embeddings: Urdu Students and Pakistani Culture\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.legend(title=\"Categories\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLIDE_8_Markdown"
      },
      "source": [
        "## Slide 8: What is a Vector Store & RAG?\n",
        "\n",
        "**Indexing & Vector Stores**\n",
        "- After creating embeddings, store them in a vector store for fast similarity search.\n",
        "\n",
        "**RAG (Retrieval-Augmented Generation)**\n",
        "- Combine an LLM with external knowledge.\n",
        "- Retrieve relevant docs, feed them to the LLM as context.\n",
        "- LLM generates the final answer with that context.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLIDE_9A_Markdown"
      },
      "source": [
        "## Slide 9: Build a Simple RAG\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLIDE_9A_Code"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import GPT4AllEmbeddings\n",
        "\n",
        "urls = [\n",
        "     \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "     \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "]\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=250,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"rag-chroma1\",\n",
        "    embedding=GPT4AllEmbeddings(),\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever(k=5)\n",
        "query = \"What are the critical features of a generative AI-powered autonomous agent system?\"\n",
        "documents = retriever.invoke(query)\n",
        "\n",
        "context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(documents)])\n",
        "prompt = f\"\"\"\n",
        "You are an expert in Generative AI and autonomous agent systems.\n",
        "Below is the context retrieved from relevant documents.\n",
        "Use this context to provide a detailed and accurate answer.\n",
        "If you don't know, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Query:\n",
        "{query}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "url = 'http://localhost:11434/api/generate'\n",
        "payload = {\n",
        "    \"model\": \"llama3.2:3b\",\n",
        "    \"prompt\": prompt,\n",
        "    \"num_predict\": 2000,\n",
        "    \"temperature\": 0.0,\n",
        "    \"stream\": True\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=payload, stream=True)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    print(\"Model Response:\")\n",
        "    assembled_response = \"\"\n",
        "    for line in response.iter_lines(decode_unicode=True):\n",
        "        if line.strip():\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "                if \"response\" in data:\n",
        "                    assembled_response += data[\"response\"]\n",
        "                    print(data[\"response\"], end='', flush=True)\n",
        "                if data.get(\"done\", False):\n",
        "                    break\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error decoding JSON: {e}\")\n",
        "else:\n",
        "    print(f\"Error: {response.status_code} - {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLIDE_9B_Markdown"
      },
      "source": [
        "## Slide 9: Build a Simple RAG with Langchain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLIDE_9B_Code"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_community.embeddings import GPT4AllEmbeddings\n",
        "import os\n",
        "\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = \"<insert your key here>\"\n",
        "os.environ[\"GROQ_API_KEY\"] = \"<insert your key here>\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = \"<insert your key here>\"\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"HF_TOKEN\"] = \"<insert your key here>\"\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.3-70b-versatile\",#\"mixtral-8x7b-32768\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "chat = llm\n",
        "\n",
        "# INDEXING\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=GPT4AllEmbeddings())\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Prompt from LangChain Hub\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "rag_chain = (\n",
        "    {\n",
        "        \"context\": retriever | format_docs,\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(rag_chain.invoke(\"What is Task Decomposition?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLIDE_10_Markdown"
      },
      "source": [
        "## Slide 10: Using LangChain Chains"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLIDE_10_Code"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from langchain.chains import SequentialChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    temperature=0.9,\n",
        "    max_tokens=400,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Translate the following review to English:\\n\\n{Review}\"\n",
        ")\n",
        "chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key=\"English_Review\")\n",
        "\n",
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Can you summarize the following review in 1 sentence:\\n\\n{English_Review}\"\n",
        ")\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key=\"summary\")\n",
        "\n",
        "third_prompt = ChatPromptTemplate.from_template(\n",
        "    \"What language is the following review:\\n\\n{Review}\"\n",
        ")\n",
        "chain_three = LLMChain(llm=llm, prompt=third_prompt, output_key=\"language\")\n",
        "\n",
        "fourth_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a follow up response to the following summary in the specified language:\\n\\nSummary: {summary}\\nLanguage: {language}\"\n",
        ")\n",
        "chain_four = LLMChain(llm=llm, prompt=fourth_prompt, output_key=\"followup_message\")\n",
        "\n",
        "fifth_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Translate the followup message to English language:\\n\\n{followup_message}\"\n",
        ")\n",
        "chain_five = LLMChain(llm=llm, prompt=fifth_prompt, output_key=\"english_translation\")\n",
        "\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[chain_one, chain_two, chain_three, chain_four, chain_five],\n",
        "    input_variables=[\"Review\"],\n",
        "    output_variables=[\"English_Review\", \"summary\", \"language\", \"followup_message\", \"english_translation\"],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "review = \"میں نے کنگ سائز سیٹ آرڈر کیا تھا۔ میری واحد تنقید یہ ہوگی کہ کاش بیچنے والا کنگ سائز سیٹ کے ساتھ چار تکیے کے غلاف فراہم کرتا...\"\n",
        "\n",
        "results = overall_chain(review)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLIDE_11_Markdown"
      },
      "source": [
        "## Slide 11: LangChain Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLIDE_11_Code"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0.9)\n",
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
        "\n",
        "# Store sample conversation\n",
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
        "response = conversation.predict(input=\"What is on the schedule today?\")\n",
        "print(response)\n",
        "\n",
        "# Another example\n",
        "schedule = \"At 6:30 AM, you have a GEN AI Practical Presentation...\"\n",
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n",
        "memory.save_context({\"input\": \"What is on the schedule today?\"}, {\"output\": f\"{schedule}\"})\n",
        "memory.load_memory_variables({})\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
        "print(conversation.predict(input=\"suggest some good questions to ask about genAI in practice?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLIDE_12_Markdown"
      },
      "source": [
        "## Slide 12: Build “Habib” Conversational Agent (Example)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLIDE_12_Code"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import requests\n",
        "\n",
        "class HabibUniversity:\n",
        "    def __init__(self, model = \"llama3.2:1b\", temperature=0):\n",
        "        self.model = model\n",
        "        self.temperature = temperature\n",
        "        self.url = 'http://localhost:11434/api/generate'\n",
        "        self.system_prompt = \"\"\"\n",
        "        Your name is Habib-Pro. You are a knowledgeable and empathetic virtual assistant for Habib University...\n",
        "        \"\"\"\n",
        "\n",
        "    def generate(self, prompt):\n",
        "        full_prompt = f\"{self.system_prompt}\\n\\nUser: {prompt}\\nAssistant:\"\n",
        "        payload = {\n",
        "            \"model\": self.model,\n",
        "            \"prompt\": full_prompt,\n",
        "            \"temperature\": self.temperature\n",
        "        }\n",
        "        response = requests.post(self.url, json=payload)\n",
        "        return self._handle_response(response)\n",
        "\n",
        "    def _handle_response(self, response):\n",
        "        assembled_response = \"\"\n",
        "        for line in response.iter_lines(decode_unicode=True):\n",
        "            if line.strip():\n",
        "                try:\n",
        "                    data = json.loads(line)\n",
        "                    if \"response\" in data:\n",
        "                        assembled_response += data[\"response\"]\n",
        "                        print(data[\"response\"], end='', flush=True)\n",
        "                    if data.get(\"done\", False):\n",
        "                        break\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"Error decoding JSON: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    local_llm = \"llama3.2:1b\"\n",
        "    llm = HabibUniversity(model=local_llm, temperature=0)\n",
        "    prompt = \"what's your name\"\n",
        "    response = llm.generate(prompt)\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLIDE_12_END_Markdown"
      },
      "source": [
        "## Putting It All Together\n",
        "- **Embeddings** represent text as dense vectors.\n",
        "- **Vector Stores** like Chroma store these vectors.\n",
        "- **RAG** uses external knowledge with LLM prompts.\n",
        "- **LangChain** orchestrates multi-step processes.\n",
        "- **Local/Open-Source LLMs** can be integrated.\n",
        "\n",
        "### Thank You!\n",
        "- Questions?\n",
        "\n",
        "End of Notebook."
      ]
    }
  ]
}