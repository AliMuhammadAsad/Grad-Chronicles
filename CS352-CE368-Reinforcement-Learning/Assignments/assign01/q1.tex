\subsection*{Q1 - [50 points] - Environment Setup}

\begin{solution}
\begin{enumerate}
    \item[(a)] \textbf{[05 points]} 

    $ |\mathcal{S}| $ denotes the total number of possible states in the grid, which can be represented as $ m \times n \times |\{ N, E, S, W \}| $ where the set $ \{ N, E, S, W \} $ denotes the four possible directions (North, East, South, West) that the agent can take, and $m$ and $n$ represent the number of rows and columns in the grid, respectively. Thus, the total number of states in the grid is $ |\mathcal{S}| = 4 \times 4 \times 4 = 64 $.

    Since we only have 3 actions, the total number of possible actions is $ |\mathcal{A}| = 3 $.


    \item[(b)] \textbf{[05 points]} 
    
    This is called a ``Markov'' decision process since it satisfies the Markov property, which states that the future state only depends on the current state and action, and not the states prior. So basically state $s_t$ only dpeends on $s_{t-1}$ and $a_{t-1}$, and not on $s_{t-2}$, $a_{t-2}$, etc.

    \item[(c)] \textbf{[05 points]}

    $ p((1, 1, N) \mid (1, 1, N), M) = 1 $ since there is a wall if they try to move North, hence, they remain in the same state.

    $ p((1, 1, N) \mid (1, 1, E), L) = 1 $ since they'd be turning left while facing east, hence their direction would then be north, but there is a wall up north, hence they remain in the same state.

    $ p((2, 1, S) \mid (1, 1, S), M) = 1 $ since moving forward while facing south on $(1, 1, S)$ would take them to $(2, 1, S)$.

    $p((2, 1, E) \mid (1, 1, S), M) = 0$ since moving forward while facing south on $(1, 1, S)$ would take them to $(2, 1, S)$, but they are trying to move east, hence the probability is 0.


    \item[(d)] \textbf{[05 points]} 
    
    position = $ (1, 1, E) $, $ \gamma = 0.5 $. Then $ R(s, a = R) = R(s, a = L) = 0 $ since both actions only change direction and future rewards will be discounted at $\gamma = 0.5$, unless they are already at the goal state in which case $R = 5$.

    \item[(e)] \textbf{[05 points]}
    
    % The optimal action would be to first turn right, then move forward until $ (4, 1, S) $ is reached, in which case we move left, move forward till $ (4, 4, E) $, turn left, move forward till $ (1, 3, N) $ is reached, then right, move forward to $ (1, 4, E) $, then turn right until $ (4, 4, S) $. 

    The optimal policy if we consider the state $ s = (1, 1, E) $ would be to turn right, move straight until $ (4, 1, S) $ is reached, then turn left, move straight until $ (4, 3, E) $ is reached, then turn left, move straight until $ (1, 3, N) $ is reached, then turn right, move straight until $ (1, 4, E) $ is reached, then turn right until $ (4, 4, S) $ is reached.

    If the agent is already on any of the states amongst these paths, then it just needs to adjust its position to the next state in the path. If its not already on the path, then it should get to the closest state in the path and then follow the path.

    \item[(f)] \textbf{[05 points]}

    A higher discount factor of $ \gamma = 0.9 $ means that it increases the improtance of future rewards. This is because the future rewards are discounted by a factor of $ \gamma $, and a higher discount factor means that the future rewards are discounted less, hence they are more important. However, since the goal state remains the same, the optimal policy would remain the same.

    \item[(g)] \textbf{[05 points]}
    
    The new policy of $ R(s, a) = 0 $ for the avenger's compound and -1 otherwise means that the agent would explore less, and would want to rush to the avenger's compound in order to minimize the penalties. Hence, the optimal policy changes as it explores less and tries to reach the avenger's compound as soon as possible, ideally the shortest path.

    \item[(h)] \textbf{[05 points]}
    
    When $\gamma = 1$, all rewards are weighted equally, hence, the optimal policy remains the same as the agent would try to maximize the rewards by following the same path as before. However, since the agent considers all rewards equally, it would consider all paths equally, hence it would explore more which could lead to a longer path to the avenger's compound.

    \item[(i)] \textbf{[10 points]}
    
    Since moving forward now has only an 80\% chance of success, with a 20\% chance to move 2 steps, the expected reward is $ E[R] = 0.8R_1 + 0.2R_2 $ where $ R_1 $ is the reward after 1 step, and $ R_2 $ is the reward after 2 steps. 

    Then starting from $ s = (2, 4, S) $, the expected reward is $ E[R] = 0.8 \times 0 + 0.2 \times 5 = 1 $.
\end{enumerate}
\end{solution}