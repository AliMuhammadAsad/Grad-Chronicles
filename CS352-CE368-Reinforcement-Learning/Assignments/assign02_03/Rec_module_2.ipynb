{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProValarous/Reinforcement-Learning-Labs/blob/main/Rec_module_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXh6tvrFSRj3"
      },
      "source": [
        "# Recitation Module #2: Dynammic Programming & Monte Carlo Methods\n",
        "\n",
        "Topics covered:\n",
        "- Value Iteration\n",
        "- Policy Interation\n",
        "- Monte-Carlo Prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8HzL-UCzQBl"
      },
      "source": [
        "## Importing Libraries and Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lm6lY8G7zOZm"
      },
      "outputs": [],
      "source": [
        "# HIDE OUTPUT\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZLt4ZtDzb0W"
      },
      "outputs": [],
      "source": [
        "# HIDE OUTPUT\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!sudo apt-get install xvfb\n",
        "!pip install xvfbwrapper\n",
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Rsnoz7pDzfdM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nightwing/.local/lib/python3.12/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
            "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "import gym\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "C4jTuXs1zlPE"
      },
      "outputs": [],
      "source": [
        "## Some Display Utility functions\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment\n",
        "and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data=''''''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def wrap_env(env):\n",
        "    env = RecordVideo(env, './video')\n",
        "    return env\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV02S0DVToWZ"
      },
      "source": [
        "### Techniques for solving MDPs can be separated into three categories:\n",
        "\n",
        "1. **Value-based techniques:** aim to learn the value of states (or learn an estimate for value of states) and\n",
        "actions: that is, they learn value functions or Q functions. We then use policy extraction to get a\n",
        "policy for deciding actions.\n",
        "2. **Policy-based techniques:** learn a policy directly, which completely by-passes learning values of states\n",
        "or actions all together. This is important if for example, the state space or the action space are massive\n",
        "or infinite. If the action space is infinite, then using policy extraction as defined in Part I is not\n",
        "possible because we must iterate over all actions to find the optimal one. If we learn the policy\n",
        "directly, we do not need this.\n",
        "3. **Hybrid techniques:** that combine value- and policy-based techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwqNjI8rB4lr"
      },
      "source": [
        "## Value Iteration\n",
        "\n",
        "Value Iteration is a dynamic-programming method for finding the optimal value function $V^*$ by solving the\n",
        "Bellman equations iteratively. It uses the concept of dynamic programming to maintain a value function $V^*$\n",
        "that approximates the optimal value function $V^*$ , iteratively improving $V$ until it converges to $V^*$ (or\n",
        "close to it).\n",
        "\n",
        "\n",
        "\n",
        "**The algorithm is as follows:**\n",
        "\n",
        "**Input:** MDP $M = \\langle S, s_0, A, P_a(s' \\mid s), r(s, a, s') \\rangle$  \n",
        "**Output:** Value function $V$  \n",
        "\n",
        "Set $V$ to arbitrary value function; e.g., $V(s) = 0$ for all $s$  \n",
        "\n",
        "---\n",
        "\n",
        "**Repeat**  \n",
        "- $\\Delta \\gets 0$  \n",
        "- **For each** $s \\in S$  \n",
        "  - $V'(s) \\gets \\max_{a \\in A(s)} \\sum_{s' \\in S} P_a(s' \\mid s) \\left[ r(s, a, s') + \\gamma V(s') \\right]$ ← _Bellman equation_  \n",
        "  - $\\Delta \\gets \\max(\\Delta, |V'(s) - V(s)|)$  \n",
        "- $V \\gets V'$  \n",
        "\n",
        "**Until** $\\Delta \\leq \\theta$  \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIZMkZqSCGWl"
      },
      "source": [
        "We could also write the algorithm using the idea of Q-values, which is closer to a code-based\n",
        "implementation:\n",
        "\n",
        "<img src=\"https://lcalem.github.io/imgs/sutton/value_iteration.png\" alt=\"Value Iteration\" width=\"700\" height=\"300\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eMxJpfYlCJZA"
      },
      "outputs": [],
      "source": [
        "def value_iteration(env, gamma = 1.0):\n",
        "\n",
        "    # initialize value table with zeros\n",
        "    value_table = np.zeros(env.observation_space.n)\n",
        "\n",
        "    # set number of iterations and threshold\n",
        "    no_of_iterations = 100000\n",
        "    threshold = 1e-20\n",
        "\n",
        "    for i in range(no_of_iterations):\n",
        "\n",
        "        # On each iteration, copy the value table to the updated_value_table\n",
        "        updated_value_table = np.copy(value_table)\n",
        "\n",
        "        # Now we calculate Q Value for each actions in the state\n",
        "        # and update the value of a state with maximum Q value\n",
        "\n",
        "        for state in range(env.observation_space.n):\n",
        "            Q_value = []\n",
        "            for action in range(env.action_space.n):\n",
        "                next_states_rewards = []\n",
        "\n",
        "                # (TODO) implement bellman equation for the value function\n",
        "                # for next_state_reward in env.P[state][action]:\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    next_states_rewards.append(prob * (reward + gamma * value_table[next_state] * (not done)))\n",
        "                \n",
        "                Q_value.append(sum(next_states_rewards))\n",
        "\n",
        "            updated_value_table[state] = max(Q_value)\n",
        "\n",
        "        # we will check whether we have reached the convergence i.e whether the difference\n",
        "        # between our value table and updated value table is very small. But how do we know it is very\n",
        "        # small? We set some threshold and then we will see if the difference is less\n",
        "        # than our threshold, if it is less, we break the loop and return the value function as optimal\n",
        "        # value function\n",
        "\n",
        "        # (TODO) implement convergence check\n",
        "        error = np.sum(np.fabs(updated_value_table - value_table))\n",
        "        value_table = updated_value_table.copy()\n",
        "\n",
        "        if error < threshold:\n",
        "            print(\"Converged at iteration: {}\".format(i))\n",
        "            break\n",
        "\n",
        "\n",
        "    return value_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ImwL44zMCM4Q"
      },
      "outputs": [],
      "source": [
        "def extract_policy(env, value_table, gamma = 1.0):\n",
        "    # initialize the policy with zeros\n",
        "    policy = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for state in range(env.observation_space.n):\n",
        "\n",
        "        # (TODO) initialize the Q table for a state\n",
        "        Q_table = np.zeros(env.action_space.n)\n",
        "\n",
        "        # (TODO) compute Q value for all ations in the state\n",
        "        for action in range(env.action_space.n):\n",
        "            # for next_sr in env.P[state][action]:\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                Q_table[action] += prob * (reward + gamma * value_table[next_state] * (not done))\n",
        "                \n",
        "\n",
        "        # (TODO) select the action which has maximum Q value as an optimal action of the state\n",
        "        policy[state] = np.argmax(Q_table)\n",
        "\n",
        "    return policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gymnasium.envs.toy_text.frozen_lake import generate_random_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "W6xV8v3DCYbp"
      },
      "outputs": [],
      "source": [
        "env_train = gym.make('FrozenLake-v1',desc=generate_random_map(size=8),render_mode=\"rgb_array\")\n",
        "venv_train = env_train.unwrapped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dHlN2bZ9Ca-s"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converged at iteration: 64\n"
          ]
        }
      ],
      "source": [
        "optimal_value_function = value_iteration(env=venv_train,gamma=0.5)\n",
        "# optimal_value_function = value_iteration(env=env_train,gamma=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hYbqsc23CfTo"
      },
      "outputs": [],
      "source": [
        "optimal_policy = extract_policy(venv_train,optimal_value_function, gamma=0.9)\n",
        "# optimal_policy = extract_policy(env_train,optimal_value_function, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mWa3rehwCfxL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1. 2. 3. 2. 2. 2. 0. 0. 1. 0. 0. 2. 2. 1. 0. 3. 1. 1. 1. 2. 2. 3. 0. 0.\n",
            " 3. 2. 2. 3. 0. 0. 2. 1. 0. 2. 0. 0. 2. 1. 3. 2. 1. 1. 1. 1. 2. 0. 0. 2.\n",
            " 3. 3. 0. 0. 0. 0. 0. 2. 0. 0. 2. 1. 1. 1. 1. 0.]\n"
          ]
        }
      ],
      "source": [
        "print(optimal_policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_je5ogVPCkyb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Building video /home/nightwing/Desktop/Habib/Sem8/rl/assignments/assign02/video/rl-video-episode-0.mp4.\n",
            "MoviePy - Writing video /home/nightwing/Desktop/Habib/Sem8/rl/assignments/assign02/video/rl-video-episode-0.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Done !\n",
            "MoviePy - video ready /home/nightwing/Desktop/Habib/Sem8/rl/assignments/assign02/video/rl-video-episode-0.mp4\n",
            "Episode finished with total reward: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create an environment (e.g., CartPole)\n",
        "total_reward = 0\n",
        "\n",
        "while total_reward <= 0:\n",
        "    env = gym.make('FrozenLake-v1', desc=generate_random_map(size=8), render_mode=\"rgb_array\")\n",
        "    env = gym.wrappers.RecordVideo(\n",
        "        env, \n",
        "        './video',\n",
        "        episode_trigger=lambda episode_id: True,\n",
        "        name_prefix=\"frozen_lake\")\n",
        "\n",
        "    env = wrap_env(env_train)\n",
        "\n",
        "    # Reset the environment\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    # Interaction loop\n",
        "    while not done:\n",
        "        # Get action from the optimal policy\n",
        "        action = int(optimal_policy[state])\n",
        "        \n",
        "        # Step through the environment\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        \n",
        "        # Update state and total reward\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        done = terminated or truncated\n",
        "\n",
        "    print(f\"Episode finished with total reward: {total_reward}\")\n",
        "\n",
        "    # Close the environment\n",
        "    env.close()\n",
        "\n",
        "# Display the video\n",
        "show_video()\n",
        "\n",
        "\n",
        "\n",
        "# # Reset the environment\n",
        "# state, _ = env.reset()\n",
        "\n",
        "# # Start the recorder (utility for displaying output)\n",
        "# env.start_video_recorder()\n",
        "\n",
        "# next_state = state\n",
        "\n",
        "# # Example of an interaction loop\n",
        "# for _ in range(10000):\n",
        "\n",
        "\n",
        "#     # Render the environment\n",
        "#     env.render()\n",
        "\n",
        "\n",
        "#     # Sample random action from action space\n",
        "#     # action = env.action_space.sample()\n",
        "#     action = int(optimal_policy[next_state])\n",
        "\n",
        "#     # print(type(action),next_state)\n",
        "\n",
        "#     # Step through the environment using the action\n",
        "#     next_state, reward, done, info = env.step(action)\n",
        "\n",
        "#     # Break the loop if the episode is done\n",
        "#     if done:\n",
        "#         break\n",
        "\n",
        "\n",
        "# # close the video recorder(utility for displaying output)\n",
        "# env.close_video_recorder()\n",
        "\n",
        "# # Close the environment\n",
        "# env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HqlP0px7CmyC"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Display Output\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts9iXKRf_YDu"
      },
      "source": [
        "## **Policy Iteration**\n",
        "\n",
        "Policy Iteration is a fundamental method in **Dynamic Programming (DP)** to solve **Markov Decision Processes (MDPs)**. It helps find an optimal policy $ \\pi^* $ that maximizes the expected cumulative reward.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Idea**  \n",
        "Policy Iteration alternates between two steps:\n",
        "\n",
        "1. **Policy Evaluation** – Compute the value function $ V^\\pi(s) $ for a given policy $ \\pi $.\n",
        "2. **Policy Improvement** – Update the policy by selecting actions that maximize the expected reward.\n",
        "\n",
        "This repeats until the policy converges to the optimal policy.\n",
        "\n",
        "---\n",
        "\n",
        "### **Steps of Policy Iteration**\n",
        "1. **Initialize** a random policy $ \\pi_0 $.\n",
        "----\n",
        "2. **Policy Evaluation:**  \n",
        "   - Compute the value function $ V^\\pi(s) $ by solving the Bellman equation: $$ V^\\pi(s) = \\sum_{s'} P(s' | s, \\pi(s)) \\left[ R(s, \\pi(s), s') + \\gamma V^\\pi(s') \\right] $$\n",
        "   - This finds the expected reward for following $ \\pi $.\n",
        "\n",
        "**Here's a more algorithmic approach:**\n",
        "\n",
        "**Input:** $\\pi$ the policy for evaluation, $V^\\pi$ value function, and  \n",
        "MDP $M = \\langle S, s_0, A, P_a(s' \\mid s), r(s, a, s') \\rangle$  \n",
        "\n",
        "**Output:** Value function $V^\\pi$  \n",
        "\n",
        "**Repeat**  \n",
        "- $\\Delta \\gets 0$  \n",
        "- **For each** $s \\in S$  \n",
        "  - $$V'^\\pi(s) \\gets \\sum_{s' \\in S} P_{\\pi(s)}(s' \\mid s) \\left[ r(s, a, s') + \\gamma V^\\pi(s') \\right]$$  \n",
        "    _Policy evaluation equation_  \n",
        "  - $\\Delta \\gets \\max(\\Delta, |V'^\\pi(s) - V^\\pi(s)|)$  \n",
        "- $V^\\pi \\gets V'^\\pi$  \n",
        "\n",
        "**Until** $\\Delta \\leq \\theta$  \n",
        "\n",
        "-----\n",
        "\n",
        "3. **Policy Improvement:**  \n",
        "   - For each state $ s $, update the policy:$$ \\pi(s) = \\arg\\max_a \\sum_{s'} P(s' | s, a) \\left[ R(s, a, s') + \\gamma V^\\pi(s') \\right]$$\n",
        "   - If the policy doesn't change, stop. Otherwise, repeat from Step 2.\n",
        "\n",
        "---\n",
        "**Here's a full algorithmic Policy Iteration:**\n",
        "\n",
        "\n",
        "**Input:** MDP $M = \\langle S, s_0, A, P_a(s' \\mid s), r(s, a, s') \\rangle$  \n",
        "**Output:** Policy $\\pi$  \n",
        "\n",
        "Set $V^\\pi$ to arbitrary value function; e.g., $V^\\pi(s) = 0$ for all $s$  \n",
        "Set $\\pi$ to arbitrary policy; e.g., $\\pi(s) = a$ for all $s$, where $a \\in A$ is an arbitrary action  \n",
        "\n",
        "**Repeat**  \n",
        "- Compute $V^\\pi(s)$ for all $s$ *(using policy evaluation)*  \n",
        "- **For each** $s \\in S$:  \n",
        "  - $$\\pi(s) \\gets \\arg\\max_{a \\in A(s)} Q^\\pi(s, a)$$  \n",
        "- **Until** $\\pi$ does not change *(policy Improvement)*\n",
        "\n",
        "---\n",
        "\n",
        "### **Example: Grid World**\n",
        "Consider a 3×3 grid where an agent moves **up, down, left, or right** to reach a goal while receiving rewards.\n",
        "\n",
        "Policy Iteration helps the agent learn the best moves by:\n",
        "- Evaluating the current policy.\n",
        "- Updating the policy to improve its actions.\n",
        "- Repeating until the policy stops changing.\n",
        "\n",
        "---\n",
        "\n",
        "### **Comparison with Value Iteration**\n",
        "| **Method**          | **Approach** |\n",
        "|---------------------|-------------|\n",
        "| **Policy Iteration** | Alternates between policy evaluation and improvement. |\n",
        "| **Value Iteration**  | Updates values directly using the Bellman optimality equation. |\n",
        "\n",
        "Both find the optimal policy, but **Value Iteration** can be more efficient for large state spaces.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "HSnPHl-Xl-aT"
      },
      "outputs": [],
      "source": [
        "def compute_value_function(env, policy, gamma=1.0):\n",
        "\n",
        "    #now, let's define the number of iterations\n",
        "    num_iterations = 1000\n",
        "\n",
        "    #define the threshold value\n",
        "    threshold = 1e-20\n",
        "\n",
        "    #set the discount factor\n",
        "    gamma = 0.99\n",
        "\n",
        "    #now, we will initialize the value table, with the value of all states to zero\n",
        "    value_table = np.zeros(env.observation_space.n)\n",
        "\n",
        "    #for every iteration\n",
        "    for i in range(num_iterations):\n",
        "\n",
        "        #update the value table, that is, we learned that on every iteration, we use the updated value\n",
        "        #table (state values) from the previous iteration\n",
        "        updated_value_table = np.copy(value_table)\n",
        "\n",
        "        #thus, for each state, we select the action according to the given policy and then we update the\n",
        "        #value of the state using the selected action as shown below\n",
        "        #for each state\n",
        "        for s in range(env.observation_space.n):\n",
        "            #select the action in the state according to the policy\n",
        "            a = policy[s]\n",
        "\n",
        "            # TODO: compute the value of the state using the selected action\n",
        "            new_value = 0\n",
        "\n",
        "            for prob, next_state, reward, done in env.P[s][a]:\n",
        "                new_value += prob * (reward + gamma * value_table[next_state] * (not done))\n",
        "            updated_value_table[s] = new_value\n",
        "\n",
        "\n",
        "        #after computing the value table, that is, value of all the states, we check whether the\n",
        "        #difference between value table obtained in the current iteration and previous iteration is\n",
        "        #less than or equal to a threshold value if it is less then we break the loop and return the\n",
        "        #value table as an accurate value function of the given policy\n",
        "\n",
        "        # (TODO) implement convergence check\n",
        "        error = np.sum(np.fabs(updated_value_table - value_table))\n",
        "        value_table = updated_value_table.copy()\n",
        "        if error < threshold:\n",
        "            print(\"Converged at iteration: {}\".format(i))\n",
        "            break\n",
        "\n",
        "    return value_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "20mr7fG-mUNl"
      },
      "outputs": [],
      "source": [
        "def policy_iteration(env):\n",
        "\n",
        "    #set the number of iterations\n",
        "    num_iterations = 1000\n",
        "\n",
        "    #we learned that in the policy iteration method, we begin by initializing a random policy.\n",
        "    #so, we will initialize the random policy which selects the action 0 in all the states\n",
        "    policy = np.zeros(env.observation_space.n)\n",
        "\n",
        "    #for every iteration\n",
        "    for i in range(num_iterations):\n",
        "        #compute the value function using the policy\n",
        "        value_function = compute_value_function(env, policy)\n",
        "\n",
        "        #extract the new policy from the computed value function\n",
        "        new_policy = extract_policy(env, value_function, gamma=0.9)\n",
        "\n",
        "        # TODO: if the policy and new_policy are same then break the loop\n",
        "        if np.array_equal(policy, new_policy):\n",
        "            print(\"Converged at iteration: {}\".format(i))\n",
        "            break\n",
        "\n",
        "        # TODO: else, update the current policy to new_policy\n",
        "        policy = new_policy\n",
        "\n",
        "\n",
        "    return policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "DzpQV140m1je"
      },
      "outputs": [],
      "source": [
        "env_train = gym.make('FrozenLake-v1',desc=generate_random_map(size=8),render_mode=\"rgb_array\")\n",
        "venv_train = env_train.unwrapped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "_SlSK514m-S5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converged at iteration: 62\n",
            "Converged at iteration: 302\n",
            "Converged at iteration: 758\n",
            "Converged at iteration: 11\n"
          ]
        }
      ],
      "source": [
        "optimal_policy = policy_iteration(venv_train)\n",
        "# optimal_policy = policy_iteration(env_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 2. 2. 0. 0. 1. 1. 1. 1. 2. 2. 0. 3. 3. 3. 3. 3. 2. 2. 0. 0.\n",
            " 3. 3. 0. 0. 2. 3. 3. 1. 0. 0. 2. 1. 0. 0. 0. 2. 3. 1. 3. 3. 0. 0. 1. 2.\n",
            " 3. 3. 0. 0. 2. 1. 2. 1. 0. 2. 0. 0. 0. 0. 2. 0.]\n"
          ]
        }
      ],
      "source": [
        "print(optimal_policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "5L5DAsSMnGQ2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nightwing/.local/lib/python3.12/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/nightwing/Desktop/Habib/Sem8/rl/assignments/assign02/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 0.0\n",
            "Episode finished with total reward: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Create a new environment with video recording\n",
        "total_reward = 0\n",
        "while total_reward <= 0:\n",
        "    env = gym.make('FrozenLake-v1', desc=generate_random_map(size=8), render_mode=\"rgb_array\")\n",
        "    env = gym.wrappers.RecordVideo(\n",
        "        env, \n",
        "        \"video\", \n",
        "        episode_trigger=lambda episode_id: True,  # record every episode\n",
        "        name_prefix=\"frozen-lake-policy-iteration\"\n",
        "    )\n",
        "\n",
        "    # Reset the environment\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    # Interaction loop\n",
        "    while not done:\n",
        "        # Get action from the optimal policy\n",
        "        action = int(optimal_policy[state])\n",
        "        \n",
        "        # Step through the environment\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        \n",
        "        # Update state and total reward\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        done = terminated or truncated\n",
        "\n",
        "    print(f\"Episode finished with total reward: {total_reward}\")\n",
        "\n",
        "    # Close the environment\n",
        "    env.close()\n",
        "\n",
        "################################################################\n",
        "\n",
        "# # Create an environment (e.g., CartPole)\n",
        "# env = wrap_env(env_train)\n",
        "\n",
        "\n",
        "# # Reset the environment\n",
        "# state, _ = env.reset()\n",
        "\n",
        "# # Start the recorder (utility for displaying output)\n",
        "# env.start_video_recorder()\n",
        "\n",
        "# next_state = state\n",
        "\n",
        "# # Example of an interaction loop\n",
        "# for _ in range(10000):\n",
        "\n",
        "#     # Render the environment\n",
        "#     env.render()\n",
        "\n",
        "\n",
        "#     # Sample random action from action space\n",
        "#     # action = env.action_space.sample()\n",
        "#     action = int(optimal_policy[next_state])\n",
        "\n",
        "#     # print(type(action),next_state)\n",
        "\n",
        "#     # Step through the environment using the action\n",
        "#     next_state, reward, done, info = env.step(action)\n",
        "\n",
        "#     # Break the loop if the episode is done\n",
        "#     if done:\n",
        "#         break\n",
        "\n",
        "\n",
        "# # close the video recorder(utility for displaying output)\n",
        "# env.close_video_recorder()\n",
        "\n",
        "# # Close the environment\n",
        "# env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "NMFM0lQnnOZo"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Display Output\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z_rpJFmSfZX"
      },
      "source": [
        "## What is Monte Carlo?\n",
        "\n",
        "Monte Carlo methods are a class of algorithms that rely on repeated random sampling to obtain numerical results. The core idea is to use randomness to solve problems that might be deterministic in principle. In the context of Reinforcement Learning (RL), Monte Carlo methods are used to estimate the value of states or state-action pairs by running multiple episodes and observing the outcomes. The key concept here is **trial and error**—we perform random experiments (trials), and based on the results, we compute averages that approximate the true values.\n",
        "\n",
        "#### Key Points:\n",
        "- **Random sampling**: Monte Carlo methods use randomness to simulate and explore possible outcomes.\n",
        "- **Episodic tasks**: They are often used in episodic environments where each episode ends in a terminal state.\n",
        "- **No need for a model**: Monte Carlo doesn’t require a complete model of the environment, unlike some other methods in RL, such as Dynamic Programming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfkDo71pSk2Y"
      },
      "source": [
        "### Where is it Applicable in Reinforcement Learning?\n",
        "\n",
        "In RL, Monte Carlo methods are used to solve Markov Decision Processes (MDPs) by estimating the value functions based on actual experiences, which is different from Dynamic Programming (DP) methods like Policy Iteration or Value Iteration where we know the model dynamics.\n",
        "\n",
        "- **Monte Carlo**:\n",
        "    - **Model-free**: It doesn’t need the transition probabilities or reward model of the environment.\n",
        "    - **Episodic**: It requires full episodes to compute an average return.\n",
        "    - **Learning from interaction**: Monte Carlo learns from real or simulated experiences, by averaging returns for visited states.\n",
        "    - **Convergence**: It only converges after multiple episodes, especially for larger MDPs.\n",
        "  \n",
        "- **Dynamic Programming**:\n",
        "    - **Model-based**: DP requires a known model of the environment, including transition probabilities and rewards.\n",
        "    - **Bootstrapping**: DP methods use current estimates of value functions to update state values iteratively.\n",
        "    - **Complete information**: It works by sweeping over the entire state space and updating values simultaneously.\n",
        "\n",
        "Monte Carlo methods are more useful when we can’t directly model the environment, whereas Dynamic Programming is effective when we know the full MDP model. Monte Carlo works well for large-scale problems or when interacting with the environment is cheap or necessary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFzYue8ASqyN"
      },
      "source": [
        "### Estimating pi-value using Monte Carlo\n",
        "\n",
        "Monte Carlo methods are often applied to estimate values that are otherwise hard to calculate, such as the mathematical constant π (pi). Here’s how we can estimate π using Monte Carlo:\n",
        "\n",
        "- **Basic Idea**: Imagine a unit square with a circle inscribed in it (diameter = 1). The area of the square is 1, and the area of the circle is π/4. If we randomly generate points within the square, the ratio of points that land inside the circle to the total number of points should approximate the area ratio between the circle and square, which is π/4.\n",
        "\n",
        "- **Steps**:\n",
        "  1. Randomly generate points $(x, y)$ in the unit square.\n",
        "  2. Check if the point lies inside the circle i.e., if $( x^2 + y^2 \\leq 1 )$.\n",
        "  3. We calculate the value π (pi) by multiplying four to the division of the number of points inside the circle to the number of points inside the square.\n",
        "  4. If we increase the number of samples, the better we can approximate.\n",
        "\n",
        "This is a simple but powerful demonstration of Monte Carlo’s ability to estimate quantities through random sampling.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://help.ovhcloud.com/public_cloud-data_analytics-data_processing-40_tutorial_calculate_pi-images-monte_carlo_graph.png\" alt=\"pi_est\" width=\"200\" height=\"200\">\n",
        "</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKiS0N_WXhdo"
      },
      "outputs": [],
      "source": [
        "### TODO: Estimate the value of pi using Monte Carlo\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def pi_check(estimate): # DO NOT MODIFY THIS FUNCTION\n",
        "    pi = math.pi\n",
        "    difference = abs(pi - estimate)\n",
        "    if difference < 0.00001:\n",
        "        print('Test Case passed')\n",
        "    else:\n",
        "        print('Test Case not passed')\n",
        "\n",
        "# Implement you solution below this line\n",
        "def mc_pi_estimator():\n",
        "    num_strata = 1000\n",
        "    num_samples = 1000000\n",
        "\n",
        "    total_inside_circle = 0\n",
        "    samples_per_stratum = num_samples // num_strata\n",
        "    \n",
        "    for i in range(num_strata):\n",
        "        for j in range(num_strata):\n",
        "            # Define the bounds of the stratum\n",
        "            x_min, x_max = -1 + i * (2 / num_strata), -1 + (i + 1) * (2 / num_strata)\n",
        "            y_min, y_max = -1 + j * (2 / num_strata), -1 + (j + 1) * (2 / num_strata)\n",
        "            \n",
        "            # Generate random points within the stratum\n",
        "            x = np.random.uniform(x_min, x_max, samples_per_stratum)\n",
        "            y = np.random.uniform(y_min, y_max, samples_per_stratum)\n",
        "\n",
        "            distance = np.sqrt(x**2 + y**2)\n",
        "            total_inside_circle += np.sum(distance <= 1)\n",
        "    \n",
        "    pi_estimate = (total_inside_circle / num_samples) * 4\n",
        "    return pi_estimate / 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimated π: 3.141590548\n",
            "Test Case passed\n"
          ]
        }
      ],
      "source": [
        "pi_estimate = mc_pi_estimator()\n",
        "print(f\"Estimated π: {pi_estimate}\")\n",
        "pi_check(pi_estimate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0GUZLK1SyEG"
      },
      "source": [
        "### Making a Monte Carlo Integrator\n",
        "\n",
        "Monte Carlo methods are widely used to approximate integrals, especially for high-dimensional or complex integrals where traditional methods fail or are inefficient.\n",
        "\n",
        "- **Problem**: You want to compute the integral of a function  $f(x)$ over an interval $[a, b]$.\n",
        "  \n",
        "- **Monte Carlo Approach**:\n",
        "  1. **Random Sampling**: Generate random points $ x_i $ uniformly in the interval $[a, b]$.\n",
        "  2. **Function Evaluation**: Compute the function value $ f(x_i) $ for each sampled point.\n",
        "  3. **Averaging**: Compute the average of the function values and scale it by the interval width, $( b - a )$.\n",
        "\n",
        "The estimated integral is:\n",
        "$$\n",
        "\\int_a^b f(x) dx \\approx (b - a) \\times \\frac{1}{N} \\sum_{i=1}^{N} f(x_i)\n",
        "$$\n",
        "where $( N )$ is the number of samples.\n",
        "\n",
        "This approach is useful when the function is difficult to integrate analytically, or when the domain of integration is high-dimensional. Monte Carlo integration's accuracy improves as the number of samples increases, though convergence is slow compared to other numerical methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sqdYTjX9ZolR"
      },
      "outputs": [],
      "source": [
        "### TODO: compute the integral of a function  f(x) over an interval [a, b] using Monte Carlo\n",
        "\n",
        "# Implement you solution below this line\n",
        "def mc_integrator(func, a, b, num_samples=1000000):\n",
        "    x_samples = np.random.uniform(a, b, num_samples)\n",
        "    f_vals = np.array([func(x) for x in x_samples])\n",
        "\n",
        "    integral_estimate = (b - a) * np.mean(f_vals)\n",
        "    return integral_estimate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNiDAedubAig"
      },
      "source": [
        "Solve following Integral using your implemented MC Integrator:\n",
        "\n",
        "$$\n",
        "    \\int_0^{\\frac{\\pi}{2}} \\frac{\\sqrt{sin(x)}}{\\sqrt{sin(x)}+\\sqrt{cos(x)}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i4dQzWhrb2fi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimated integral: 0.784928786440443\n"
          ]
        }
      ],
      "source": [
        "### TODO: Solve the above integral using your implemented MC Integrator\n",
        "def specific_integral_function(x):\n",
        "    def less_than(val1, val2):\n",
        "        return val1 < val2\n",
        "    # if (math.sin(x) < 1e-10) or (math.cos < 1e-10):\n",
        "    if less_than(math.sin(x), 1e-10) or less_than(math.cos(x), 1e-10):\n",
        "        return 0.0\n",
        "    \n",
        "    numer = math.sqrt(math.sin(x))\n",
        "    denom = math.sqrt(math.sin(x)) + math.sqrt(math.cos(x))\n",
        "\n",
        "    if denom < 1e-10:\n",
        "        return 0.0\n",
        "    return numer / denom\n",
        "\n",
        "a, b = 0, math.pi / 2\n",
        "result = mc_integrator(specific_integral_function, a, b)\n",
        "print(f\"Estimated integral: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-U5u-ixKaMcj"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "### TODO: Plot a graph of MC Integrator Accuracy vs Speed\n",
        "import time\n",
        "\n",
        "sample_sizes = [10, 100, 1000, 10000, 100000, 1000000]\n",
        "results, times = [], []\n",
        "\n",
        "for n in sample_sizes:\n",
        "    start_time = time.time()\n",
        "    integral = mc_integrator(specific_integral_function, a, b, n)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    results.append(integral)\n",
        "    times.append(elapsed_time)\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "color = \"tab:blue\"\n",
        "ax1.set_xlabel(\"Sample Size (log scale)\")\n",
        "ax1.set_ylabel(\"Estimated Integral\", color=color)\n",
        "ax1.plot(sample_sizes, results, 'o-', color=color)\n",
        "ax1.tick_params(axis='y', labelcolor=color)\n",
        "ax1.set_xscale('log')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "color = \"tab:red\"\n",
        "ax2.set_ylabel(\"Time (seconds)\", color=color)  # we already handled the x-label with ax1\n",
        "ax2.plot(sample_sizes, times, 'o-', color=color)\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "plt.title(\"MC Integrator Accuracy vs Speed\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIMTUKl_TFPG"
      },
      "source": [
        "## Monte Carlo Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htC-zp6-TIev"
      },
      "source": [
        "### General Overview\n",
        "\n",
        "Monte Carlo Prediction is a technique in Reinforcement Learning (RL) used to estimate the value of states (or state-action pairs) based on the observed returns from sampled episodes. The goal is to approximate the value function, which represents the expected return (cumulative future rewards) starting from a given state under a specific policy.\n",
        "\n",
        "- **Objective**: For a given policy $ \\pi $, Monte Carlo methods aim to estimate the **value function** $ V^\\pi(s) $ for each state $ s $, which is the expected return starting from $ s $, and following the policy $ \\pi $. We estimate the expected return using mean returns.\n",
        "- **Episode-based learning**: Monte Carlo methods compute value estimates by averaging returns across multiple episodes. An episode is a sequence of states, actions, and rewards that ends in a terminal state.\n",
        "  \n",
        "In Monte Carlo Prediction, the value of a state is updated based on the actual returns received in episodes, which allows it to handle non-Markov environments and environments where a model of the dynamics is unknown. This is why Monte Carlo prediction is called *'model-free'* learning algorithm. This method works particularly well for **episodic tasks** where we can easily define when an episode starts and ends.\n",
        "\n",
        "---\n",
        "\n",
        "The steps involved in Monte Carlo prediction are as follows:\n",
        "\n",
        "1. First, we initialize a random value to our value function.\n",
        "2. Then we initialize an emplty list called a return to store our return.\n",
        "3. Then for each state in the episode, we calculate the return.\n",
        "4. Next, we append the return to out return list.\n",
        "5. Finally, we take average of return as our value function.\n",
        "\n",
        "----\n",
        "\n",
        "The Monte Carlo Prediction algorithm is of two types;\n",
        "\n",
        "- First Visit Monte Carlo\n",
        "- Every Visit Monte Carlo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDnnajAOTLxX"
      },
      "source": [
        "### First Visit Monte Carlo\n",
        "\n",
        "In First Visit Monte Carlo (MC) we average the return only the first time the state is visited in an episode. In Reinforcement Learning, a state may be visited multiple times within a single episode.\n",
        "- First Visit Monte Carlo focuses on the first occurrence of each state to provide an unbiased estimate of its value.\n",
        "\n",
        "- First Visit ensures that the value of a state is only updated once per episode, based on the first time it was encountered.\n",
        "- This method helps avoid correlation between multiple visits to the same state in a single episode, which can introduce bias into the value estimation.\n",
        "By averaging the returns for the first visits over many episodes, we get a robust estimate of the value function for each state.\n",
        "\n",
        "#### Algorithm for First Visit MC Prediction:\n",
        "\n",
        "1. **Initialize** the value function $ V(s) $ for all states arbitrarily (or to zeros) and a counter $ N(s) = 0 $ for each state $ s $ to track the number of first visits to $ s $.\n",
        "\n",
        "2. **Simulate episodes**: For each episode:\n",
        "    - Generate a complete episode by following the given policy $ \\pi $.\n",
        "    - An episode is a sequence of states, actions, and rewards:\n",
        "      $$\n",
        "      (S_0, A_0, R_1, S_1, A_1, R_2, \\dots, S_T)\n",
        "      $$\n",
        "      where $ S_T $ is the terminal state, and $ T $ is the time step at which the episode ends.\n",
        "\n",
        "3. **Track first visits**: For each state $ S_t $ visited in the episode, check if this is the first time it has been visited within the episode. If so:\n",
        "    - **Compute the return** $ G_t $, which is the total discounted reward from time step $ t $ until the end of the episode:\n",
        "      $$\n",
        "      G_t = R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma^{T-t} R_T\n",
        "      $$\n",
        "    - **Update the value function** $ V(S_t) $ based on this return:\n",
        "      $$\n",
        "      V(S_t) \\leftarrow V(S_t) + \\alpha [G_t - V(S_t)]\n",
        "      $$\n",
        "      where $ \\alpha $ is the learning rate, or:\n",
        "      $$\n",
        "      α = \\frac{1}{N(S_t)}\n",
        "      $$\n",
        "      if you are averaging over all returns for state $ S_t $.\n",
        "  \n",
        "    - **Update the count** for the first visit to the state:\n",
        "      $$\n",
        "      N(S_t) \\leftarrow N(S_t) + 1\n",
        "      $$\n",
        "\n",
        "4. **Repeat** this process over many episodes to improve the estimates of the value function.\n",
        "\n",
        "---\n",
        "\n",
        "More concisely, the algorithm is as follows:\n",
        "\n",
        "<img src=\"https://lcalem.github.io/imgs/sutton/first_visit.png\" alt=\"Girl in a jacket\" width=\"700\" height=\"300\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Gh7pVXUXG6MT"
      },
      "outputs": [],
      "source": [
        "### TODO: Implement a function to generate an episode using policy (pi)\n",
        "\n",
        "def generate_episodes(env, policy=None, n_episodes=1) -> list:\n",
        "    episodes = []\n",
        "    \n",
        "    for _ in range(n_episodes):\n",
        "        episode = []\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        \n",
        "        while not done:\n",
        "            if policy is None: action = env.action_space.sample()  # Random policy\n",
        "            else: action = policy(state)\n",
        "            \n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            \n",
        "            state = next_state\n",
        "            done = terminated or truncated\n",
        "        episodes.append(episode)\n",
        "    \n",
        "    return episodes if n_episodes > 1 else episodes[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "d1UKMW8jWyGN"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def first_visit_mc_prediction(env, n_episodes, gamma=1.0, policy=None):\n",
        "    # First, we initialize the empty value table as a dictionary for storing the values of each state\n",
        "    value_table = defaultdict(float)\n",
        "    N = defaultdict(int)\n",
        "\n",
        "    for i in range(n_episodes):\n",
        "        # TODO: Generate the epsiode and store the states and rewards\n",
        "        episode = generate_episodes(env, policy=policy)\n",
        "\n",
        "        # TODO: For each step, we store the rewards to a variable R and states to S, and we calculate\n",
        "        # returns as a sum of rewards\n",
        "        states = [s for s, _, _ in episode]\n",
        "        rewards = [r for _, _, r in episode]\n",
        "        visited_states = set()\n",
        "\n",
        "        G = 0\n",
        "\n",
        "        for t in range(len(states) - 1, -1, -1):\n",
        "            G = gamma * G + rewards[t]\n",
        "            state = states[t]\n",
        "\n",
        "            # TODO: Perform first visit MC, we check if the episode is visited for the first time, if yes,\n",
        "            # we simply take the average of returns and assign the value of the state as an average of returns\n",
        "\n",
        "            if state not in visited_states: \n",
        "                visited_states.add(state)\n",
        "                N[state] += 1\n",
        "                value_table[state] += (G - value_table[state]) / N[state]\n",
        "        \n",
        "        if (i + 1) % 10000 == 0:\n",
        "            print(f\"First-visit MC: Completed {i + 1}/{n_episodes} episodes\")\n",
        "\n",
        "    return value_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaT1HXyvG3j0"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRXD7LgkTP12"
      },
      "source": [
        "### Every Visit Monte Carlo\n",
        "\n",
        "**Every Visit Monte Carlo** is a model-free prediction method in Reinforcement Learning (RL) used to estimate the value function $ V^\\pi(s) $ of a given policy $ \\pi $. Like other Monte Carlo methods, it uses complete episodes of experience and updates the estimated value of a state based on the **returns** from multiple episodes.\n",
        "\n",
        "The key feature of **Every Visit Monte Carlo** is that it updates the value of a state every time the state is visited within an episode, rather than just the first time (which is what happens in First Visit Monte Carlo).\n",
        "\n",
        "---\n",
        "\n",
        "### Why Use Every Visit Monte Carlo?\n",
        "\n",
        "In environments where states can be revisited multiple times within the same episode, **Every Visit Monte Carlo** provides a way to leverage all available information by updating the value of a state each time it is visited.\n",
        "\n",
        "- **More frequent updates**: By updating the value of a state for every visit, you can learn faster in environments where the agent frequently revisits the same state.\n",
        "- **Utilizes more data**: Every occurrence of a state gives more opportunities to estimate the value, which may lead to faster convergence of the value function compared to First Visit Monte Carlo.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Cd6hOcLEa81x"
      },
      "outputs": [],
      "source": [
        "### TODO: Complete 'every_visit_mc_prediction()' function\n",
        "\n",
        "def every_visit_mc_prediction(env, n_episodes, gamma=1.0):\n",
        "\n",
        "    # First, we initialize the empty value table as a dictionary for storing the values of each state\n",
        "    value_table = defaultdict(float)\n",
        "    N = defaultdict(int)\n",
        "\n",
        "\n",
        "    for i in range(n_episodes):\n",
        "        # TODO: Generate the epsiode and store the states and rewards\n",
        "        episode = generate_episodes(env)\n",
        "\n",
        "        states = [s for s, _, _ in episode]\n",
        "        rewards = [r for _, _, r in episode]\n",
        "\n",
        "        G = 0\n",
        "        # TODO: For each step, we store the rewards to a variable R and states to S, and we calculate\n",
        "        # returns as a sum of rewards\n",
        "        for t in range(len(states) - 1, -1, -1):\n",
        "            G = gamma * G + rewards[t]\n",
        "            state = states[t]\n",
        "\n",
        "            # TODO: Perform every visit MC, we simply take the average of returns and assign the value of the state as an average of returns\n",
        "            N[state] += 1\n",
        "            value_table[state] += (G - value_table[state]) / N[state]\n",
        "\n",
        "        if (i + 1) % 10000 == 0:\n",
        "            print(f\"Every-visit MC: Completed {i + 1}/{n_episodes} episodes\")\n",
        "\n",
        "    return value_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgwURwMAON3_"
      },
      "source": [
        "\n",
        "### Key Differences between First Visit Monte Carlo and Every Visit Monte Carlo:\n",
        "\n",
        "- **Every Visit Monte Carlo** updates the value of a state every time it is visited within an episode, whereas **First Visit Monte Carlo** updates the value only the first time a state is encountered.\n",
        "- **Every Visit Monte Carlo** tends to use more data for updating the value function since it uses all occurrences of each state within an episode, potentially leading to faster learning in some cases.\n",
        "- **First Visit Monte Carlo** avoids correlation between multiple visits to the same state, while Every Visit Monte Carlo exploits multiple visits to gather more data but may introduce more noise or bias in certain environments.\n",
        "\n",
        "Both methods will converge to the correct value function given enough episodes, but they may differ in how fast they converge depending on the environment and the task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TAInFcMQPpF"
      },
      "source": [
        "### Playing Black-Jack with Monte Carlo Methods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_wcgBvQQaly"
      },
      "outputs": [],
      "source": [
        "def extract_policy(env,value_table, gamma = 1.0):\n",
        "    if isinstance(env.observation_space, gym.spaces.Discrete):\n",
        "        policy = np.zeros(env.observation_space.n, dtype=int)  # Use integers for actions\n",
        "    else:\n",
        "        policy = {}\n",
        "\n",
        "    # Iterate through all states in the value table\n",
        "    for state in value_table.keys():\n",
        "        # Compute the action with the maximum expected value\n",
        "        best_action = np.argmax([\n",
        "            value_table.get((state, action), -1) for action in range(env.action_space.n)\n",
        "        ])\n",
        "        policy[state] = best_action\n",
        "    \n",
        "    return policy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "heZMvWZSQYrM"
      },
      "outputs": [],
      "source": [
        "env_train = gym.make('Blackjack-v1', natural=False, sab=False)\n",
        "venv_train = env_train.unwrapped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "WIm0pfihRGE0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First-visit MC: Completed 10000/500000 episodes\n",
            "First-visit MC: Completed 20000/500000 episodes\n",
            "First-visit MC: Completed 30000/500000 episodes\n",
            "First-visit MC: Completed 40000/500000 episodes\n",
            "First-visit MC: Completed 50000/500000 episodes\n",
            "First-visit MC: Completed 60000/500000 episodes\n",
            "First-visit MC: Completed 70000/500000 episodes\n",
            "First-visit MC: Completed 80000/500000 episodes\n",
            "First-visit MC: Completed 90000/500000 episodes\n",
            "First-visit MC: Completed 100000/500000 episodes\n",
            "First-visit MC: Completed 110000/500000 episodes\n",
            "First-visit MC: Completed 120000/500000 episodes\n",
            "First-visit MC: Completed 130000/500000 episodes\n",
            "First-visit MC: Completed 140000/500000 episodes\n",
            "First-visit MC: Completed 150000/500000 episodes\n",
            "First-visit MC: Completed 160000/500000 episodes\n",
            "First-visit MC: Completed 170000/500000 episodes\n",
            "First-visit MC: Completed 180000/500000 episodes\n",
            "First-visit MC: Completed 190000/500000 episodes\n",
            "First-visit MC: Completed 200000/500000 episodes\n",
            "First-visit MC: Completed 210000/500000 episodes\n",
            "First-visit MC: Completed 220000/500000 episodes\n",
            "First-visit MC: Completed 230000/500000 episodes\n",
            "First-visit MC: Completed 240000/500000 episodes\n",
            "First-visit MC: Completed 250000/500000 episodes\n",
            "First-visit MC: Completed 260000/500000 episodes\n",
            "First-visit MC: Completed 270000/500000 episodes\n",
            "First-visit MC: Completed 280000/500000 episodes\n",
            "First-visit MC: Completed 290000/500000 episodes\n",
            "First-visit MC: Completed 300000/500000 episodes\n",
            "First-visit MC: Completed 310000/500000 episodes\n",
            "First-visit MC: Completed 320000/500000 episodes\n",
            "First-visit MC: Completed 330000/500000 episodes\n",
            "First-visit MC: Completed 340000/500000 episodes\n",
            "First-visit MC: Completed 350000/500000 episodes\n",
            "First-visit MC: Completed 360000/500000 episodes\n",
            "First-visit MC: Completed 370000/500000 episodes\n",
            "First-visit MC: Completed 380000/500000 episodes\n",
            "First-visit MC: Completed 390000/500000 episodes\n",
            "First-visit MC: Completed 400000/500000 episodes\n",
            "First-visit MC: Completed 410000/500000 episodes\n",
            "First-visit MC: Completed 420000/500000 episodes\n",
            "First-visit MC: Completed 430000/500000 episodes\n",
            "First-visit MC: Completed 440000/500000 episodes\n",
            "First-visit MC: Completed 450000/500000 episodes\n",
            "First-visit MC: Completed 460000/500000 episodes\n",
            "First-visit MC: Completed 470000/500000 episodes\n",
            "First-visit MC: Completed 480000/500000 episodes\n",
            "First-visit MC: Completed 490000/500000 episodes\n",
            "First-visit MC: Completed 500000/500000 episodes\n"
          ]
        }
      ],
      "source": [
        "value_first_visit_mc = first_visit_mc_prediction(venv_train, n_episodes=500000)\n",
        "optimal_policy_first_visit = extract_policy(venv_train,value_first_visit_mc, gamma=0.9)\n",
        "# value_first_visit_mc = first_visit_mc_prediction(env_train, n_episodes=500000)\n",
        "# optimal_policy_first_visit = extract_policy(env_train,value_first_visit_mc, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "collapsed": true,
        "id": "bd7lpiAPRPxg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Every-visit MC: Completed 10000/500000 episodes\n",
            "Every-visit MC: Completed 20000/500000 episodes\n",
            "Every-visit MC: Completed 30000/500000 episodes\n",
            "Every-visit MC: Completed 40000/500000 episodes\n",
            "Every-visit MC: Completed 50000/500000 episodes\n",
            "Every-visit MC: Completed 60000/500000 episodes\n",
            "Every-visit MC: Completed 70000/500000 episodes\n",
            "Every-visit MC: Completed 80000/500000 episodes\n",
            "Every-visit MC: Completed 90000/500000 episodes\n",
            "Every-visit MC: Completed 100000/500000 episodes\n",
            "Every-visit MC: Completed 110000/500000 episodes\n",
            "Every-visit MC: Completed 120000/500000 episodes\n",
            "Every-visit MC: Completed 130000/500000 episodes\n",
            "Every-visit MC: Completed 140000/500000 episodes\n",
            "Every-visit MC: Completed 150000/500000 episodes\n",
            "Every-visit MC: Completed 160000/500000 episodes\n",
            "Every-visit MC: Completed 170000/500000 episodes\n",
            "Every-visit MC: Completed 180000/500000 episodes\n",
            "Every-visit MC: Completed 190000/500000 episodes\n",
            "Every-visit MC: Completed 200000/500000 episodes\n",
            "Every-visit MC: Completed 210000/500000 episodes\n",
            "Every-visit MC: Completed 220000/500000 episodes\n",
            "Every-visit MC: Completed 230000/500000 episodes\n",
            "Every-visit MC: Completed 240000/500000 episodes\n",
            "Every-visit MC: Completed 250000/500000 episodes\n",
            "Every-visit MC: Completed 260000/500000 episodes\n",
            "Every-visit MC: Completed 270000/500000 episodes\n",
            "Every-visit MC: Completed 280000/500000 episodes\n",
            "Every-visit MC: Completed 290000/500000 episodes\n",
            "Every-visit MC: Completed 300000/500000 episodes\n",
            "Every-visit MC: Completed 310000/500000 episodes\n",
            "Every-visit MC: Completed 320000/500000 episodes\n",
            "Every-visit MC: Completed 330000/500000 episodes\n",
            "Every-visit MC: Completed 340000/500000 episodes\n",
            "Every-visit MC: Completed 350000/500000 episodes\n",
            "Every-visit MC: Completed 360000/500000 episodes\n",
            "Every-visit MC: Completed 370000/500000 episodes\n",
            "Every-visit MC: Completed 380000/500000 episodes\n",
            "Every-visit MC: Completed 390000/500000 episodes\n",
            "Every-visit MC: Completed 400000/500000 episodes\n",
            "Every-visit MC: Completed 410000/500000 episodes\n",
            "Every-visit MC: Completed 420000/500000 episodes\n",
            "Every-visit MC: Completed 430000/500000 episodes\n",
            "Every-visit MC: Completed 440000/500000 episodes\n",
            "Every-visit MC: Completed 450000/500000 episodes\n",
            "Every-visit MC: Completed 460000/500000 episodes\n",
            "Every-visit MC: Completed 470000/500000 episodes\n",
            "Every-visit MC: Completed 480000/500000 episodes\n",
            "Every-visit MC: Completed 490000/500000 episodes\n",
            "Every-visit MC: Completed 500000/500000 episodes\n"
          ]
        }
      ],
      "source": [
        "value_every_visit_mc = every_visit_mc_prediction(venv_train, n_episodes=500000)\n",
        "optimal_policy_every_visit = extract_policy(venv_train,value_every_visit_mc, gamma=0.9)\n",
        "# value_every_visit_mc = every_visit_mc_prediction(env_train, n_episodes=500000)\n",
        "# optimal_policy_every_visit = extract_policy(env_train,value_every_visit_mc, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "zJeqe36mxy8i"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode finished with total reward: 1.0\n"
          ]
        }
      ],
      "source": [
        "total_reward = 0\n",
        "\n",
        "# env = gym.make('Blackjack-v1', natural=False, sab=False)\n",
        "env = gym.make('Blackjack-v1', natural=False, sab=False, render_mode=\"rgb_array\")\n",
        "env = gym.wrappers.RecordVideo(\n",
        "    env, \n",
        "    './video',\n",
        "    episode_trigger=lambda episode_id: True,\n",
        "    name_prefix=\"blackjack\")\n",
        "\n",
        "state, _ = env.reset()\n",
        "# print(f\"Effin State: {state} and underscore: {_}\")\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "# Interaction loop\n",
        "while not done:\n",
        "    # Get action from the optimal policy\n",
        "    action = int(optimal_policy_first_visit[state])\n",
        "    \n",
        "    # Step through the environment\n",
        "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "    \n",
        "    # Update state and total reward\n",
        "    state = next_state\n",
        "    total_reward += reward\n",
        "    done = terminated or truncated\n",
        "print(f\"Episode finished with total reward: {total_reward}\")\n",
        "\n",
        "env.close()\n",
        "\n",
        "\n",
        "# env = wrap_env(gym.make('Blackjack-v1', natural=False, sab=False))\n",
        "\n",
        "\n",
        "# # Reset the environment\n",
        "# state, _ = env.reset()\n",
        "\n",
        "# # Start the recorder (utility for displaying output)\n",
        "# env.start_video_recorder()\n",
        "\n",
        "# next_state = state\n",
        "\n",
        "# # Example of an interaction loop\n",
        "# for _ in range(10000):\n",
        "\n",
        "\n",
        "#     # Render the environment\n",
        "#     env.render()\n",
        "\n",
        "\n",
        "#     # Sample random action from action space\n",
        "#     # action = env.action_space.sample()\n",
        "#     action = int(optimal_policy_first_visit[next_state])\n",
        "\n",
        "#     # print(type(action),next_state)\n",
        "\n",
        "#     # Step through the environment using the action\n",
        "#     next_state, reward, done, info = env.step(action)\n",
        "\n",
        "#     # Break the loop if the episode is done\n",
        "#     if done:\n",
        "#         break\n",
        "\n",
        "\n",
        "# # close the video recorder(utility for displaying output)\n",
        "# env.close_video_recorder()\n",
        "\n",
        "# # Close the environment\n",
        "# env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "jVGDXkBXy_89"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Display Output\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_l7gMbAcTH06"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyP5XbwjMjhL4PdltKUeR3N8",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
